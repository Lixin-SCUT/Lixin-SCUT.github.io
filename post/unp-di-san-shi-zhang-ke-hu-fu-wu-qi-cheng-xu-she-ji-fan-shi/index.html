<html>
  <head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>UNP 第三十章 客户/服务器程序设计范式 | Lixin-ee</title>
<link rel="shortcut icon" href="https://lixin-scut.github.io//favicon.ico?v=1581347470384">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://lixin-scut.github.io//styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://lixin-scut.github.io/">
  <img class="avatar" src="https://lixin-scut.github.io//images/avatar.png?v=1581347470384" alt="">
  </a>
  <h1 class="site-title">
    Lixin-ee
  </h1>
  <p class="site-description">
    好景在望。
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          时间轴
        </a>
      
    
      
        <a href="/tags" class="menu">
          分类/标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>


        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              UNP 第三十章 客户/服务器程序设计范式
            </h2>
            <div class="post-info">
              <time class="post-time">
                · 2020-01-21 ·
              </time>
              
                <a href="https://lixin-scut.github.io//tag/FaScKSk5i" class="post-tag">
                  # 读书笔记
                </a>
              
                <a href="https://lixin-scut.github.io//tag/V7j6WsNEp" class="post-tag">
                  # UNIX网络编程
                </a>
              
            </div>
            
            <div class="post-content">
              <p>当开发一个Unix服务器程序时，我们有如下类型的进程控制可供选择。<br>
・本书第一个服务器程序即图1-9是一个迭代服务器(iterative server)程序，不过这种类型 的适用情形极为有限，因为这样的服务器在完成对当前客户的服务之前无法处理已等待 服务的新客户。<br>
・图5-2是本书第一个并发服务器(concurrent server)程序，它为每个客户调用fork派生一 个子进程。传统上大多数Unix服务器程序属于这种类型。<br>
・在6.8节，我们开发的另一个版本的TCP服务器程序由使用select处理任意多个客户的单 个进程构成。<br>
・在图26-3中我们的并发服务器程序被改为服务器为每个客户创建一个线程，以取代派生 一个进程。</p>
<p>我们将在本章探究并发服务器程序设计的另两类变体。<br>
・预先派生子进程(prefbrking)是让服务器在启动阶段调用fork创建一个子进程池。每个 客户请求由当前可用子进程池中的某个(闲置)子进程处理。<br>
・预先创建线程(prethreading)是让服务器在启动阶段创建一个线程池，每个客户由当前可用线程池中的某个(闲置)线程处理。</p>
<p>从其他服务器的实际CPU时间中减去迭代服务器的实际GPU 时间就得到相应服务器用于进程控制所需的CPU时间，因为迭代服务器没有进程控制开销。</p>
<h3 id="tcp客户程序设计范式">TCP客户程序设计范式</h3>
<p>我们已经探究了客户程序的各种设计范式，这里有必要汇总它们各自的优缺点。<br>
・图5-5是基本的TCP客户程序。该程序存在两个问题。首先，进程在被阻塞以等待用户输入期间，看不到诸如对端关闭连接等网络事件。其次，它以停-等模式运作，批处理效率极低。<br>
・图6-9是下一个迭代客户程序，它通过调用select使得进程能够在等待用户输入期间得 到网络事件通知。然而该程序存在不能正确地处理批量输入的问题。图6-13通过使用 shutdown函数解决了这个问题。<br>
・从图16-3开始给出的是使用非阻塞式I/O实现的客户程序。<br>
・第一个超越单进程单线程设计范畴的客户程序是图16-10,它使用fork派生一个子进程， 并由父进程（或子进程）处理从客户到服务器的数据，由子进程（或父进程）处理从服 务器到客户的数据。<br>
・图26-2使用两个线程取代两个进程。<br>
非阻塞式I/O版本尽管是最快的，其代码却比较复杂；使用两个进程或两个线程的版本相比之下代码简化得多，而运行速度只是稍逊而已。</p>
<h3 id="tcp迭代服务器程序">TCP迭代服务器程序</h3>
<p>迭代服务器没有执行任何进程控制。这就让我们测量出服务器处理如此数目客户所需CPU时间的一个<strong>基准值</strong>，从其他服务器的实测CPU时间中减去该值就能得到它们的进程控制时间。从<strong>进程控制角度</strong>看迭代服务器是最快的，因为它不执行进程控制。有了基准值之后，我们在图30-1 中比较各个实测CPU时间与基准值的差值。<br>
<img src="https://lixin-scut.github.io//post-images/1579572365941.png" alt=""></p>
<h3 id="tcp并发服务器程序每个客户一个子进程">TCP并发服务器程序，每个客户一个子进程</h3>
<p>传统上并发服务器调用fork派生一个子进程来处理每个客户。这使得服务器能够同时为多 个客户服务，每个进程-个客户。客户数目的唯一限制是操作系统对以其名义运行服务器的用 户ID能够同时拥有多少子进程的限制。图5-12就是一个并发服务器程序的例子，绝大多数TCP服务器程序也按照这个范式编写。<br>
并发服务器的问题在于为每个客户现场fork一个子进程比较耗费CPU时间。</p>
<pre><code>//serv01.c
//Code by Lixin on 2020.01.31

#include &quot;unp.h&quot;

int main(int argc,char **argv)
{
	int listenfd,connfd;
	pid_t pid;
	void sig_chld(int),sig_int(int),web_child(int);
	socklen_t clilen,addrlen;
	struct sockaddr *cliaddr;

	if(argc==2)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==3)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv01 [&lt;host&gt;] &lt;potr#&gt;&quot;);
	cliaddr=malloc(addrlen);

	signal(SIGCHLD,sig_chld);
	signal(SIGINT,sig_int);

	for(;;){
		clilen=addrlen;
		if((connfd=accept(listenfd,cliaddr,&amp;clilen))&lt;0){
			if(errno=EINTR)
				continue;
			else
			 err_sys(&quot;accept errpr&quot;);
		}

		if((childpid=fork())==0){
			close(listenfd);
			web_child(connfd);
			eixt(0);
		}
		close(connfd);
	}
}

void sig_int(int signo){
	void pr_cpu_time(void);

	pr_cpu_time();
	exit(0);
}
</code></pre>
<pre><code>//pr_cpu_time.c
//Code by Lixin on 2020.01.31

#include &quot;unp.h&quot;
#include &lt;sys/resource.h&gt;

#ifndef HAVE_GETRUSAGE_PROTO
int getrusage(int,struct rusage*);
#endif

void pr_cpu_time(void)
{
	double user,sys;
	struct rusage,myusage,childusage;

	if(getrusage(RUSAGE_SELF,&amp;myusage)&lt;0)
		err_sys(&quot;getruasge error&quot;);
	if(getrusage(RUSAGE_CHILDREN,&amp;childusage)&lt;0)
		err_sys(&quot;getrusage error&quot;);
	
	user=(double) myusage.ru_utime.tv_sec+myusage.ru_utime.tv_usec/1000000.0;
	user+=(double) childusage.ru_utime.tv_sec+childusage.ru_utime.tv_usec/1000000.0;
	
	sys=(double) myusage.ru_stime.tv_sec+myusage.ru_stime.tv_usec/1000000.0;
	sys+=(double) childusage.ru_stime.tv_sec+childusage.ru_stime.tv_usec/1000000.0;

	printf(&quot;\nuser time = %g,sys time = %g\n&quot;,user,sys);
}
</code></pre>
<pre><code>//web_child.c
//Code by Lixin on 2020.01.31

#include &quot;unp.h&quot;

#define MAXN 16384

void web_child(int sockfd)
{	int ntowrite;
	ssize_t nread;
	char line[MAXLINE],result[MAXN];

	for(;;){
		if((nread=readline(sockfd,line,MAXLINE))==0)
			return;
		ntowrite=atol(line);
		if((ntowrite&lt;=0)||(ntowrite&gt;MAXN))
			err_quit(&quot;client request for %d bytes&quot;,ntowrite);

		writen(sockfd,result,ntowrite);
	}

}
</code></pre>
<h3 id="tcp预先派生子进程服务器程序accept无上锁保护">TCP预先派生子进程服务器程序：accept无上锁保护</h3>
<p>预先派生子进程(prefdrking)：启动阶段预先派生一定数量的子进程，当各个客户连接到达时，这些子进程立即就能为它们服务。<br>
这种技术的优点在于无须引入父进程执行fork的开销就能处理新到的客户。缺点则是父进 程必须在服务器启动阶段猜测需要预先派生多少子进程。如果某个时刻客户数恰好等于子进程 总数，那么新到的客户将被忽略，直到至少有一个子进程重新可用。<br>
然而这些客户并未被完全忽略。内核将为每个新到的客户完成三路握手，直到达到相应套接字上listen调用的backlog数为止，然后在服务器调用accept时把这些已完成的连接传递给它。这么一来客户就能觉察到服务器在响应时间上的恶化，因为尽管它的connect调用可能立即返回 <strong>（注意这里也要区分好TCP返回和应用层返回的区别）</strong>，但是它的第一个请求可能是在一段时间之后才被服务器处理。<br>
<img src="https://lixin-scut.github.io//post-images/1579573503597.png" alt=""><br>
通过增加一些代码，服务器总能应对客户负载的变动。父进程必须做的就是持续监视可用 (即闲置)子进程数，一旦该值降到低于某个阈值就派生额外的子进程。同样，一旦该值超过另 一个阈值就终止一些过剩的子进程，过多的可用子进程也会导致性能退化。</p>
<pre><code>//serv02.c
//Code by Lixin on 2020.01.31

#include &quot;unp.h&quot;

static int nchildren;
static pid_t *pids;

int main(int argc,char **argv)
{
	int listenfd,i;
	void sig_int(int);
	socklen_t addrlen;
	pid_t child_make(int,int,int);

	if(argc==3)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==4)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv02 [&lt;host&gt;] &lt;potr#&gt;&lt;children#&gt;&quot;);
	nchildren=atoi(argv[argc-1]);
	pids=calloc(nchildren,sizeof(pid_t));

	for(i=0;i&lt;nchildren;++i)
		pids[i]=child_make(i,listenfd,addrlen);

	signal(SIGINT,sig_int);

	for(;;){
		pause();
		clilen=addrlen;
		if((connfd=accept(listenfd,cliaddr,&amp;clilen))&lt;0){
			if(errno=EINTR)
				continue;
			else
			 err_sys(&quot;accept errpr&quot;);
		}

		if((childpid=fork())==0){
			close(listenfd);
			web_child(connfd);
			eixt(0);
		}
		close(connfd);
	}
}

void sig_int(int signo){
	void pr_cpu_time(void);
	int i;

	for(i=0;i&lt;nchildren;++i)
		kill(pids[i],SIGTERM);
	while(wait(NULL)&gt;0)
		;
	if(errno!=ECHILD)
		err_sys(&quot;wait error&quot;);
	
	pr_cpu_time();
	exit(0);
}
</code></pre>
<pre><code>//child02.c
//Code by Lixin on 2020.01.31

#include &quot;unp.h&quot;

pid_t child_make(int i,int listenfd,int addrlen)
{
	pid_t pid;
	void child_main(int,int,int);

	if((pid=fork())&gt;0)
		return pid;
	
	child_main(i,listenfd,addrlen);
}

void child_main(int i,int listenfd,int addrlen)
{
	int connfd;
	void web_child(int);
	socklen_t clilen;
	struct sockaddr *cliaddr;

	cliaddr=malloc(addrlen);

	printf(&quot;child %ld starting\n&quot;,(long) getpid());
	for(;;){
		clilen=addrlen;
		connfd=accept(listenfd,cliaddr,&amp;lilen);

		web_child(connfd);
		close(connfd);
	}
}
</code></pre>
<p>既然getrusage汇报的是已终止子进程的资源利用统计，在调用pr_cpu_time之前就必须终止所有子进程。我们通过给每个子进程发送SIGTERM信号终止它们，并通过调用 wait汇集所有子进程的资源利用统计。<br>
调用fork派生子进程后只有父进程返回。子进程调用图30-12给出的child_main函数， 它是个无限循环。<br>
每个子进程调用accept返回一个已连接套接字，然后调用web.child处理客户请求，最后关闭连接。<strong>子进程一直在这个循环中反复，直到被父进程终止</strong>。</p>
<p><strong>多个进程在同一个监听描述符上调用accept</strong><br>
<img src="https://lixin-scut.github.io//post-images/1579573862224.png" alt=""><br>
描述符只是本进程引用file结构的proc结构中一个数组中某个元素的下标而已。fork调用 执行期间为子进程复制描述符的特性之一是：子进程中一个给定描述符引用的file结构正是父进程中同一个描述符引用的file结构。<br>
每个file结构都有一个引用计数。当打开一个文件或套 接字时，内核将为之构造一个file结构，并由作为打开操作返回值的描述符引用，它的引用计 数初值自然为1;以后每当调用fork以派生子进程或对打开操作返回的描述符（或其复制品） 调用dup以复制描述符时，该file结构的引用计数就递增（每次增1）。在我们的N个子进程的例 子中，file结构的引用计数为N+1 （别忘了父进程仍然保持该监听描述符打开着，不过它从不调用 accept）<br>
服务器进程在程序启动阶段派生N个子进程，它们各自调用accept并因而均被内核投入睡 眠。当第一个客户连接到达时，所有N个子进程均被唤醒。这是因为所有N个子进程所用的监听描述符（它们有相同的值）指向同一个socket结构，致使它们在同一个等待通道（wait channel）即这个socket结构的so_timeo成员上进入睡眠。<br>
尽管所有N个子进程均被唤醒，其中只有最先运行的子进程获得那个客户连接，其余N-1个子进程继续回复唾眠， 因为当它们将发现队列长度为0 （因为最先运行的连接早已取走了本就只有一个的连接）。<br>
这就是有时候称为<strong>惊群（thundering herd）</strong> 的问题，因为尽管只有一个子进程将获得连接， 所有N个子进程却都被唤醒了<br>
当可用子进程阻塞在accept调用上时，内核调度算法把各个连接均匀地散布到各个子进程。</p>
<p><strong>select 冲突</strong><br>
select函数的冲突(collision)现象以及内核如何处理<br>
当<strong>多个进程</strong>在引用<strong>同一个套接字的描述符上</strong>调用select时就会发生冲突，因为在socket结构中为存放本套接字就绪之时应该唤醒哪些进程而<strong>分配的仅仅是一个进程ID的空间</strong>。如果有多个进程在等待同一个套接字，那么内核必须唤醒的是阻塞在select调用中的所有进程，因为它不知道哪些进程受刚变得就绪的这个套接字影响。（注意，这里指的是多个进程同时调用select，亦即多个select，而不是一个select中可以存放多个同一套接字）<br>
可以迫使本服务器程序发生select冲突，办法是在调用accept之前加上一个select调用，等待监听套接字变为可读。各个子进程将阻塞在select调用而不是accept调用之中。<br>
<img src="https://lixin-scut.github.io//post-images/1579574743412.png" alt=""><br>
如果有<strong>多个进程阻塞在引用同一个实体</strong>(例如套接字或普通文件，由file结构直接或间接描述)的描述符上，那么最好直接阻塞在诸如accept之类的函数<strong>而不是select之中</strong>。</p>
<h3 id="tcp预先派生子进程服务器程序accept使用文件上锁保护">TCP预先派生子进程服务器程序,accept使用文件上锁保护</h3>
<p>允许多个进程在引用同一个监听套接字的描述符上调用accept的做法也仅仅适用于在内核中实现accept的内核，作为一个库函数实现accept的内核可能不允许这么做，某个子进程的accept就会返回EPROTO错误(表示协议有错)，原因在于库函数版本的accept并非一个原子操作</p>
<p>解决办法是让应用进程在<strong>调用accept前后安置某种形式的锁(lock)</strong>,这样任意时刻<strong>只有一个子进程阻塞在accept调用中</strong>，<strong>其他子进程则阻塞在试图获取</strong>用于保护accept的锁上。<br>
<img src="https://lixin-scut.github.io//post-images/1579575352861.png" alt=""><br>
<img src="https://lixin-scut.github.io//post-images/1579575446659.png" alt=""><br>
<img src="https://lixin-scut.github.io//post-images/1579575460814.png" alt=""></p>
<h3 id="tcp预先派生子进程服务器程序accept使用线程上锁保护">TCP预先派生子进程服务器程序，accept使用线程上锁保护</h3>
<p>上一节使用的POSEX文件上锁方法可移植到所有POSIX兼容系统，不过它涉及文件系统操作，可能比较耗时。<br>
本节我们改用线程上锁保护 accept,因为这种方法不仅适用于同一进程内各线程之间的上锁，而且适用于不同进程之间的上锁。<br>
（为了使用线程上锁，我们的main、child_make和child_main函数都保持不变，唯一需要改动的是那3个上锁函数。）<br>
在不同进程之间使用线程上锁要求：(1)<strong>互斥锁变量</strong>必须存放在由<strong>所有进程共享的内存区中</strong>；(2)必须<strong>告知线程函数库</strong>这是在<strong>不同进程之间共享</strong>的互斥锁。<br>
<img src="https://lixin-scut.github.io//post-images/1579576163684.png" alt=""><br>
<img src="https://lixin-scut.github.io//post-images/1579576201447.png" alt=""></p>
<pre><code>//lock_pthread.c
//Code by Lixin on 2020.02.01

#include &quot;unphtread.h&quot;
#include&lt;sys/mman.h&gt;

static pthread_mutex_t *mptr;

void my_lock_init(char *pathname)
{
	int fd;
	pthread_mutexattr_t mattr;

	fd=open(&quot;/dev/zero&quot;,O_RDWR,0);

	mptr=mmap(0,sizeof(pthread_mutex_t),PROT_READ|PROT_WRITE,MAP_SHARED,fd,0);
	close(fd);

	pthread_mutexattr_init(&amp;mattr);
	pthread_mutexattr_setpshared(&amp;mattr,PTHREAD_PROCESS_SHARED);
	pthread_mutex_init(mptr,&amp;mattr);
}

void my_lock_wait()
{
	pthread_mutex_lock(mptr);
}

void my_lock_release()
{
	pthread_mutex_unlock(mptr);
}
</code></pre>
<h3 id="tcp预先派生子进程服务器程序传递描述符">TCP预先派生子进程服务器程序，传递描述符</h3>
<p>派生子进程的最后一个版本：只让父进程调用accept,然后把所接 受的己连接套接字&quot;传递”给某个子进程。<br>
这么做绕过了为所有子进程的accept调用提供上锁 保护的可能需求，不过需要从父进程到子进程的某种形式的描述符传递。这种技术会使代码多少有点复杂，因为父进程必须跟踪子进程的忙闲状态，以便给空闲子进程传递新的套接字。<br>
<img src="https://lixin-scut.github.io//post-images/1579576278446.png" alt=""></p>
<pre><code>//child.h
//Code by Lixin on 2020.02.01

typedef struct {
	pid_t child_pid;
	int child_pipefd;
	int child_status;
	long child_count;
} 	Child;
Child *cptr;
</code></pre>
<p>结构中存放相应子进程的进程ID、父进程中连接到该子进程的字节流管道描述 符、子进程状态以及该子进程已处理客户的计数。<br>
在调用fork之前先创建一个<strong>字节流管道</strong>， 它是一对<strong>Unix域字节流套接字</strong>。派生出子进程之后，父进程关闭其中一个描述符 (sockfd[1]),子进程关闭另一个描述符(sockfd[0] )子进程还把流管道的自身拥有端 (sockfd[1])复制到标准错误输出，这样每个子进程就通过读写标准错误输出和父进程通信。</p>
<pre><code>//child05.c
//Code by Lixin on 2020.02.01

#include &quot;unp.h&quot;
#include &quot;child.h&quot;

pid_t child_make(int i,int listenfd,int addrlen)
{
	int sockf[2];
	pid_t pid;
	void child_main(int,int,int);

	socketpair(AF_LOCAL,SOCK_STREAM,0,sockfd);

	if((pid=fork())&gt;0){
	close(sockffd[1]);
	cptr[i].child_pid=pid;
	cptr[i].child_pipefd=sockfd[0];
	cptr[i].child_status=0;
	return pid;
	}

	dup2(sockfd[1],STDERR_FILENO);
	close(sockfd[0]);
	close(sockfd[1]);
	close(listenfd);
	child_main(i,listenfd,addrlen);
}

void child_main(int i,int listenfd,int addrlen)
{
	char c;
	int connfd;
	ssize_t n;
	void web_child(int);

	printf(&quot;child %ld starting\n&quot;,(long)getpid());
	for(;;){
		if((n=read_fd(STDERR_FINENO,&amp;c,1,&amp;connfd))==0)
			err_quit(&quot;read_fd return 0&quot;);
		if(connfd&lt;0)
			err_quit(&quot;no descriptor from read_fd&quot;);

		web_child(connfd);
		close(connfd);

		write(SRDERR,&quot;&quot;,1);
	}
}
</code></pre>
<p><img src="https://lixin-scut.github.io//post-images/1579576602577.png" alt=""></p>
<pre><code>//serv05.c
//Code by Lixin on 2020.02.01

#include &quot;unp.h&quot;
#include &quot;child.h&quot;

static int nchildren;

int main(int argc,char **argv)
{
	int listenfd,i,navail,maxfd,nsel,connfd,rc;
	void sig_int(int);
	pid_t child_make(int,int,int);
	ssize_t n;
	fd_set rset,masterset;
	socklen_t addrlen,clilen;
	struct sockaddr *cliaddr;

	if(argc==3)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==4)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv05 [&lt;host&gt;] &lt;port#&gt;&lt;#children&gt;&quot;);

	FD_ZERO(&amp;masterset);
	FD_SET(listenfd,&amp;masterset);
	maxfd=listenfd;
	cliaddr=malloc(addrlen);

	nchildren=atoi(argv[argc-1]);
	navail=nchildren;
	cptr=calloc(nchildren,sizeof(Child));

	for(i=0;i&lt;nchildren;++i){
		child_make(i,listenfd,addrlen);
		FD_SET(cptr[i].child_pipefd,&amp;masterset);
		maxfd=max(maxfd,cptr[i].child_pipefd);
	}

	signal(SIGINT,sig_int);
	for(;;){
		rset=masterset;
		if(navail&lt;0)
			FD_CLR(listenfd,&amp;rset);
		nsel=select(maxfd+1,&amp;rset NULL,NULL,NULL);

		if(FD_ISSET(listenfd,&amp;rset)){
			clilen=addrlen;
			connfd=accpet(listenfd,cliaddr,&amp;clilen);

			for(i=0;i&lt;nchildren;++i)
				if(cptr[i].child_status==0)
					break;
			if(i==nchildren)
				err_quit(&quot;no available children&quot;);
			cptr[i].child_stauts=1;
			cptr[i].child_count++;
			--navail;

			n=write_fd(cptr[i].childpipefd,&quot;&quot;,1,connfd);
			close(connfd);
			if(--nsel==0)
				continue;
		}

		for(i=0;i&lt;children;++i){
			if(FD_ISSET(cptr[i].child_pipefd,&amp;rset)){
				if((n=read(cptr[i].child_pipefd,&amp;rc,1))==0)
					err_quit(&quot;child %d terminated unexpectedly&quot;,i);
				cptr[i].child_status=0;
				++navail;
				if(--nsel==0)
					break;
			}
		}
	}
}
</code></pre>
<p>计数器navail用于跟踪当前可用的子进程数。如果其值为0,那就从select的读描述符 集中关掉与监听套接字对应的位。这么做防止父进程在无可用子进程的情况下accept 新连接。<br>
write_fd函数把就绪的已连接套接字传递给该子 进程<br>
child_main函数在调用子进程<strong>处理完一个客户</strong>之后，通过该子进程的<strong>字节流管道拥有端</strong>向父进程<strong>写回单个字节</strong>。这使得该字节流管道的父进程拥有端变为可读。<br>
父进程读入这个单字节（忽略其值），把该子进程标为可用，并递增navail计数器。要 是该子进程意外终止，它的字节流管道拥有端将被关闭，因而read将返回0。父进程察觉到之后就终止近行，不过更好的做法是登记这个错误，并重新派生一个子进程取代意外终止的那个子进程。<br>
子进程不再调用accept,而是阻塞在 read_fd调用中，等待父进程传递过来一个巳连接套接字描述符</p>
<h3 id="tcp并发服务器程序每个客户一个线程">TCP并发服务器程序:每个客户一个线程</h3>
<p>如果服务器主机支持线程，我们就可以改用线程以取代子进程。<br>
为每个客户创建一个线程，以取代为每个客户派生一个子进程。</p>
<pre><code>//serv06.c
//Code by Lixin on 2020.02.01

#include &quot;unpthread.h&quot;

int main(int argc,char **argv)
{
	int listenfd,connfd;
	void sig_int(int);
	void* doit(void*);
	pthread_t tid;
	socklen_t addrlen,clilen;
	struct sockaddr *cliaddr;

	if(argc==2)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==3)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv06 [&lt;host&gt;] &lt;port#&gt;&quot;);
	cliaddr=malloc(addrlen);
	
	signal(SIGINT,sig_int);

	for(;;){
		clilen=addrlen;
		connfd=accept(listenfd,cliaddr,&amp;clilen);
		pthread_create(&amp;tid,NULL,&amp;doit,(void*)connfd);
	}
}

void* doit(void *arg)
{
	void web_child(int);
	pthread_detach(pthread_self());
	web_child((int)arg);
	close((int)arg);
	return NULL;
}
</code></pre>
<p>doit函数先让自己脱离，使得主线程不必等待它</p>
<h3 id="tcp预先创建线程服务器程序每个线程各自accept">TCP预先创建线程服务器程序，每个线程各自accept</h3>
<p>在支持线程的系统上，我们有理由预期在服务器启动阶段预先创建一个线程池以取代为每 个客户现场创建一个线程的做法有类似的性能加速。<br>
本服务器的基本设计是预先创建一个线程 池，并让每个线程各自调用accept„取代让每个线程都阻塞在accept调用之中的做法，我们改用互斥锁以保证任何时刻只有一个线程在调用accept<br>
这里没有理由使用文件上锁保护各个线程中的accept调用，因为对于单个进程中的多个线程，我们总可以使用互斥锁达到同样目的。<br>
<img src="https://lixin-scut.github.io//post-images/1579577167043.png" alt=""></p>
<pre><code>//pthread07.h
//Code by Lixin on 2020.02.01

typedef struct{
	pthread_t thread_tid;
	long thread_count;
}Thread;
Thread *tptr;

int listenfd,nthreads;
socklen_t addrlen;
pthread_mutex_t mlock;
</code></pre>
<p>我们还声明了一些全局变量，譬如监听套接字描述符和一个需由所有线程共享的互斥锁变量等。</p>
<pre><code>//serv07.c
//Code by Lixin on 2020.02.01

#include &quot;unpthread.h&quot;
#include &quot;pthread07.h&quot;

pthread_mutex_t mlock=PTHREAD_MUTEX_INITIALIZER; 

int main(int argc, char **argv){
	int i;
	void sig_int(int),thread_make(int);
	
	if(argc==3)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==4)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv07 [&lt;host&gt;] &lt;port&gt;&lt;#threads&gt;&quot;);
	
	nthread=atoi(argv[argc-1]);
	tptr=calloc(nthreads,sizeof(Thread));
	for(i=0;i&lt;nthread;++i)
		thread_make(i);

	signal(SIGINT,sig_int);
	for(;;)
		pause;
}
</code></pre>
<pre><code>//pthread07.c
//Code by Lixin on 2020.02.01

#include &quot;unpthread.h&quot;
#include &quot;pthread07.h&quot;

void thread_make(int i)
{
	void *thread_main(void*);

	pthread_create(&amp;tptr[i].thread_tid,NULL,&amp;thread_main,(void*)i);
	return;
}

void* thread_main(void *arg)
{
	int connfd;
	void web_child(int);
	socklen_t clilen;
	struct sockaddr *cliaddr;

	cliaddr=malloc(addrlen);

	printf(&quot;thread %d starting\n&quot;,(int)arg);
	for(;;){
		clilen=addrlen;
		pthread_mutex_lock(&amp;mlock);
		connfd=accept(listenfd,cliaddr,&amp;clilen);
		pthread_mutex_unlock(&amp;mlock);
		tptr[(int)arg].thread_count++;

		web_child(connfd);
		close(connfd);
	}
}
</code></pre>
<h3 id="tcp预先创建线程服务器程序主线程统一accept">TCP预先创建线程服务器程序，主线程统一accept</h3>
<p>最后一个使用线程的服务器程序设计范式是在程序启动阶段创建一个线程池之后只让主线程调用accept并把每个客户连接传递给池中某个可用线程<br>
本设计范式的问题在于主线程如何把一个已连接套接字传递给线程池中某个可用线程。<br>
既然所有线程和所有描述符都在<strong>同一个进程</strong>之内，我们<strong>没有必要</strong>把一个描述符从一个线程传递到另一个线程。接收线程只需知道这个己连接套接字描述符的值，而描述符传递实际传递的并非这个值，而是<strong>对这个套接字的一个引用</strong>，因而将<strong>返回一个不同于原值的描述符</strong>（该套接字的引用计数也被递增）。<br>
<img src="https://lixin-scut.github.io//post-images/1579577365769.png" alt=""></p>
<pre><code>//pthread08.h
//Code by Lixin on 2020.02.01

typedef struct{
	pthread_t thread_tid;
	long thread_count;
}Thread;
Thread *tptr;

#define MAXNCLI 32
int clifd[MAXNCLI],iget,iput;
pthread_mutex_t clifd_mutex;
pthread_cond_t clifd_cond;
</code></pre>
<p>还定义一个clifd数组，由<strong>主线程</strong>往中存入<strong>已接受的已连接套接字描述符</strong>，并由<strong>线程池</strong>中的<strong>可用线程</strong>从中<strong>取出一个以服务相应的客户</strong>。iput是主线程将往该数组中存入 的下一个元素的下标，iget是线程池中某个线程将从该数组中取出的下一个元素的下 标。这个由<strong>所有线程共享的数据结构</strong>自然<strong>必须得到保护</strong>，我们使用互斥锁和条件变量做到这一点。</p>
<pre><code>//serv08.c
//Code by Lixin on 2020.02.01

#include &quot;unpthread.h&quot;
#include &quot;pthread08.h&quot;

static int nthreads;
pthread_mutex_t clifd_mutex=PTHREAD_MUTEX_INITIALIZER; 
pthread_cond_t clifd_cond=PTHREAD_COND_INITIALIZER;

int main(int argc, char **argv){
	int i,listenfd,connfd;
	void sig_int(int),thread_make(int);
	socklen_t addrlen,clilen;
	struct sockaddr *cliaddr;

	if(argc==3)
		listenfd=tcp_listen(NULL,argv[1],&amp;addrlen);
	else if(argc==4)
		listenfd=tcp_listen(argv[1],argv[2],&amp;addrlen);
	else
		err_quit(&quot;usage:serv08 [&lt;host&gt;] &lt;port&gt;&lt;#threads&gt;&quot;);
	cliaddr=malloc(addrlen);
	
	nthread=atoi(argv[argc-1]);
	tptr=calloc(nthreads,sizeof(Thread));
	iget=iput=0;
	
	for(i=0;i&lt;nthread;++i)
		thread_make(i);

	signal(SIGINT,sig_int);
	
	for(;;){
		clilen=addrlen;
		connfd=accept(listenfd,cliaddr,&amp;clilen);

		pthread_mutex_lock(&amp;child_mutex);
		clifd[iput]=connfd;
		if(++iput==MAXNCLI)
			iput=0;
		if(iput==iget)
			err_quit(&quot;iget==iput==%d&quot;,iput);
		phtread_cond_signal(&amp;child_cond);
		pthread_mutex_unlock(&amp;child_mutex);
	}
}
</code></pre>
<pre><code>//pthread08.c
//Code by Lixin on 2020.02.01

#include &quot;unpthread.h&quot;
#include &quot;pthread08.h&quot;

void thread_make(int i)
{
	void *thread_main(void*);

	pthread_create(&amp;tptr[i].thread_tid,NULL,&amp;thread_main,(void*)i);
	return;
}

void* thread_main(void *arg)
{
	int connfd;
	void web_child(int);

	pritf(&quot;thread %d starting\n&quot;,(int)arg);
	for(;;){
		pthread_mutex_lock(&amp;child_mutex);
		while(iget==iput)
			pthread_cond_wait(&amp;child_cond,&amp;child_mutex);
		connfd=clifd[iget];
		if(++iget==MAXNCLI)
			iget=0;
		pthread_mutex_unlock(&amp;clifd_mutex);
		tptr[(int)arg].thread_count++;

		web_child(connfd);
		close(fd);
	}
}
</code></pre>
<p>主线程大部分时间阻塞在accept调用中，等待各个客户连接的到达。一旦某个客户连 接到达，主线程就把它的已连接套接字描述符存入clifd数组的下—个元素，不过需事 先获取保护该数组的互斥锁。<br>
主线 程接受一个连接后将调用pthread_cond_signal向条件变量发送信号，以唤醒睡眠在 其上的线程。<br>
当主线程调用pthread_cond_signal引起线程函数库基于条件变量执行唤醒 工作时，该函数库在所有可用线程中轮循唤醒其中一个。</p>
<p>小结：<br>
<img src="https://lixin-scut.github.io//post-images/1579577535464.png" alt=""></p>

            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://lixin-scut.github.io//post/unp-di-er-shi-liu-zhang-xian-cheng">
              <h3 class="post-title">
                 UNP 第二十六章 线程
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '253c8689f3b11b51dcf1',
    clientSecret: '64df7c097665e7dd6f416ecfaba36581a91bdb63',
    repo: 'Lixin-SCUT.github.io',
    owner: 'Lixin-SCUT',
    admin: ['Lixin-SCUT'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://lixin-scut.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

      </div>
    </div>
  </body>
</html>
