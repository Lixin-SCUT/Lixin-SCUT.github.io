<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lixin-scut.github.io/</id>
    <title>Lixin-SCUT</title>
    <updated>2020-05-13T07:55:29.322Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lixin-scut.github.io/"/>
    <link rel="self" href="https://lixin-scut.github.io//atom.xml"/>
    <subtitle>千里之行，始于足下。不积跬步，无以至千里。</subtitle>
    <logo>https://lixin-scut.github.io//images/avatar.png</logo>
    <icon>https://lixin-scut.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, Lixin-SCUT</rights>
    <entry>
        <title type="html"><![CDATA[Python - 常用第三方模块]]></title>
        <id>https://lixin-scut.github.io//post/python-chang-yong-di-san-fang-mo-kuai</id>
        <link href="https://lixin-scut.github.io//post/python-chang-yong-di-san-fang-mo-kuai">
        </link>
        <updated>2020-05-13T07:24:22.000Z</updated>
        <content type="html"><![CDATA[<p>除了内建的模块外，Python还有大量的第三方模块。</p>
<p>基本上，所有的第三方模块都会在PyPI - the Python Package Index上注册，只要找到对应的模块名字，即可用pip安装。</p>
<p>此外，在安装第三方模块一节中，我们强烈推荐安装Anaconda，安装后，数十个常用的第三方模块就已经就绪，不用pip手动安装。</p>
<h2 id="pillow-图像处理">Pillow - 图像处理</h2>
<p>PIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。</p>
<p>由于PIL仅支持到Python 2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python 3.x，又加入了许多新特性，因此，我们可以直接安装使用Pillow。</p>
<h3 id="安装pillow">安装Pillow</h3>
<p>如果安装了Anaconda，Pillow就已经可用了。否则，需要在命令行下通过pip安装：</p>
<pre><code>$ pip install pillow
</code></pre>
<p>如果遇到Permission denied安装失败，请加上sudo重试。</p>
<h3 id="操作图像">操作图像</h3>
<p>来看看最常见的图像缩放操作，只需三四行代码：</p>
<pre><code>
from PIL import Image

# 打开一个jpg图像文件，注意是当前路径:
im = Image.open('test.jpg')
# 获得图像尺寸:
w, h = im.size
print('Original image size: %sx%s' % (w, h))
# 缩放到50%:
im.thumbnail((w//2, h//2))
print('Resize image to: %sx%s' % (w//2, h//2))
# 把缩放后的图像用jpeg格式保存:
im.save('thumbnail.jpg', 'jpeg')

</code></pre>
<p>其他功能如切片、旋转、滤镜、输出文字、调色板等一应俱全。</p>
<p>比如，模糊效果也只需几行代码：</p>
<pre><code>from PIL import Image, ImageFilter

# 打开一个jpg图像文件，注意是当前路径:
im = Image.open('test.jpg')
# 应用模糊滤镜:
im2 = im.filter(ImageFilter.BLUR)
im2.save('blur.jpg', 'jpeg')

</code></pre>
<p>PIL的ImageDraw提供了一系列绘图方法，让我们可以直接绘图。比如要生成字母验证码图片：</p>
<pre><code>from PIL import Image, ImageDraw, ImageFont, ImageFilter

import random

# 随机字母:
def rndChar():
    return chr(random.randint(65, 90))

# 随机颜色1:
def rndColor():
    return (random.randint(64, 255), random.randint(64, 255), random.randint(64, 255))

# 随机颜色2:
def rndColor2():
    return (random.randint(32, 127), random.randint(32, 127), random.randint(32, 127))

# 240 x 60:
width = 60 * 4
height = 60
image = Image.new('RGB', (width, height), (255, 255, 255))
# 创建Font对象:
font = ImageFont.truetype('Arial.ttf', 36)
# 创建Draw对象:
draw = ImageDraw.Draw(image)
# 填充每个像素:
for x in range(width):
    for y in range(height):
        draw.point((x, y), fill=rndColor())
# 输出文字:
for t in range(4):
    draw.text((60 * t + 10, 10), rndChar(), font=font, fill=rndColor2())
# 模糊:
image = image.filter(ImageFilter.BLUR)
image.save('code.jpg', 'jpeg')

</code></pre>
<p>如果运行的时候报错：<br>
<code>IOError: cannot open resource</code><br>
这是因为PIL无法定位到字体文件的位置，可以根据操作系统提供绝对路径，比如：<br>
<code>'/Library/Fonts/Arial.ttf'</code><br>
要详细了解PIL的强大功能，请请参考Pillow官方文档：</p>
<p><code>https://pillow.readthedocs.org/</code></p>
<h3 id="小结">小结</h3>
<p>PIL提供了操作图像的强大功能，可以通过简单的代码完成复杂的图像处理。</p>
<h2 id="requests-处理url资源">requests - 处理URL资源</h2>
<p>我们已经讲解了Python内置的urllib模块，用于访问网络资源。但是，它用起来比较麻烦，而且，缺少很多实用的高级功能。</p>
<p>更好的方案是使用requests。它是一个Python第三方库，处理URL资源特别方便。</p>
<h3 id="安装requests">安装requests</h3>
<p>如果安装了Anaconda，requests就已经可用了。否则，需要在命令行下通过pip安装：</p>
<pre><code>$ pip install requests
</code></pre>
<p>如果遇到Permission denied安装失败，请加上sudo重试。</p>
<h3 id="使用requests">使用requests</h3>
<p>要通过GET访问一个页面，只需要几行代码：</p>
<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; r = requests.get('https://www.douban.com/') # 豆瓣首页
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; r.text
r.text
'&lt;!DOCTYPE HTML&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta name=&quot;description&quot; content=&quot;提供图书、电影、音乐唱片的推荐、评论和...'
</code></pre>
<p>对于带参数的URL，传入一个dict作为params参数：</p>
<pre><code>&gt;&gt;&gt; r = requests.get('https://www.douban.com/search', params={'q': 'python', 'cat': '1001'})
&gt;&gt;&gt; r.url # 实际请求的URL
'https://www.douban.com/search?q=python&amp;cat=1001'
</code></pre>
<p>requests自动检测编码，可以使用encoding属性查看：</p>
<pre><code>&gt;&gt;&gt; r.encoding
'utf-8'
</code></pre>
<p>无论响应是文本还是二进制内容，我们都可以用content属性获得bytes对象：</p>
<pre><code>&gt;&gt;&gt; r.content
b'&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n...'
</code></pre>
<p>requests的方便之处还在于，对于特定类型的响应，例如JSON，可以直接获取：</p>
<pre><code>&gt;&gt;&gt; r = requests.get('https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20weather.forecast%20where%20woeid%20%3D%202151330&amp;format=json')
&gt;&gt;&gt; r.json()
{'query': {'count': 1, 'created': '2017-11-17T07:14:12Z', ...
</code></pre>
<p>需要传入HTTP Header时，我们传入一个dict作为headers参数：</p>
<pre><code>&gt;&gt;&gt; r = requests.get('https://www.douban.com/', headers={'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit'})
&gt;&gt;&gt; r.text
'&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=&quot;UTF-8&quot;&gt;\n &lt;title&gt;豆瓣(手机版)&lt;/title&gt;...'
</code></pre>
<p>要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据：</p>
<pre><code>&gt;&gt;&gt; r = requests.post('https://accounts.douban.com/login', data={'form_email': 'abc@example.com', 'form_password': '123456'})
</code></pre>
<p>requests默认使用<code>application/x-www-form-urlencoded</code>对POST数据编码。如果要传递JSON数据，可以直接传入json参数：</p>
<pre><code>params = {'key': 'value'}
r = requests.post(url, json=params) # 内部自动序列化为JSON
</code></pre>
<p>类似的，上传文件需要更复杂的编码格式，但是requests把它简化成files参数：</p>
<pre><code>&gt;&gt;&gt; upload_files = {'file': open('report.xls', 'rb')}
&gt;&gt;&gt; r = requests.post(url, files=upload_files)
</code></pre>
<p>在读取文件时，注意务必使用'rb'即二进制模式读取，这样获取的bytes长度才是文件的长度。</p>
<p>把post()方法替换为put()，delete()等，就可以以PUT或DELETE方式请求资源。</p>
<p>除了能轻松获取响应内容外，requests对获取HTTP响应的其他信息也非常简单。例如，获取响应头：</p>
<pre><code>&gt;&gt;&gt; r.headers
{Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Content-Encoding': 'gzip', ...}
&gt;&gt;&gt; r.headers['Content-Type']
'text/html; charset=utf-8'
</code></pre>
<p>requests对Cookie做了特殊处理，使得我们不必解析Cookie就可以轻松获取指定的Cookie：</p>
<pre><code>&gt;&gt;&gt; r.cookies['ts']
'example_cookie_12345'
</code></pre>
<p>要在请求中传入Cookie，只需准备一个dict传入cookies参数：</p>
<pre><code>&gt;&gt;&gt; cs = {'token': '12345', 'status': 'working'}
&gt;&gt;&gt; r = requests.get(url, cookies=cs)
</code></pre>
<p>最后，要指定超时，传入以秒为单位的timeout参数：</p>
<pre><code>&gt;&gt;&gt; r = requests.get(url, timeout=2.5) # 2.5秒后超时
</code></pre>
<h2 id="chardet-字符串编码检测">chardet - 字符串编码检测</h2>
<p>字符串编码一直是令人非常头疼的问题，尤其是我们在处理一些不规范的第三方网页的时候。虽然Python提供了Unicode表示的str和bytes两种数据类型，并且可以通过encode()和decode()方法转换，但是，在不知道编码的情况下，对bytes做decode()不好做。</p>
<p>对于未知编码的bytes，要把它转换成str，需要先“猜测”编码。猜测的方式是先收集各种编码的特征字符，根据特征字符判断，就能有很大概率“猜对”。</p>
<p>当然，我们肯定不能从头自己写这个检测编码的功能，这样做费时费力。chardet这个第三方库正好就派上了用场。用它来检测编码，简单易用。</p>
<h3 id="安装chardet">安装chardet</h3>
<p>如果安装了Anaconda，chardet就已经可用了。否则，需要在命令行下通过pip安装：</p>
<pre><code>$ pip install chardet
</code></pre>
<p>如果遇到Permission denied安装失败，请加上sudo重试。</p>
<h3 id="使用chardet">使用chardet</h3>
<p>当我们拿到一个bytes时，就可以对其检测编码。用chardet检测编码，只需要一行代码：</p>
<pre><code>&gt;&gt;&gt; chardet.detect(b'Hello, world!')
{'encoding': 'ascii', 'confidence': 1.0, 'language': ''}
</code></pre>
<p>检测出的编码是ascii，注意到还有个confidence字段，表示检测的概率是1.0（即100%）。</p>
<p>我们来试试检测GBK编码的中文：</p>
<pre><code>&gt;&gt;&gt; data = '离离原上草，一岁一枯荣'.encode('gbk')
&gt;&gt;&gt; chardet.detect(data)
{'encoding': 'GB2312', 'confidence': 0.7407407407407407, 'language': 'Chinese'}
</code></pre>
<p>检测的编码是GB2312，注意到GBK是GB2312的超集，两者是同一种编码，检测正确的概率是74%，language字段指出的语言是'Chinese'。</p>
<p>对UTF-8编码进行检测：</p>
<pre><code>&gt;&gt;&gt; data = '离离原上草，一岁一枯荣'.encode('utf-8')
&gt;&gt;&gt; chardet.detect(data)
{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}
</code></pre>
<p>我们再试试对日文进行检测：</p>
<pre><code>&gt;&gt;&gt; data = '最新の主要ニュース'.encode('euc-jp')
&gt;&gt;&gt; chardet.detect(data)
{'encoding': 'EUC-JP', 'confidence': 0.99, 'language': 'Japanese'}
</code></pre>
<p>可见，用chardet检测编码，使用简单。获取到编码后，再转换为str，就可以方便后续处理。</p>
<p>chardet支持检测的编码列表请参考官方文档Supported encodings。</p>
<h2 id="psutil">psutil</h2>
<p>用Python来编写脚本简化日常的运维工作是Python的一个重要用途。在Linux下，有许多系统命令可以让我们时刻监控系统运行的状态，如ps，top，free等等。要获取这些系统信息，Python可以通过subprocess模块调用并获取结果。但这样做显得很麻烦，尤其是要写很多解析代码。</p>
<p>在Python中获取系统信息的另一个好办法是使用psutil这个第三方模块。顾名思义，psutil = process and system utilities，它不仅可以通过一两行代码实现系统监控，还可以跨平台使用，支持Linux／UNIX／OSX／Windows等，是系统管理员和运维小伙伴不可或缺的必备模块。</p>
<h3 id="安装psutil-系统监控">安装psutil - 系统监控</h3>
<p>如果安装了Anaconda，psutil就已经可用了。否则，需要在命令行下通过pip安装：</p>
<pre><code>$ pip install psutil
</code></pre>
<p>如果遇到Permission denied安装失败，请加上sudo重试。</p>
<h3 id="获取cpu信息">获取CPU信息</h3>
<p>我们先来获取CPU的信息：</p>
<pre><code>&gt;&gt;&gt; import psutil
&gt;&gt;&gt; psutil.cpu_count() # CPU逻辑数量
4
&gt;&gt;&gt; psutil.cpu_count(logical=False) # CPU物理核心
2
# 2说明是双核超线程, 4则是4核非超线程
</code></pre>
<p>统计CPU的用户／系统／空闲时间：</p>
<pre><code>&gt;&gt;&gt; psutil.cpu_times()
scputimes(user=10963.31, nice=0.0, system=5138.67, idle=356102.45)
</code></pre>
<p>再实现类似top命令的CPU使用率，每秒刷新一次，累计10次：</p>
<pre><code>&gt;&gt;&gt; for x in range(10):
...     print(psutil.cpu_percent(interval=1, percpu=True))
... 
[14.0, 4.0, 4.0, 4.0]
[12.0, 3.0, 4.0, 3.0]
[8.0, 4.0, 3.0, 4.0]
[12.0, 3.0, 3.0, 3.0]
[18.8, 5.1, 5.9, 5.0]
[10.9, 5.0, 4.0, 3.0]
[12.0, 5.0, 4.0, 5.0]
[15.0, 5.0, 4.0, 4.0]
[19.0, 5.0, 5.0, 4.0]
[9.0, 3.0, 2.0, 3.0]
</code></pre>
<h3 id="获取内存信息">获取内存信息</h3>
<p>使用psutil获取物理内存和交换内存信息，分别使用：</p>
<pre><code>&gt;&gt;&gt; psutil.virtual_memory()
svmem(total=8589934592, available=2866520064, percent=66.6, used=7201386496, free=216178688, active=3342192640, inactive=2650341376, wired=1208852480)
&gt;&gt;&gt; psutil.swap_memory()
sswap(total=1073741824, used=150732800, free=923009024, percent=14.0, sin=10705981440, sout=40353792)
</code></pre>
<p>返回的是字节为单位的整数，可以看到，总内存大小是8589934592 = 8 GB，已用7201386496 = 6.7 GB，使用了66.6%。</p>
<p>而交换区大小是1073741824 = 1 GB。</p>
<h3 id="获取磁盘信息">获取磁盘信息</h3>
<p>可以通过psutil获取磁盘分区、磁盘使用率和磁盘IO信息：</p>
<pre><code>&gt;&gt;&gt; psutil.disk_partitions() # 磁盘分区信息
[sdiskpart(device='/dev/disk1', mountpoint='/', fstype='hfs', opts='rw,local,rootfs,dovolfs,journaled,multilabel')]
&gt;&gt;&gt; psutil.disk_usage('/') # 磁盘使用情况
sdiskusage(total=998982549504, used=390880133120, free=607840272384, percent=39.1)
&gt;&gt;&gt; psutil.disk_io_counters() # 磁盘IO
sdiskio(read_count=988513, write_count=274457, read_bytes=14856830464, write_bytes=17509420032, read_time=2228966, write_time=1618405)
可以看到，磁盘'/'的总容量是998982549504 = 930 GB，使用了39.1%。文件格式是HFS，opts中包含rw表示可读写，journaled表示支持日志。
</code></pre>
<h3 id="获取网络信息">获取网络信息</h3>
<p>psutil可以获取网络接口和网络连接信息：</p>
<pre><code>&gt;&gt;&gt; psutil.net_io_counters() # 获取网络读写字节／包的个数
snetio(bytes_sent=3885744870, bytes_recv=10357676702, packets_sent=10613069, packets_recv=10423357, errin=0, errout=0, dropin=0, dropout=0)
&gt;&gt;&gt; psutil.net_if_addrs() # 获取网络接口信息
{
  'lo0': [snic(family=&lt;AddressFamily.AF_INET: 2&gt;, address='127.0.0.1', netmask='255.0.0.0'), ...],
  'en1': [snic(family=&lt;AddressFamily.AF_INET: 2&gt;, address='10.0.1.80', netmask='255.255.255.0'), ...],
  'en0': [...],
  'en2': [...],
  'bridge0': [...]
}
&gt;&gt;&gt; psutil.net_if_stats() # 获取网络接口状态
{
  'lo0': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=16384),
  'en0': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=1500),
  'en1': snicstats(...),
  'en2': snicstats(...),
  'bridge0': snicstats(...)
}
</code></pre>
<p>要获取当前网络连接信息，使用net_connections()：</p>
<pre><code>&gt;&gt;&gt; psutil.net_connections()
Traceback (most recent call last):
  ...
PermissionError: [Errno 1] Operation not permitted

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  ...
psutil.AccessDenied: psutil.AccessDenied (pid=3847)
</code></pre>
<p>你可能会得到一个AccessDenied错误，原因是<strong>psutil获取信息也是要走系统接口，而获取网络连接信息需要root权限</strong>，这种情况下，可以退出Python交互环境，用sudo重新启动：</p>
<pre><code>$ sudo python3
Password: ******
Python 3.8 ... on darwin
Type &quot;help&quot;, ... for more information.
&gt;&gt;&gt; import psutil
&gt;&gt;&gt; psutil.net_connections()
[
    sconn(fd=83, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, laddr=addr(ip='::127.0.0.1', port=62911), raddr=addr(ip='::127.0.0.1', port=3306), status='ESTABLISHED', pid=3725),
    sconn(fd=84, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, laddr=addr(ip='::127.0.0.1', port=62905), raddr=addr(ip='::127.0.0.1', port=3306), status='ESTABLISHED', pid=3725),
    sconn(fd=93, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, laddr=addr(ip='::', port=8080), raddr=(), status='LISTEN', pid=3725),
    sconn(fd=103, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, laddr=addr(ip='::127.0.0.1', port=62918), raddr=addr(ip='::127.0.0.1', port=3306), status='ESTABLISHED', pid=3725),
    sconn(fd=105, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, ..., pid=3725),
    sconn(fd=106, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, ..., pid=3725),
    sconn(fd=107, family=&lt;AddressFamily.AF_INET6: 30&gt;, type=1, ..., pid=3725),
    ...
    sconn(fd=27, family=&lt;AddressFamily.AF_INET: 2&gt;, type=2, ..., pid=1)
]
</code></pre>
<h3 id="获取进程信息">获取进程信息</h3>
<p>通过psutil可以获取到所有进程的详细信息：</p>
<pre><code>&gt;&gt;&gt; psutil.pids() # 所有进程ID
[3865, 3864, 3863, 3856, 3855, 3853, 3776, ..., 45, 44, 1, 0]
&gt;&gt;&gt; p = psutil.Process(3776) # 获取指定进程ID=3776，其实就是当前Python交互环境
&gt;&gt;&gt; p.name() # 进程名称
'python3.6'
&gt;&gt;&gt; p.exe() # 进程exe路径
'/Users/michael/anaconda3/bin/python3.6'
&gt;&gt;&gt; p.cwd() # 进程工作目录
'/Users/michael'
&gt;&gt;&gt; p.cmdline() # 进程启动的命令行
['python3']
&gt;&gt;&gt; p.ppid() # 父进程ID
3765
&gt;&gt;&gt; p.parent() # 父进程
&lt;psutil.Process(pid=3765, name='bash') at 4503144040&gt;
&gt;&gt;&gt; p.children() # 子进程列表
[]
&gt;&gt;&gt; p.status() # 进程状态
'running'
&gt;&gt;&gt; p.username() # 进程用户名
'michael'
&gt;&gt;&gt; p.create_time() # 进程创建时间
1511052731.120333
&gt;&gt;&gt; p.terminal() # 进程终端
'/dev/ttys002'
&gt;&gt;&gt; p.cpu_times() # 进程使用的CPU时间
pcputimes(user=0.081150144, system=0.053269812, children_user=0.0, children_system=0.0)
&gt;&gt;&gt; p.memory_info() # 进程使用的内存
pmem(rss=8310784, vms=2481725440, pfaults=3207, pageins=18)
&gt;&gt;&gt; p.open_files() # 进程打开的文件
[]
&gt;&gt;&gt; p.connections() # 进程相关网络连接
[]
&gt;&gt;&gt; p.num_threads() # 进程的线程数量
1
&gt;&gt;&gt; p.threads() # 所有线程信息
[pthread(id=1, user_time=0.090318, system_time=0.062736)]
&gt;&gt;&gt; p.environ() # 进程环境变量
{'SHELL': '/bin/bash', 'PATH': '/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:...', 'PWD': '/Users/michael', 'LANG': 'zh_CN.UTF-8', ...}
&gt;&gt;&gt; p.terminate() # 结束进程
Terminated: 15 &lt;-- 自己把自己结束了
</code></pre>
<p>和获取网络连接类似，获取一个root用户的进程需要root权限，启动Python交互环境或者.py文件时，需要sudo权限。</p>
<p>psutil还提供了一个test()函数，可以模拟出ps命令的效果：</p>
<pre><code>$ sudo python3
Password: ******
Python 3.6.3 ... on darwin
Type &quot;help&quot;, ... for more information.
&gt;&gt;&gt; import psutil
&gt;&gt;&gt; psutil.test()
USER         PID %MEM     VSZ     RSS TTY           START    TIME  COMMAND
root           0 24.0 74270628 2016380 ?             Nov18   40:51  kernel_task
root           1  0.1 2494140    9484 ?             Nov18   01:39  launchd
root          44  0.4 2519872   36404 ?             Nov18   02:02  UserEventAgent
root          45    ? 2474032    1516 ?             Nov18   00:14  syslogd
root          47  0.1 2504768    8912 ?             Nov18   00:03  kextd
root          48  0.1 2505544    4720 ?             Nov18   00:19  fseventsd
_appleeven    52  0.1 2499748    5024 ?             Nov18   00:00  appleeventsd
root          53  0.1 2500592    6132 ?             Nov18   00:02  configd
...
</code></pre>
<p>psutil还可以获取用户信息、Windows服务等很多有用的系统信息，具体请参考psutil的官网：https://github.com/giampaolo/psutil</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Python - 常用内建模块]]></title>
        <id>https://lixin-scut.github.io//post/python-chang-yong-nei-jian-mo-kuai</id>
        <link href="https://lixin-scut.github.io//post/python-chang-yong-nei-jian-mo-kuai">
        </link>
        <updated>2020-05-13T02:36:20.000Z</updated>
        <content type="html"><![CDATA[<h2 id="datetime">datetime</h2>
<p>datetime是Python处理日期和时间的标准库。</p>
<h3 id="获取当前日期和时间">获取当前日期和时间</h3>
<p>获取当前日期和时间：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; now = datetime.now() # 获取当前datetime
&gt;&gt;&gt; print(now)
2015-05-18 16:28:07.198690
&gt;&gt;&gt; print(type(now))
&lt;class 'datetime.datetime'&gt;
</code></pre>
<p>注意到<strong>datetime是模块，datetime模块还包含一个datetime类</strong>，通过from datetime import datetime导入的才是datetime这个类。</p>
<p><strong>如果仅导入import datetime，则必须引用全名datetime.datetime</strong>。</p>
<p><strong>datetime.now()返回当前日期和时间，其类型是datetime。</strong></p>
<h3 id="获取指定日期和时间">获取指定日期和时间</h3>
<p>要指定某个日期和时间，我们直接用参数构造一个datetime：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime
&gt;&gt;&gt; print(dt)

2015-04-19 12:20:00
</code></pre>
<h3 id="datetime转换为timestamp">datetime转换为timestamp</h3>
<p>在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前<strong>时间就是相对于epoch time的秒数，称为timestamp</strong>。</p>
<p>你可以认为：</p>
<pre><code>timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00
</code></pre>
<p>对应的北京时间是：</p>
<pre><code>timestamp = 0 = 1970-1-1 08:00:00 UTC+8:00
</code></pre>
<p>可见<strong>timestamp的值与时区毫无关系，因为timestamp一旦确定，其UTC时间就确定了</strong>，转换到任意时区的时间也是完全确定的，这就是为什么计算机存储的当前时间是以timestamp表示的，因为<strong>全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）</strong>。</p>
<p>把一个datetime类型转换为timestamp只需要简单调用<strong>timestamp()方法</strong>：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime
&gt;&gt;&gt; dt.timestamp() # 把datetime转换为timestamp
1429417200.0
</code></pre>
<p>注意Python的<strong>timestamp是一个浮点数</strong>。如果<strong>有小数位，小数位表示毫秒数。</strong></p>
<p><strong>某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数</strong>，这种情况下<strong>只需要把timestamp除以1000</strong>就得到Python的浮点表示方法。</p>
<h3 id="timestamp转换为datetime">timestamp转换为datetime</h3>
<p>要把timestamp转换为datetime，使用datetime提供的<strong>fromtimestamp()方法</strong>：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; t = 1429417200.0
&gt;&gt;&gt; print(datetime.fromtimestamp(t))
2015-04-19 12:20:00
</code></pre>
<p>注意到timestamp是一个浮点数，它没有时区的概念，而<strong>datetime是有时区的。上述转换是在timestamp和本地时间做转换。</strong></p>
<p>本地时间是指当前操作系统设定的时区。例如北京时区是东8区，则本地时间：</p>
<pre><code>2015-04-19 12:20:00
</code></pre>
<p>实际上就是UTC+8:00时区的时间：</p>
<pre><code>2015-04-19 12:20:00 UTC+8:00
</code></pre>
<p>而此刻的格林威治标准时间与北京时间差了8小时，也就是UTC+0:00时区的时间应该是：</p>
<pre><code>2015-04-19 04:20:00 UTC+0:00
</code></pre>
<p>timestamp也可以直接被转换到UTC标准时区的时间：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; t = 1429417200.0
&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间
2015-04-19 12:20:00
&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间
2015-04-19 04:20:00
</code></pre>
<h3 id="str转换为datetime">str转换为datetime</h3>
<p>很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把str转换为datetime。转换方法是通过<strong>datetime.strptime()</strong> 实现，需要一个日期和时间的格式化字符串：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; cday = datetime.strptime('2015-6-1 18:19:59', '%Y-%m-%d %H:%M:%S')
&gt;&gt;&gt; print(cday)
2015-06-01 18:19:59
</code></pre>
<p><strong>字符串<code>'%Y-%m-%d %H:%M:%S'</code>规定了日期和时间部分的格式</strong>。详细的说明请参考Python文档。</p>
<p>注意转换后的datetime是没有时区信息的。</p>
<h3 id="datetime转换为str">datetime转换为str</h3>
<p>如果已经有了datetime对象，要把它格式化为字符串显示给用户，就需要转换为str，转换方法是通过<strong>strftime()</strong> 实现的，<strong>同样需要一个日期和时间的格式化字符串</strong>：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; now = datetime.now()
&gt;&gt;&gt; print(now.strftime('%a, %b %d %H:%M'))
Mon, May 05 16:28
</code></pre>
<h3 id="datetime加减">datetime加减</h3>
<p>对日期和时间进行加减实际上就是把datetime往后或往前计算，得到新的datetime。加减可以直接用+和-运算符，不过需要导入timedelta这个类：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta
&gt;&gt;&gt; now = datetime.now()
&gt;&gt;&gt; now
datetime.datetime(2015, 5, 18, 16, 57, 3, 540997)
&gt;&gt;&gt; now + timedelta(hours=10)
datetime.datetime(2015, 5, 19, 2, 57, 3, 540997)
&gt;&gt;&gt; now - timedelta(days=1)
datetime.datetime(2015, 5, 17, 16, 57, 3, 540997)
&gt;&gt;&gt; now + timedelta(days=2, hours=12)
datetime.datetime(2015, 5, 21, 4, 57, 3, 540997)
</code></pre>
<p>可见，使用timedelta你可以很容易地算出前几天和后几天的时刻。</p>
<h3 id="本地时间转换为utc时间">本地时间转换为UTC时间</h3>
<p>本地时间是指系统设定时区的时间，例如北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间。</p>
<p>一个datetime类型有一个<strong>时区属性tzinfo，但是默认为None</strong>，所以无法区分这个datetime到底是哪个时区，除非强行给datetime设置一个时区：</p>
<pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta, timezone
&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00
&gt;&gt;&gt; now = datetime.now()
&gt;&gt;&gt; now
datetime.datetime(2015, 5, 18, 17, 2, 10, 871012)

&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00
&gt;&gt;&gt; dt
datetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800)))
</code></pre>
<p>如果系统时区恰好是UTC+8:00，那么上述代码就是正确的，否则，不能强制设置为UTC+8:00时区。</p>
<h3 id="时区转换">时区转换</h3>
<p>我们可以先通过utcnow()拿到当前的UTC时间，再转换为任意时区的时间：</p>
<pre><code>

# 拿到UTC时间，并强制设置时区为UTC+0:00:
&gt;&gt;&gt; utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)
&gt;&gt;&gt; print(utc_dt)
2015-05-18 09:05:12.377316+00:00

# astimezone()将转换时区为北京时间:
&gt;&gt;&gt; bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))
&gt;&gt;&gt; print(bj_dt)
2015-05-18 17:05:12.377316+08:00

# astimezone()将转换时区为东京时间:
&gt;&gt;&gt; tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=9)))
&gt;&gt;&gt; print(tokyo_dt)
2015-05-18 18:05:12.377316+09:00

# astimezone()将bj_dt转换时区为东京时间:
&gt;&gt;&gt; tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=9)))
&gt;&gt;&gt; print(tokyo_dt2)
2015-05-18 18:05:12.377316+09:00

</code></pre>
<p>时区转换的关键在于，拿到一个datetime时，要获知其正确的时区，然后强制设置时区，作为基准时间。</p>
<p>利用带时区的datetime，通过astimezone()方法，可以转换到任意时区。</p>
<p>注：不是必须从UTC+0:00时区转换到其他时区，任何带时区的datetime都可以正确转换，例如上述bj_dt到tokyo_dt的转换。</p>
<h3 id="小结">小结</h3>
<p>datetime表示的时间需要时区信息才能确定一个特定的时间，否则只能视为本地时间。</p>
<p>如果要存储datetime，最佳方法是将其转换为timestamp再存储，因为timestamp的值与时区完全无关。</p>
<h2 id="collections">collections</h2>
<p>collections是Python内建的一个集合模块，提供了许多有用的集合类。</p>
<h3 id="namedtuple">namedtuple</h3>
<p>我们知道tuple可以表示不变集合，例如，一个点的二维坐标就可以表示成：</p>
<pre><code>&gt;&gt;&gt; p = (1, 2)
</code></pre>
<p>但是，看到(1, 2)，<strong>很难看出这个tuple是用来表示一个坐标的</strong>。</p>
<p>定义一个class又小题大做了，这时，namedtuple就派上了用场：</p>
<pre><code>&gt;&gt;&gt; from collections import namedtuple
&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])
&gt;&gt;&gt; p = Point(1, 2)
&gt;&gt;&gt; p.x
1
&gt;&gt;&gt; p.y
2
</code></pre>
<p>namedtuple<strong>是一个函数</strong>，它用来<strong>创建一个自定义的tuple对象</strong>，并且<strong>规定了tuple元素的个数</strong>，并可以<strong>用属性而不是索引来引用tuple的某个元素</strong>。</p>
<p>这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。</p>
<p>可以验证<strong>创建的Point对象是tuple的一种子类</strong>：</p>
<pre><code>&gt;&gt;&gt; isinstance(p, Point)
True
&gt;&gt;&gt; isinstance(p, tuple)
True
</code></pre>
<p>类似的，如果要用坐标和半径表示一个圆，也可以用namedtuple定义：</p>
<pre><code>
# namedtuple('名称', [属性list]):
Circle = namedtuple('Circle', ['x', 'y', 'r'])

</code></pre>
<h3 id="deque">deque</h3>
<p>使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。</p>
<p>deque是为了高效实现<strong>插入和删除操作的双向列表</strong>，<strong>适合用于队列和栈</strong>：</p>
<pre><code>&gt;&gt;&gt; from collections import deque
&gt;&gt;&gt; q = deque(['a', 'b', 'c'])
&gt;&gt;&gt; q.append('x')
&gt;&gt;&gt; q.appendleft('y')
&gt;&gt;&gt; q
deque(['y', 'a', 'b', 'c', 'x'])
</code></pre>
<p>deque除了实现list的<strong>append()和pop()</strong> 外，还支持<strong>appendleft()和popleft()</strong>，这样就可以非常高效地往头部添加或删除元素。</p>
<h3 id="defaultdict">defaultdict</h3>
<p>使用dict时，如果引用的Key不存在，就会抛出KeyError。<strong>如果希望key不存在时，返回一个默认值，就可以用defaultdict</strong>：</p>
<pre><code>&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')
&gt;&gt;&gt; dd['key1'] = 'abc'
&gt;&gt;&gt; dd['key1'] # key1存在
'abc'
&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值
'N/A'
</code></pre>
<p>注意默认值是<strong>调用函数返回</strong>的，而函数在创建defaultdict对象时传入。</p>
<p>除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。</p>
<h3 id="ordereddict">OrderedDict</h3>
<p>使用<strong>dict时，Key是无序的</strong>。在对dict做迭代时，我们无法确定Key的顺序。</p>
<p>如果要<strong>保持Key的顺序，可以用OrderedDict</strong>：</p>
<pre><code>&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])
&gt;&gt;&gt; d # dict的Key是无序的
{'a': 1, 'c': 3, 'b': 2}
&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])
&gt;&gt;&gt; od # OrderedDict的Key是有序的
OrderedDict([('a', 1), ('b', 2), ('c', 3)])
</code></pre>
<p>注意，OrderedDict的Key会<strong>按照插入的顺序排列，不是Key本身排序</strong>：</p>
<pre><code>&gt;&gt;&gt; od = OrderedDict()
&gt;&gt;&gt; od['z'] = 1
&gt;&gt;&gt; od['y'] = 2
&gt;&gt;&gt; od['x'] = 3
&gt;&gt;&gt; list(od.keys()) # 按照插入的Key的顺序返回
['z', 'y', 'x']
</code></pre>
<p>OrderedDict可以实现一个<strong>FIFO（先进先出）的dict</strong>，当<strong>容量超出限制时，先删除最早添加的Key</strong>：</p>
<pre><code>from collections import OrderedDict

class LastUpdatedOrderedDict(OrderedDict):

    def __init__(self, capacity):
        super(LastUpdatedOrderedDict, self).__init__()
        self._capacity = capacity

    def __setitem__(self, key, value):
        containsKey = 1 if key in self else 0
        if len(self) - containsKey &gt;= self._capacity:
            last = self.popitem(last=False)
            print('remove:', last)
        if containsKey:
            del self[key]
            print('set:', (key, value))
        else:
            print('add:', (key, value))
        OrderedDict.__setitem__(self, key, value)
</code></pre>
<h3 id="chainmap">ChainMap</h3>
<p>ChainMap可以把<strong>一组dict</strong>串起来并组成一个<strong>逻辑上的dict</strong>。ChainMap<strong>本身也是一个dict</strong>，但是查找的时候，会<strong>按照顺序在内部的dict依次查找</strong>。</p>
<p>什么时候使用ChainMap最合适？举个例子：应用程序往往都需要传入参数，参数可以通过命令行传入，可以通过环境变量传入，还可以有默认参数。我们可以<strong>用ChainMap实现参数的优先级查找</strong>，即先查命令行参数，如果没有传入，再查环境变量，如果没有，就使用默认参数。</p>
<p>下面的代码演示了如何查找user和color这两个参数：</p>
<pre><code>
from collections import ChainMap
import os, argparse

# 构造缺省参数:
defaults = {
    'color': 'red',
    'user': 'guest'
}

# 构造命令行参数:
parser = argparse.ArgumentParser()
parser.add_argument('-u', '--user')
parser.add_argument('-c', '--color')
namespace = parser.parse_args()
command_line_args = { k: v for k, v in vars(namespace).items() if v }

# 组合成ChainMap:
combined = ChainMap(command_line_args, os.environ, defaults)

# 打印参数:
print('color=%s' % combined['color'])
print('user=%s' % combined['user'])
没有任何参数时，打印出默认参数：

$ python3 use_chainmap.py 
color=red
user=guest

</code></pre>
<p>当传入命令行参数时，优先使用命令行参数：</p>
<pre><code>$ python3 use_chainmap.py -u bob
color=red
user=bob
</code></pre>
<p>同时传入命令行参数和<strong>环境变量</strong>，命令行参数的优先级较高：</p>
<pre><code># python3 前面的都是环境变量
$ user=admin color=green python3 use_chainmap.py -u bob
color=green
user=bob
</code></pre>
<h3 id="counter">Counter</h3>
<p>Counter是一个简单的计数器，例如，统计字符出现的个数：</p>
<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; c = Counter()
&gt;&gt;&gt; for ch in 'programming':
...     c[ch] = c[ch] + 1
...
&gt;&gt;&gt; c
Counter({'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1})
&gt;&gt;&gt; c.update('hello') # 也可以一次性update
&gt;&gt;&gt; c
Counter({'r': 2, 'o': 2, 'g': 2, 'm': 2, 'l': 2, 'p': 1, 'a': 1, 'i': 1, 'n': 1, 'h': 1, 'e': 1})
</code></pre>
<p><strong>Counter实际上也是dict的一个子类</strong>，上面的结果可以看出每个字符出现的次数。</p>
<h2 id="base64">base64</h2>
<p>Base64是一种<strong>用64个字符来表示任意二进制数据</strong>的方法。</p>
<p>用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。</p>
<p>Base64的原理很简单，首先，准备一个包含64个字符的数组：</p>
<pre><code>['A', 'B', 'C', ... 'a', 'b', 'c', ... '0', '1', ... '+', '/']
</code></pre>
<p>然后，对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589339175185.png" alt=""></p>
<p>这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。</p>
<p>所以，Base64编码会把3字节的二进制数据编码为4字节的文本数据，长度增加33%，好处是编码后的文本数据可以在邮件正文、网页等直接显示。</p>
<p>如果要编码的二进制数据不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号，表示补了多少字节，解码的时候，会自动去掉。</p>
<p>Python内置的base64可以直接进行base64的编解码：</p>
<pre><code>&gt;&gt;&gt; import base64
&gt;&gt;&gt; base64.b64encode(b'binary\x00string')
b'YmluYXJ5AHN0cmluZw=='
&gt;&gt;&gt; base64.b64decode(b'YmluYXJ5AHN0cmluZw==')
b'binary\x00string'
</code></pre>
<p>由于标准的Base64编码后可能出现字符+和/，在URL中就不能直接作为参数，所以又有一种&quot;url safe&quot;的base64编码，其实就是把字符+和/分别变成-和_：</p>
<pre><code>&gt;&gt;&gt; base64.b64encode(b'i\xb7\x1d\xfb\xef\xff')
b'abcd++//'
&gt;&gt;&gt; base64.urlsafe_b64encode(b'i\xb7\x1d\xfb\xef\xff')
b'abcd--__'
&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')
b'i\xb7\x1d\xfb\xef\xff'
</code></pre>
<p>还可以自己定义64个字符的排列顺序，这样就可以自定义Base64编码，不过，通常情况下完全没有必要。</p>
<p>Base64是一种通过查表的编码方法，不能用于加密，即使使用自定义的编码表也不行。</p>
<p>Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。</p>
<p>由于=字符也可能出现在Base64编码中，但=用在URL、Cookie里面会造成歧义，所以，很多Base64编码后会把=去掉：</p>
<pre><code>
# 标准Base64:
'abcd' -&gt; 'YWJjZA=='
# 自动去掉=:
'abcd' -&gt; 'YWJjZA'

</code></pre>
<p>去掉=后怎么解码呢？因为Base64是把3个字节变为4个字节，所以，Base64编码的长度永远是4的倍数，因此，需要加上=把Base64字符串的长度变为4的倍数，就可以正常解码了。</p>
<h2 id="struct">struct</h2>
<p>准确地讲，Python没有专门处理字节的数据类型。但由于b'str'可以表示字节，所以，字节数组＝二进制str。而在C语言中，我们可以很方便地用struct、union来处理字节，以及字节和int，float的转换。</p>
<p>在Python中，比方说要把一个32位无符号整数变成字节，也就是4个长度的bytes，你得配合位运算符这么写：</p>
<pre><code>&gt;&gt;&gt; n = 10240099
&gt;&gt;&gt; b1 = (n &amp; 0xff000000) &gt;&gt; 24
&gt;&gt;&gt; b2 = (n &amp; 0xff0000) &gt;&gt; 16
&gt;&gt;&gt; b3 = (n &amp; 0xff00) &gt;&gt; 8
&gt;&gt;&gt; b4 = n &amp; 0xff
&gt;&gt;&gt; bs = bytes([b1, b2, b3, b4])
&gt;&gt;&gt; bs
b'\x00\x9c@c'
</code></pre>
<p>非常麻烦。如果换成浮点数就无能为力了。</p>
<p>好在Python提供了一个struct模块来<strong>解决bytes和其他二进制数据类型的转换</strong>。</p>
<p>struct的pack函数把任意数据类型变成bytes：</p>
<pre><code>&gt;&gt;&gt; import struct
&gt;&gt;&gt; struct.pack('&gt;I', 10240099)
b'\x00\x9c@c'
</code></pre>
<p>pack的第一个参数是处理指令，'&gt;I'的意思是：</p>
<p><code>&gt;</code>表示字节顺序是big-endian，也就是网络序，I表示4字节无符号整数。</p>
<p>后面的参数个数要和处理指令一致。</p>
<p>unpack把bytes变成相应的数据类型：</p>
<pre><code>&gt;&gt;&gt; struct.unpack('&gt;IH', b'\xf0\xf0\xf0\xf0\x80\x80')
(4042322160, 32896)
</code></pre>
<p>根据&gt;IH的说明，后面的bytes依次变为I：4字节无符号整数和H：2字节无符号整数。</p>
<p>所以，尽管Python不适合编写底层操作字节流的代码，但在对性能要求不高的地方，利用struct就方便多了。</p>
<p>struct模块定义的数据类型可以参考Python官方文档：</p>
<p>https://docs.python.org/3/library/struct.html#format-characters</p>
<p>Windows的位图文件（.bmp）是一种非常简单的文件格式，我们来用struct分析一下。</p>
<p>首先找一个bmp文件。</p>
<p>读入前30个字节来分析：</p>
<pre><code>
&gt;&gt;&gt; s = b'\x42\x4d\x38\x8c\x0a\x00\x00\x00\x00\x00\x36\x00\x00\x00\x28\x00\x00\x00\x80\x02\x00\x00\x68\x01\x00\x00\x01\x00\x18\x00'

</code></pre>
<p>BMP格式采用小端方式存储数据，文件头的结构按顺序如下：</p>
<p>两个字节：'BM'表示Windows位图，'BA'表示OS/2位图； 一个4字节整数：表示位图大小； 一个4字节整数：保留位，始终为0； 一个4字节整数：实际图像的偏移量； 一个4字节整数：Header的字节数； 一个4字节整数：图像宽度； 一个4字节整数：图像高度； 一个2字节整数：始终为1； 一个2字节整数：颜色数。</p>
<p>所以，组合起来用unpack读取：</p>
<pre><code>&gt;&gt;&gt; struct.unpack('&lt;ccIIIIIIHH', s)
</code></pre>
<p>(b'B', b'M', 691256, 0, 54, 40, 640, 360, 1, 24)<br>
结果显示，b'B'、b'M'说明是Windows位图，位图大小为640x360，颜色数为24。</p>
<h2 id="hashlib">hashlib</h2>
<p>摘要算法简介</p>
<p>摘要算法在很多地方都有广泛的应用。要注意摘要算法不是加密算法，不能用于加密（因为无法通过摘要反推明文），只能用于防篡改，但是它的单向计算特性决定了可以在不存储明文口令的情况下验证用户口令。</p>
<p>Python的<strong>hashlib提供了常见的摘要算法，如MD5，SHA1等等</strong>。</p>
<p>什么是摘要算法呢？摘要算法又称<strong>哈希算法、散列算法</strong>。它通过一个函数，把<strong>任意长度的数据转换为一个长度固定的数据串</strong>（通常用16进制的字符串表示）。</p>
<p>举个例子，你写了一篇文章，内容是一个字符串'how to use python hashlib - by Michael'，并附上这篇文章的摘要是'2d73d4f15c0db7f5ecb321b6a65e5d6d'。如果有人篡改了你的文章，并发表为'how to use python hashlib - by Bob'，你可以一下子指出Bob篡改了你的文章，因为根据'how to use python hashlib - by Bob'计算出的摘要不同于原始文章的摘要。</p>
<p>可见，<strong>摘要算法就是通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是为了发现原始数据是否被人篡改过</strong>。</p>
<p>摘要算法之所以能指出数据是否被篡改过，就是因为<strong>摘要函数是一个单向函数，计算f(data)很容易，但通过digest反推data却非常困难</strong>。而且，<strong>对原始数据做一个bit的修改，都会导致计算出的摘要完全不同</strong>。</p>
<p>我们以常见的摘要算法MD5为例，计算出一个字符串的MD5值：</p>
<pre><code>import hashlib

md5 = hashlib.md5()
md5.update('how to use md5 in python hashlib?'.encode('utf-8'))
print(md5.hexdigest())
</code></pre>
<p>计算结果如下：</p>
<pre><code>d26a53750bc40b38b65a520292f69306
</code></pre>
<p>如果数据量很大，可以分块多次调用update()，最后计算的结果是一样的：</p>
<pre><code>import hashlib

md5 = hashlib.md5()
md5.update('how to use md5 in '.encode('utf-8'))
md5.update('python hashlib?'.encode('utf-8'))
print(md5.hexdigest())
</code></pre>
<p>改动一个字母，计算的结果完全不同。</p>
<p>MD5是最常见的摘要算法，速度很快，生成结果是固定的128 bit字节，通常用一个32位的16进制字符串表示。</p>
<p>另一种常见的摘要算法是SHA1，调用SHA1和调用MD5完全类似：</p>
<pre><code>import hashlib

sha1 = hashlib.sha1()
sha1.update('how to use sha1 in '.encode('utf-8'))
sha1.update('python hashlib?'.encode('utf-8'))
print(sha1.hexdigest())
</code></pre>
<p>SHA1的结果是160 bit字节，通常用一个40位的16进制字符串表示。</p>
<p>比SHA1更安全的算法是SHA256和SHA512，不过越安全的算法不仅越慢，而且摘要长度更长。</p>
<p>有没有可能<strong>两个不同的数据通过某个摘要算法得到了相同的摘要</strong>？完全有可能，因为任何摘要算法都是<strong>把无限多的数据集合映射到一个有限的集合中</strong>。这种情况称为碰撞，比如Bob试图根据你的摘要反推出一篇文章'how to learn hashlib in python - by Bob'，并且这篇文章的摘要恰好和你的文章完全一致，这种情况也并非不可能出现，但是非常非常困难。</p>
<p>摘要算法应用</p>
<p>摘要算法能应用到什么地方？举个常用例子：</p>
<p>任何允许用户登录的网站都会存储用户登录的用户名和口令。如何存储用户名和口令呢？方法是存到数据库表中：</p>
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">password</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">michael</td>
<td style="text-align:center">123456</td>
</tr>
<tr>
<td style="text-align:center">bob</td>
<td style="text-align:center">abc999</td>
</tr>
<tr>
<td style="text-align:center">alice</td>
<td style="text-align:center">alice2008</td>
</tr>
</tbody>
</table>
<p>如果以明文保存用户口令，如果数据库泄露，所有用户的口令就落入黑客的手里。此外，网站运维人员是可以访问数据库的，也就是能获取到所有用户的口令。</p>
<p>正确的保存口令的方式是<strong>不存储用户的明文口令，而是存储用户口令的摘要</strong>，比如MD5：</p>
<table>
<thead>
<tr>
<th style="text-align:center">username</th>
<th style="text-align:center">password</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">michael</td>
<td style="text-align:center">e10adc3949ba59abbe56e057f20f883e</td>
</tr>
<tr>
<td style="text-align:center">bob</td>
<td style="text-align:center">878ef96e86145580c38c87f0410ad153</td>
</tr>
<tr>
<td style="text-align:center">alice</td>
<td style="text-align:center">99b1c2188db85afee403b1536010c2c9</td>
</tr>
</tbody>
</table>
<p>当用户登录时，首先<strong>计算用户输入的明文口令的MD5，然后和数据库存储的MD5对比，如果一致，说明口令输入正确，如果不一致，口令肯定错误</strong>。</p>
<p>存储MD5的好处是即使运维人员能访问数据库，也无法获知用户的明文口令。</p>
<p>采用MD5存储口令是否就一定安全呢？也不一定。假设你是一个黑客，已经拿到了存储MD5口令的数据库，如何通过MD5反推用户的明文口令呢？暴力破解费事费力</p>
<p>考虑这么个情况，很多用户喜欢用123456，888888，password这些简单的口令，于是，黑客可以事先计算出这些常用口令的MD5值，得到一个反推表：</p>
<p>'e10adc3949ba59abbe56e057f20f883e': '123456'<br>
'21218cca77804d2ba1922c33e0151105': '888888'<br>
'5f4dcc3b5aa765d61d8327deb882cf99': 'password'<br>
这样，无需破解，只需要对比数据库的MD5，黑客就获得了使用常用口令的用户账号。</p>
<p>对于用户来讲，当然不要使用过于简单的口令。但是，我们能否在程序设计上对简单口令加强保护呢？</p>
<p>由于常用口令的MD5值很容易被计算出来，所以，要确保存储的用户口令不是那些已经被计算出来的常用口令的MD5，这一方法<strong>通过对原始口令加一个复杂字符串来实现，俗称“加盐”：</strong></p>
<pre><code>def calc_md5(password):
    return get_md5(password + 'the-Salt')
</code></pre>
<p>经过Salt处理的MD5口令，<strong>只要Salt不被黑客知道，即使用户输入简单口令，也很难通过MD5反推明文口令</strong>。</p>
<p>但是如果有两个用户都使用了相同的简单口令比如123456，在数据库中，将存储两条相同的MD5值，这说明这两个用户的口令是一样的。有没有办法让使用相同口令的用户存储不同的MD5呢--如果假定用户无法修改登录名，就可以通过把登录名作为Salt的一部分来计算MD5，从而实现相同口令的用户也存储不同的MD5。</p>
<h2 id="hmac">hmac</h2>
<p>通过哈希算法，我们可以验证一段数据是否有效，方法就是对比该数据的哈希值，例如，判断用户口令是否正确，我们用保存在数据库中的password_md5对比计算md5(password)的结果，如果一致，用户输入的口令就是正确的。</p>
<p>为了防止黑客通过彩虹表根据哈希值反推原始口令，在计算哈希的时候，不能仅针对原始输入计算，需要增加一个salt来使得相同的输入也能得到不同的哈希，这样，大大增加了黑客破解的难度。</p>
<p>如果salt是我们自己随机生成的，通常我们计算MD5时采用md5(message + salt)。但实际上，把salt看做一个“口令”，加salt的哈希就是：计算一段message的哈希时，根据不通口令计算出不同的哈希。<strong>要验证哈希值，必须同时提供正确的口令。</strong></p>
<p><strong>这实际上就是Hmac算法：Keyed-Hashing for Message Authentication</strong>。它通过一个标准算法，在计算哈希的过程中，<strong>把key混入计算过程中。</strong></p>
<p>和我们自定义的加salt算法不同，Hmac算法针对所有哈希算法都通用，无论是MD5还是SHA-1。采用Hmac替代我们自己的salt算法，可以使程序算法更标准化，也更安全。</p>
<p>Python自带的hmac模块实现了标准的Hmac算法。我们来看看如何使用hmac实现带key的哈希。</p>
<p>我们首先需要准备待计算的原始消息message，随机key，哈希算法，这里采用MD5，使用hmac的代码如下：</p>
<pre><code>&gt;&gt;&gt; import hmac
&gt;&gt;&gt; message = b'Hello, world!'
&gt;&gt;&gt; key = b'secret'
&gt;&gt;&gt; h = hmac.new(key, message, digestmod='MD5')
&gt;&gt;&gt; # 如果消息很长，可以多次调用h.update(msg)
&gt;&gt;&gt; h.hexdigest()
'fa4ee7d173f2d97ee79022d1a7355bcf'
</code></pre>
<p>可见使用hmac和普通hash算法非常类似。hmac输出的长度和原始哈希算法的长度一致。需要注意<strong>传入的key和message都是bytes类型，str类型需要首先编码为bytes。</strong></p>
<h2 id="itertools">itertools</h2>
<p>Python的内建模块itertools提供了非常有用的用于<strong>操作迭代对象</strong>的函数。</p>
<p>itertools模块提供的全部是<strong>处理迭代功能的函数</strong>，它们的<strong>返回值不是list，而是Iterator</strong>，<strong>只有用for循环迭代的时候才真正计算</strong>。</p>
<p>首先，我们看看itertools提供的几个“无限”迭代器：</p>
<p><strong>count()</strong></p>
<pre><code>&gt;&gt;&gt; import itertools
&gt;&gt;&gt; natuals = itertools.count(1)
&gt;&gt;&gt; for n in natuals:
...     print(n)
...
1
2
3
...
</code></pre>
<p>因为count()会创建一个<strong>无限的迭代器</strong>，所以上述代码会打印出自然数序列，根本停不下来，<strong>只能按Ctrl+C退出</strong>。</p>
<p><strong>cycle()</strong> 会把传入的一个序列无限重复下去：</p>
<pre><code>&gt;&gt;&gt; import itertools
&gt;&gt;&gt; cs = itertools.cycle('ABC') # 注意字符串也是序列的一种
&gt;&gt;&gt; for c in cs:
...     print(c)
...
'A'
'B'
'C'
'A'
'B'
'C'
...
</code></pre>
<p>同样停不下来。</p>
<p><strong>repeat()</strong> 负责把一个元素无限重复下去，不过<strong>如果提供第二个参数就可以限定重复次数</strong>：</p>
<pre><code>&gt;&gt;&gt; ns = itertools.repeat('A', 3)
&gt;&gt;&gt; for n in ns:
...     print(n)
...
A
A
A
</code></pre>
<p>**无限序列只有在for迭代时才会无限地迭代下去，如果只是创建了一个迭代对象，它不会事先把无限个元素生成出来，**事实上也不可能在内存中创建无限多个元素。</p>
<p>无限序列虽然可以无限迭代下去，但是通常我们会<strong>通过takewhile()等函数根据条件判断来截取出一个有限的序列</strong>：</p>
<pre><code>&gt;&gt;&gt; natuals = itertools.count(1)
&gt;&gt;&gt; ns = itertools.takewhile(lambda x: x &lt;= 10, natuals)
&gt;&gt;&gt; list(ns)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>
<p>itertools提供的几个迭代器操作函数更加有用：</p>
<h3 id="chain">chain()</h3>
<p>chain()可以把一组迭代对象串联起来，形成一个更大的迭代器：</p>
<pre><code>
&gt;&gt;&gt; for c in itertools.chain('ABC', 'XYZ'):
...     print(c)
# 迭代效果：'A' 'B' 'C' 'X' 'Y' 'Z'
</code></pre>
<h3 id="groupby">groupby()</h3>
<p>groupby()把迭代器中相邻的重复元素挑出来放在一起：</p>
<pre><code>&gt;&gt;&gt; for key, group in itertools.groupby('AAABBBCCAAA'):
...     print(key, list(group))
...
A ['A', 'A', 'A']
B ['B', 'B', 'B']
C ['C', 'C']
A ['A', 'A', 'A']
</code></pre>
<p>实际上<strong>挑选规则是通过函数完成的</strong>，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是在一组的，而<strong>函数返回值作为组的key</strong>。如果我们要忽略大小写分组，就可以让元素'A'和'a'都返回相同的key：</p>
<pre><code>&gt;&gt;&gt; for key, group in itertools.groupby('AaaBBbcCAAa', lambda c: c.upper()):
...     print(key, list(group))
...

A ['A', 'a', 'a']
B ['B', 'B', 'b']
C ['c', 'C']
A ['A', 'A', 'a']
</code></pre>
<h2 id="contextlib">contextlib</h2>
<p>在Python中，读写文件这样的资源要特别注意，必须在使用完毕后正确关闭它们。正确关闭文件资源的一个方法是使用try...finally：</p>
<pre><code>try:
    f = open('/path/to/file', 'r')
    f.read()
finally:
    if f:
        f.close()
</code></pre>
<p>写try...finally非常繁琐。Python的with语句允许我们非常方便地使用资源，而不必担心资源没有关闭，所以上面的代码可以简化为：</p>
<pre><code>with open('/path/to/file', 'r') as f:
    f.read()
</code></pre>
<p>并不是只有open()函数返回的fp对象才能使用with语句。<strong>实际上，任何对象，只要正确实现了上下文管理，就可以用于with语句</strong>。</p>
<p>实现<strong>上下文管理</strong>是通过<code>__enter__和__exit__</code>这两个方法实现的。例如，下面的class实现了这两个方法：</p>
<pre><code>class Query(object):

    def __init__(self, name):
        self.name = name

    def __enter__(self):
        print('Begin')
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type:
            print('Error')
        else:
            print('End')
    
    def query(self):
        print('Query info about %s...' % self.name)
</code></pre>
<p>这样我们就可以把自己写的资源对象用于with语句：</p>
<pre><code>with Query('Bob') as q:
    q.query()
</code></pre>
<h3 id="contextmanager">@contextmanager</h3>
<p>编写__enter__和__exit__仍然很繁琐，因此Python的标准库contextlib提供了更简单的写法，上面的代码可以改写如下：</p>
<pre><code>from contextlib import contextmanager

class Query(object):

    def __init__(self, name):
        self.name = name

    def query(self):
        print('Query info about %s...' % self.name)

@contextmanager
def create_query(name):
    print('Begin')
    q = Query(name)
    yield q
    print('End')
</code></pre>
<p>@contextmanager这个decorator接受一个generator，用yield语句把<code>with ... as var</code>把变量输出出去，然后，with语句就可以正常地工作了：</p>
<pre><code>with create_query('Bob') as q:
    q.query()
</code></pre>
<p>很多时候，我们希望在某段代码执行前后自动执行特定代码，也可以用@contextmanager实现。例如：</p>
<pre><code>@contextmanager
def tag(name):
    print(&quot;&lt;%s&gt;&quot; % name)
    yield
    print(&quot;&lt;/%s&gt;&quot; % name)

with tag(&quot;h1&quot;):
    print(&quot;hello&quot;)
    print(&quot;world&quot;)
</code></pre>
<p>上述代码执行结果为：</p>
<pre><code>&lt;h1&gt;
hello
world
&lt;/h1&gt;
</code></pre>
<p>代码的执行顺序是：</p>
<ol>
<li>with语句首先执行yield之前的语句，因此打印出<code>&lt;h1&gt;</code>；</li>
<li>yield调用会执行with语句内部的所有语句，因此打印出hello和world；</li>
<li>最后执行yield之后的语句，打印出<code>&lt;/h1&gt;</code>。<br>
因此，@contextmanager让我们通过编写generator来简化上下文管理。</li>
</ol>
<h3 id="closing">@closing</h3>
<p>如果一个对象没有实现上下文，我们就不能把它用于with语句。这个时候，可以用<strong>closing()来把该对象变为上下文对象</strong>。例如，用with语句使用urlopen()：</p>
<pre><code>from contextlib import closing
from urllib.request import urlopen

with closing(urlopen('https://www.python.org')) as page:
    for line in page:
        print(line)
</code></pre>
<p><strong>closing也是一个经过@contextmanager装饰的generator</strong>，这个generator编写起来其实非常简单：</p>
<pre><code>@contextmanager
def closing(thing):
    try:
        yield thing
    finally:
        thing.close()
</code></pre>
<p>它的作用就是把任意对象变为上下文对象，并支持with语句。</p>
<h2 id="urllib">urllib</h2>
<p>urllib提供了一系列用于<strong>操作URL的功能</strong>。</p>
<h3 id="get">Get</h3>
<p>urllib的request模块可以非常方便地抓取URL内容，也就是<strong>发送一个GET请求到指定的页面，然后返回HTTP的响应</strong>：</p>
<p>例如，对豆瓣的一个URLhttps://api.douban.com/v2/book/2129650进行抓取，并返回响应：</p>
<pre><code>from urllib import request

with request.urlopen('https://api.douban.com/v2/book/2129650') as f:
    data = f.read()
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', data.decode('utf-8'))
</code></pre>
<p>可以看到HTTP响应的头和JSON数据：</p>
<pre><code>Status: 200 OK
Server: nginx
Date: Tue, 26 May 2015 10:02:27 GMT
Content-Type: application/json; charset=utf-8
Content-Length: 2049
Connection: close
Expires: Sun, 1 Jan 2006 01:00:00 GMT
Pragma: no-cache
Cache-Control: must-revalidate, no-cache, private
X-DAE-Node: pidl1
Data: {&quot;rating&quot;:{&quot;max&quot;:10,&quot;numRaters&quot;:16,&quot;average&quot;:&quot;7.4&quot;,&quot;min&quot;:0},&quot;subtitle&quot;:&quot;&quot;,&quot;author&quot;:[&quot;廖雪峰编著&quot;],&quot;pubdate&quot;:&quot;2007-6&quot;,...}
</code></pre>
<p>如果我们要想模拟浏览器发送GET请求，就需要使用Request对象，通过往Request对象添加HTTP头，我们就可以把请求伪装成浏览器。例如，模拟iPhone 6去请求豆瓣首页：</p>
<pre><code>from urllib import request

req = request.Request('http://www.douban.com/')
req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')
with request.urlopen(req) as f:
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', f.read().decode('utf-8'))
</code></pre>
<p>这样豆瓣会返回适合iPhone的移动版网页：</p>
<pre><code>...
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0&quot;&gt;
    &lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt;
    &lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;57x57&quot; href=&quot;http://img4.douban.com/pics/cardkit/launcher/57.png&quot; /&gt;
...
</code></pre>
<h3 id="post">Post</h3>
<p>如果要以POST发送一个请求，<strong>只需要把参数data以bytes形式传入</strong>。</p>
<p>我们模拟一个微博登录，先读取登录的邮箱和口令，然后按照weibo.cn的登录页的格式以username=xxx&amp;password=xxx的编码传入：</p>
<pre><code>from urllib import request, parse

print('Login to weibo.cn...')
email = input('Email: ')
passwd = input('Password: ')
login_data = parse.urlencode([
    ('username', email),
    ('password', passwd),
    ('entry', 'mweibo'),
    ('client_id', ''),
    ('savestate', '1'),
    ('ec', ''),
    ('pagerefer', 'https://passport.weibo.cn/signin/welcome?entry=mweibo&amp;r=http%3A%2F%2Fm.weibo.cn%2F')
])

req = request.Request('https://passport.weibo.cn/sso/login')
req.add_header('Origin', 'https://passport.weibo.cn')
req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')
req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http%3A%2F%2Fm.weibo.cn%2F')

with request.urlopen(req, data=login_data.encode('utf-8')) as f:
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', f.read().decode('utf-8'))
</code></pre>
<p>如果登录成功，我们获得的响应如下：</p>
<pre><code>Status: 200 OK
Server: nginx/1.2.0
...
Set-Cookie: SSOLoginState=1432620126; path=/; domain=weibo.cn
...
Data: {&quot;retcode&quot;:20000000,&quot;msg&quot;:&quot;&quot;,&quot;data&quot;:{...,&quot;uid&quot;:&quot;1658384301&quot;}}
</code></pre>
<p>如果登录失败，我们获得的响应如下：</p>
<pre><code>...
Data: {&quot;retcode&quot;:50011015,&quot;msg&quot;:&quot;\u7528\u6237\u540d\u6216\u5bc6\u7801\u9519\u8bef&quot;,&quot;data&quot;:{&quot;username&quot;:&quot;example@python.org&quot;,&quot;errline&quot;:536}}
Handler
</code></pre>
<p>如果还需要更复杂的控制，比如<strong>通过一个Proxy去访问网站，我们需要利用ProxyHandler来处理</strong>，示例代码如下：</p>
<pre><code>proxy_handler = urllib.request.ProxyHandler({'http': 'http://www.example.com:3128/'})
proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()
proxy_auth_handler.add_password('realm', 'host', 'username', 'password')
opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)
with opener.open('http://www.example.com/login.html') as f:
    pass
</code></pre>
<p><strong>小结</strong></p>
<p>urllib提供的功能就是利用程序去执行各种HTTP请求。<strong>如果要模拟浏览器完成特定功能，需要把请求伪装成浏览器。伪装的方法是先监控浏览器发出的请求，再根据浏览器的请求头来伪装，User-Agent头就是用来标识浏览器的</strong>。</p>
<h2 id="xml">XML</h2>
<p>ML虽然比JSON复杂，在Web中应用也不如以前多了，不过仍有很多地方在用，所以，有必要了解如何操作XML。</p>
<p>DOM vs SAX</p>
<p>操作XML有两种方法：DOM和SAX。</p>
<ol>
<li>DOM会把整个XML读入内存，解析为树，因此占用内存大，解析慢，优点是可以任意遍历树的节点。</li>
<li>SAX是流模式，边读边解析，占用内存小，解析快，缺点是我们需要自己处理事件。</li>
</ol>
<p>正常情况下，<strong>优先考虑SAX，因为DOM实在太占内存</strong>。</p>
<p>在Python中使用SAX解析XML非常简洁，通常我们关心的事件是start_element，end_element和char_data，准备好这3个函数，然后就可以解析xml了。</p>
<p>举个例子，当SAX解析器读到一个节点时：</p>
<pre><code>&lt;a href=&quot;/&quot;&gt;python&lt;/a&gt;
</code></pre>
<p>会产生3个事件：</p>
<ol>
<li>
<p>start_element事件，在读取<code>&lt;a href=&quot;/&quot;&gt;</code>时；</p>
</li>
<li>
<p>char_data事件，在读取python时；</p>
</li>
<li>
<p>end_element事件，在读取<code>&lt;/a&gt;</code>时。</p>
</li>
</ol>
<p>用代码实验一下：</p>
<pre><code>from xml.parsers.expat import ParserCreate

class DefaultSaxHandler(object):
    def start_element(self, name, attrs):
        print('sax:start_element: %s, attrs: %s' % (name, str(attrs)))

    def end_element(self, name):
        print('sax:end_element: %s' % name)

    def char_data(self, text):
        print('sax:char_data: %s' % text)

xml = r'''&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;ol&gt;
    &lt;li&gt;&lt;a href=&quot;/python&quot;&gt;Python&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;/ruby&quot;&gt;Ruby&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
'''

handler = DefaultSaxHandler()
parser = ParserCreate()
parser.StartElementHandler = handler.start_element
parser.EndElementHandler = handler.end_element
parser.CharacterDataHandler = handler.char_data
parser.Parse(xml)
</code></pre>
<p>需要注意的是读取一大段字符串时，CharacterDataHandler可能被多次调用，所以需要自己保存起来，在EndElementHandler里面再合并。</p>
<p>除了解析XML外，如何生成XML呢？99%的情况下需要生成的XML结构都是非常简单的，因此，最简单也是最有效的生成XML的方法是拼接字符串：</p>
<pre><code>L = []
L.append(r'&lt;?xml version=&quot;1.0&quot;?&gt;')
L.append(r'&lt;root&gt;')
L.append(encode('some &amp; data'))
L.append(r'&lt;/root&gt;')
return ''.join(L)
</code></pre>
<p>如果要生成复杂的XML呢，此时建议不要用XML，改成JSON。</p>
<p><strong>小结</strong></p>
<p>解析XML时，注意找出自己感兴趣的节点，响应事件时，把节点数据保存起来。解析完毕后，就可以处理数据。</p>
<h2 id="htmlparser">HTMLParser</h2>
<p>如果我们要编写一个搜索引擎，第一步是用爬虫把目标网站的页面抓下来，第二步就是解析该HTML页面，看看里面的内容到底是新闻、图片还是视频。</p>
<p>假设第一步已经完成了，第二步解析HTML</p>
<p>HTML本质上是XML的子集，但是HTML的语法没有XML那么严格，所以<strong>不能用标准的DOM或SAX来解析HTML</strong>。</p>
<p>好在Python提供了HTMLParser来非常方便地解析HTML，只需简单几行代码：</p>
<pre><code>from html.parser import HTMLParser
from html.entities import name2codepoint

class MyHTMLParser(HTMLParser):

    def handle_starttag(self, tag, attrs):
        print('&lt;%s&gt;' % tag)

    def handle_endtag(self, tag):
        print('&lt;/%s&gt;' % tag)

    def handle_startendtag(self, tag, attrs):
        print('&lt;%s/&gt;' % tag)

    def handle_data(self, data):
        print(data)

    def handle_comment(self, data):
        print('&lt;!--', data, '--&gt;')

    def handle_entityref(self, name):
        print('&amp;%s;' % name)

    def handle_charref(self, name):
        print('&amp;#%s;' % name)

parser = MyHTMLParser()
parser.feed('''&lt;html&gt;
&lt;head&gt;&lt;/head&gt;
&lt;body&gt;
&lt;!-- test html parser --&gt;
    &lt;p&gt;Some &lt;a href=\&quot;#\&quot;&gt;html&lt;/a&gt; HTML&amp;nbsp;tutorial...&lt;br&gt;END&lt;/p&gt;
&lt;/body&gt;&lt;/html&gt;''')
</code></pre>
<p>feed()方法可以多次调用，也就是不一定一次把整个HTML字符串都塞进去，可以一部分一部分塞进去。</p>
<p>特殊字符有两种，一种是英文表示的<code>&amp;nbsp</code>;，一种是数字表示的<code>&amp;#1234;</code>，这两种字符都可以通过Parser解析出来。</p>
<p><strong>小结</strong></p>
<p>利用HTMLParser，可以把网页中的文本、图像等解析出来。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python - 正则表达式]]></title>
        <id>https://lixin-scut.github.io//post/python-zheng-ze-biao-da-shi</id>
        <link href="https://lixin-scut.github.io//post/python-zheng-ze-biao-da-shi">
        </link>
        <updated>2020-05-13T01:16:34.000Z</updated>
        <content type="html"><![CDATA[<p>字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。</p>
<p>正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。</p>
<p>所以我们判断一个字符串是否是合法的Email的方法是：</p>
<ol>
<li>
<p>创建一个匹配Email的正则表达式；</p>
</li>
<li>
<p>用该正则表达式去匹配用户的输入来判断是否合法。</p>
</li>
</ol>
<p>因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。</p>
<p>在正则表达式中，如果直接给出字符，就是精确匹配。<strong>用\d可以匹配一个数字</strong>，<strong>\w可以匹配一个字母或数字</strong>，所以：</p>
<ol>
<li>
<p>'00\d'可以匹配'007'，但无法匹配'00A'；</p>
</li>
<li>
<p>'\d\d\d'可以匹配'010'；</p>
</li>
<li>
<p>'\w\w\d'可以匹配'py3'；</p>
</li>
</ol>
<p><strong><code>.</code>可以匹配任意字符</strong>，所以：</p>
<p>'py.'可以匹配'pyc'、'pyo'、'py!'等等。</p>
<p>要匹配变长的字符，在正则表达式中，<strong>用<code>*</code>表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符：</strong></p>
<p>来看一个复杂的例子：\d{3}\s+\d{3,8}。</p>
<p>我们来从左到右解读一下：</p>
<p>\d{3}表示匹配3个数字，例如'010'；</p>
<p>\s可以匹配一个空格（也包括Tab等空白符），<strong>所以\s+表示至少有一个空格</strong>，例如匹配' '，' '等；</p>
<p>\d{3,8}表示3-8个数字，例如'1234567'。</p>
<p>综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。</p>
<p>如果要匹配'010-12345'这样的号码呢？由于'-'是特殊字符，在正则表达式中，要用''转义，所以，上面的正则是\d{3}-\d{3,8}。</p>
<p>但是，仍然无法匹配'010 - 12345'，因为带有空格。所以我们需要更复杂的匹配方式。</p>
<h3 id="进阶">进阶</h3>
<p>要做更精确地匹配，<strong>可以用[]表示范围</strong>，比如：</p>
<ol>
<li>
<p><code>[0-9a-zA-Z\_]</code>可以匹配<strong>一个</strong>数字、字母或者下划线；</p>
</li>
<li>
<p><code>[0-9a-zA-Z\_]+</code>可以匹配<strong>至少</strong>由一个数字、字母或者下划线组成的字符串，比如'a100'，'0_Z'，'Py3000'等等；</p>
</li>
<li>
<p><code>[a-zA-Z\_][0-9a-zA-Z\_]*</code>可以匹配由字母或下划线开头，后接<strong>任意个</strong>由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；</p>
</li>
<li>
<p><code>[a-zA-Z\_][0-9a-zA-Z\_]{0, 19}</code>更精确地限制了变量的长度是<strong>1-20个</strong>字符（前面1个字符+后面最多19个字符）。</p>
</li>
</ol>
<p><code>A|B</code>可以匹配<strong>A或B</strong>，所以(P|p)ython可以匹配'Python'或者'python'。</p>
<p><code>^</code>表示行的开头，<code>^\d</code>表示必须以数字开头。</p>
<p><code>$</code>表示行的结束，<code>\d$</code>表示必须以数字结束。</p>
<p>你可能注意到了，py也可以匹配'python'，但是<strong>加上^py$就变成了整行匹配，就只能匹配'py'了</strong>。</p>
<h3 id="re模块">re模块</h3>
<p>有了准备知识，我们就可以在Python中使用正则表达式了。Python提供<strong>re模块，包含所有正则表达式的功能</strong>。</p>
<p><strong>由于Python的字符串本身也用\转义</strong>，所以要特别注意：</p>
<pre><code>
s = 'ABC\\-001' # Python的字符串
# 对应的正则表达式字符串变成：
# 'ABC\-001'

</code></pre>
<p>因此我们<strong>强烈建议使用Python的r前缀</strong>，就不用考虑转义的问题了：</p>
<pre><code>
s = r'ABC\-001' # Python的字符串
# 对应的正则表达式字符串不变：
# 'ABC\-001'

</code></pre>
<p>先看看如何判断正则表达式是否匹配：</p>
<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; re.match(r'^\d{3}\-\d{3,8}$', '010-12345')
&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;
&gt;&gt;&gt; re.match(r'^\d{3}\-\d{3,8}$', '010 12345')
&gt;&gt;&gt;
</code></pre>
<p>match()方法判断是否匹配，<strong>如果匹配成功，返回一个Match对象，否则返回None。</strong></p>
<p>常见的判断方法就是：</p>
<pre><code>test = '用户输入的字符串'
if re.match(r'正则表达式', test):
    print('ok')
else:
    print('failed')
</code></pre>
<h3 id="切分字符串">切分字符串</h3>
<p>用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码：</p>
<pre><code>&gt;&gt;&gt; 'a b   c'.split(' ')
['a', 'b', '', '', 'c']
</code></pre>
<p>嗯，<strong>无法识别连续的空格</strong>，用正则表达式试试：</p>
<pre><code>&gt;&gt;&gt; re.split(r'\s+', 'a b   c')
['a', 'b', 'c']
</code></pre>
<p><strong>无论多少个空格都可以正常分割</strong>。</p>
<p>加入<code>,</code>试试：</p>
<pre><code>&gt;&gt;&gt; re.split(r'[\s\,]+', 'a,b, c  d')
['a', 'b', 'c', 'd']
</code></pre>
<p>再加入<code>;</code>试试：</p>
<pre><code>&gt;&gt;&gt; re.split(r'[\s\,\;]+', 'a,b;; c  d')
['a', 'b', 'c', 'd']
</code></pre>
<p>用正则表达式来把不规范的输入转化成正确的数组。</p>
<h3 id="分组">分组</h3>
<p>除了简单地判断是否匹配之外，正则表达式还有<strong>提取子串的强大功能</strong>。<strong>用()表示的就是要提取的分组（Group）</strong>。比如：</p>
<p><code>^(\d{3})-(\d{3,8})$</code>分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码：</p>
<pre><code>&gt;&gt;&gt; m = re.match(r'^(\d{3})-(\d{3,8})$', '010-12345')
&gt;&gt;&gt; m
&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;
&gt;&gt;&gt; m.group(0)
'010-12345'
&gt;&gt;&gt; m.group(1)
'010'
&gt;&gt;&gt; m.group(2)
'12345'
如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。
</code></pre>
<p>注意到<strong>group(0)永远是原始字符串</strong>，group(1)、group(2)……表示第1、2、……个子串。</p>
<p>提取子串非常有用。来看一个更厉害的例子：</p>
<pre><code>&gt;&gt;&gt; t = '19:05:30'
&gt;&gt;&gt; m = re.match(r'^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$', t)
&gt;&gt;&gt; m.groups()
('19', '05', '30')
</code></pre>
<p>这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期：</p>
<pre><code>'^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$'
</code></pre>
<p>对于'2-30'，'4-31'这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。</p>
<h3 id="贪婪匹配">贪婪匹配</h3>
<p>最后需要特别指出的是，<strong>正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符</strong>。举例如下，匹配出数字后面的0：</p>
<pre><code>&gt;&gt;&gt; re.match(r'^(\d+)(0*)$', '102300').groups()
('102300', '')
</code></pre>
<p>由于\d+采用贪婪匹配，直接把后面的0全部匹配了，结果<code>0*</code>只能匹配空字符串了。</p>
<p>必须让\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，<strong>加个?就可以让\d+采用非贪婪匹配</strong>：</p>
<pre><code>&gt;&gt;&gt; re.match(r'^(\d+?)(0*)$', '102300').groups()
('1023', '00')
</code></pre>
<h3 id="编译">编译</h3>
<p>当我们在Python中使用正则表达式时，re模块内部会干两件事情：</p>
<ol>
<li>
<p>编译正则表达式，如果正则表达式的字符串本身不合法，会报错；</p>
</li>
<li>
<p>用编译后的正则表达式去匹配字符串。</p>
</li>
</ol>
<p>如果一个正则表达式要重复使用几千次，出于效率的考虑，我们<strong>可以预编译re.compile该正则表达式，接下来重复使用时就不需要编译这个步骤了</strong>，直接匹配：</p>
<pre><code>
&gt;&gt;&gt; import re
# 编译:
&gt;&gt;&gt; re_telephone = re.compile(r'^(\d{3})-(\d{3,8})$')
# 使用：
&gt;&gt;&gt; re_telephone.match('010-12345').groups()
('010', '12345')
&gt;&gt;&gt; re_telephone.match('010-8086').groups()
('010', '8086')

</code></pre>
<p><strong>编译后生成Regular Expression对象</strong>，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。</p>
<h3 id="练习">练习</h3>
<p>请尝试写一个验证Email地址的正则表达式。版本一应该可以验证出类似的Email：</p>
<pre><code>
someone@gmail.com
bill.gates@microsoft.com
# -*- coding: utf-8 -*-
import re
def is_valid_email(addr):
    if re.match(r'[0-9a-zA-Z\.]*@[a-z]*.com',addr):
        return True
    else:
        return False
# 测试:
assert is_valid_email('someone@gmail.com')
assert is_valid_email('bill.gates@microsoft.com')
assert not is_valid_email('bob#example.com')
assert not is_valid_email('mr-bob@example.com')
print('ok')

</code></pre>
<p>版本二可以提取出带名字的Email地址：</p>
<pre><code>&lt;Tom Paris&gt; tom@voyager.org =&gt; Tom Paris
bob@example.com =&gt; bob
# -*- coding: utf-8 -*-
import re
def name_of_email(addr):
    return re.match(r'\&lt;?([a-zA-Z\s]*)\&gt;?[a-zA-Z\s]*@.*', addr).group(1)
    # return re.match(r'^\&lt;?(\w+\s*\w+)\&gt;?\s*\w*\@\w+\.\w+$', addr).group(1)
# 测试:
assert name_of_email('&lt;Tom Paris&gt; tom@voyager.org') == 'Tom Paris'
assert name_of_email('tom@voyager.org') == 'tom'
print('ok')
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python - 进程和线程	]]></title>
        <id>https://lixin-scut.github.io//post/python-jin-cheng-he-xian-cheng</id>
        <link href="https://lixin-scut.github.io//post/python-jin-cheng-he-xian-cheng">
        </link>
        <updated>2020-05-13T00:12:41.000Z</updated>
        <content type="html"><![CDATA[<p>现在，多核CPU已经非常普及了，但是，即使过去的单核CPU，也可以执行多任务。由于CPU执行代码都是顺序执行的，那么，单核CPU是怎么执行多任务的呢？</p>
<p>答案就是操作系统轮流让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。</p>
<p>真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。</p>
<p>对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。</p>
<p>有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。</p>
<p>由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。</p>
<p>前面编写的所有的Python程序，都是执行单任务的进程，也就是只有一个线程。如果我们要同时执行多个任务怎么办？</p>
<p>有两种解决方案：</p>
<p>一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。</p>
<p>还有一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。</p>
<p>当然还有第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。</p>
<p>总结一下就是，多任务的实现有3种方式：</p>
<ol>
<li>多进程模式；</li>
<li>多线程模式；</li>
<li>多进程+多线程模式。<br>
同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。</li>
</ol>
<p>因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。</p>
<p>Python既支持多进程，又支持多线程，我们会讨论如何编写这两种多任务程序。</p>
<h3 id="小结">小结</h3>
<p>线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。</p>
<p>多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。</p>
<h2 id="多进程">多进程</h2>
<p>要让Python程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。</p>
<p>Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是<strong>fork()调用一次，返回两次</strong>，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，<strong>分别在父进程和子进程内返回</strong>。</p>
<p><strong>子进程永远返回0，而父进程返回子进程的ID</strong>。这样做的理由是，一个父进程可以fork出很多子进程，所以，<strong>父进程要记下每个子进程的ID</strong>，而**子进程只需要调用getppid()**就可以拿到父进程的ID。</p>
<p>Python的<strong>os模块封装了常见的系统调用，其中就包括fork</strong>，可以在Python程序中轻松创建子进程：</p>
<pre><code>import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
</code></pre>
<p>运行结果如下：</p>
<pre><code>Process (876) start...
I (876) just created a child process (877).
I am child process (877) and my parent is 876.
</code></pre>
<p>由于Windows没有fork调用，上面的代码在Windows上无法运行。而Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的</p>
<p>有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。</p>
<h3 id="multiprocessing">multiprocessing</h3>
<p>如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？</p>
<p>由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。</p>
<p><strong>multiprocessing模块就是跨平台版本的多进程模块</strong>。</p>
<p>multiprocessing模块<strong>提供了一个Process类来代表一个进程对象</strong></p>
<p>下面的例子演示了启动一个子进程并等待其结束：</p>
<pre><code>
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
		
</code></pre>
<p>执行结果如下：</p>
<pre><code>Parent process 928.
Child process will start.
Run child process test (929)...
Process end.
</code></pre>
<p>创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。</p>
<p><strong>join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步</strong>。</p>
<h3 id="pool">Pool</h3>
<p>如果要启动大量的子进程，可以<strong>用进程池的方式批量创建子进程</strong>：</p>
<pre><code>from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
</code></pre>
<p>执行结果如下：</p>
<pre><code>Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
</code></pre>
<p>代码解读：</p>
<ol>
<li>对<strong>Pool对象调用join()方法</strong>会等待<strong>所有子进程</strong>执行完毕</li>
<li><strong>调用join()之前必须先调用close()</strong></li>
<li><strong>调用close()之后就不能继续添加新的Process了</strong>。</li>
</ol>
<p>请注意输出的结果，task 0，1，2，3是立刻执行的，而<strong>task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4</strong>，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：</p>
<pre><code>p = Pool(5)
</code></pre>
<p>就可以同时跑5个进程。</p>
<p>由于Pool的默认大小是CPU的核数，如果你拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。</p>
<h3 id="子进程">子进程</h3>
<p>很多时候，<strong>子进程并不是自身，而是一个外部进程</strong>。我们创建了子进程后，<strong>还需要控制子进程的输入和输出</strong>。</p>
<p><strong>subprocess模块</strong>可以让我们非常方便地<strong>启动一个子进程，然后控制其输入和输出。</strong></p>
<p>下面的例子演示了如何在Python代码中运行命令<code>nslookup www.python.org</code>，这和命令行直接运行的效果是一样的：</p>
<pre><code>import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
</code></pre>
<p>运行结果：</p>
<pre><code>$ nslookup www.python.org
Server:		192.168.19.4
Address:	192.168.19.4#53

Non-authoritative answer:
www.python.org	canonical name = python.map.fastly.net.
Name:	python.map.fastly.net
Address: 199.27.79.223

Exit code: 0
</code></pre>
<p>如果子进程还需要输入，则可以通过communicate()方法输入：</p>
<pre><code>import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
</code></pre>
<p>上面的代码相当于在命令行执行命令nslookup，然后手动输入：</p>
<pre><code>set q=mx
python.org
exit
</code></pre>
<p>运行结果如下：</p>
<pre><code>$ nslookup
Server:		192.168.19.4
Address:	192.168.19.4#53

Non-authoritative answer:
python.org	mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
mail.python.org	internet address = 82.94.164.166
mail.python.org	has AAAA address 2001:888:2000:d::a6


Exit code: 0
</code></pre>
<h3 id="进程间通信">进程间通信</h3>
<p>Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的<strong>multiprocessing模块</strong>包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。</p>
<p>我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据：</p>
<pre><code>
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
		
</code></pre>
<p>运行结果如下：</p>
<pre><code>
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.

</code></pre>
<p>在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。</p>
<p>由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，<strong>父进程所有Python对象都必须通过pickle序列化再传到子进程去</strong>，所以，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。</p>
<h3 id="小结-2">小结</h3>
<p>在Unix/Linux下，可以使用fork()调用实现多进程。</p>
<p>要实现跨平台的多进程，可以使用multiprocessing模块。</p>
<p>进程间通信是通过Queue、Pipes等实现的。</p>
<h2 id="多线程">多线程</h2>
<p>多任务可以由多进程完成，也可以由一个进程内的多线程完成。</p>
<p>我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。</p>
<p>由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，<strong>Python的线程是真正的Posix Thread，而不是模拟出来的线程</strong>。</p>
<p>Python的标准库<strong>提供了两个模块</strong>：<code>_thread</code>和<code>threading</code>，<code>_thread</code>是低级模块，<code>threading</code>是高级模块，对<code>_thread</code>进行了封装。绝大多数情况下，我们<strong>只需要使用<code>threading</code>这个高级模块</strong>。</p>
<p>启动一个线程就是把一个函数传入并创建Thread实例，然后<strong>调用start()开始执行</strong>：</p>
<pre><code>
import time, threading

# 新线程执行的代码:
def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n &lt; 5:
        n = n + 1
        print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join()
print('thread %s ended.' % threading.current_thread().name)

</code></pre>
<p>执行结果如下：</p>
<pre><code>
thread MainThread is running...
thread LoopThread is running...
thread LoopThread &gt;&gt;&gt; 1
thread LoopThread &gt;&gt;&gt; 2
thread LoopThread &gt;&gt;&gt; 3
thread LoopThread &gt;&gt;&gt; 4
thread LoopThread &gt;&gt;&gt; 5
thread LoopThread ended.
thread MainThread ended.

</code></pre>
<p>由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个<strong>current_thread()函数，它永远返回当前线程的实例</strong>。<br>
主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2……</p>
<h3 id="lock">Lock</h3>
<p>多线程和多进程最大的不同在于</p>
<ol>
<li><strong>多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响</strong></li>
<li>而多线程中，<strong>所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改</strong></li>
<li>因此，线程之间共享数据<strong>最大的危险在于多个线程同时改一个变量</strong>，把内容给改乱了。</li>
</ol>
<p>来看看多个线程同时操作一个变量怎么把内容给改乱了：</p>
<pre><code>
# multithread
import time, threading

# 假定这是你的银行存款:
balance = 0

def change_it(n):
    # 先存后取，结果应该为0:
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    for i in range(1000000):
        change_it(n)

t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)

</code></pre>
<p>我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，balance的结果就不一定是0了。</p>
<p>原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算：</p>
<pre><code>balance = balance + n
</code></pre>
<p>也分两步：</p>
<ol>
<li>计算balance + n，存入临时变量中；</li>
<li>将临时变量的值赋给balance。</li>
</ol>
<p>也就是可以看成：</p>
<pre><code>x = balance + n
balance = x
</code></pre>
<p>由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时：</p>
<pre><code>初始值 balance = 0

t1: x1 = balance + 5 # x1 = 0 + 5 = 5
t1: balance = x1     # balance = 5
t1: x1 = balance - 5 # x1 = 5 - 5 = 0
t1: balance = x1     # balance = 0

t2: x2 = balance + 8 # x2 = 0 + 8 = 8
t2: balance = x2     # balance = 8
t2: x2 = balance - 8 # x2 = 8 - 8 = 0
t2: balance = x2     # balance = 0
    
结果 balance = 0
</code></pre>
<p>但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：</p>
<p>初始值 balance = 0</p>
<pre><code>t1: x1 = balance + 5  # x1 = 0 + 5 = 5

t2: x2 = balance + 8  # x2 = 0 + 8 = 8
t2: balance = x2      # balance = 8

t1: balance = x1      # balance = 5
t1: x1 = balance - 5  # x1 = 5 - 5 = 0
t1: balance = x1      # balance = 0

t2: x2 = balance - 8  # x2 = 0 - 8 = -8
t2: balance = x2   # balance = -8

结果 balance = -8
</code></pre>
<p>究其原因，是因为<strong>修改balance需要多条语句</strong>，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。</p>
<p>两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。</p>
<p>如果我们要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。**创建一个锁就是通过threading.Lock()**来实现：</p>
<pre><code>balance = 0
lock = threading.Lock()

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 放心地改吧:
            change_it(n)
        finally:
            # 改完了一定要释放锁:
            lock.release()
</code></pre>
<p>当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p>
<p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们<strong>用try...finally来确保锁一定会被释放</strong>。</p>
<p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</p>
<h3 id="多核cpu">多核CPU</h3>
<p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p>
<p>如果写一个死循环的话，会出现什么情况呢？</p>
<p>打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。</p>
<p>我们可以监控到一个死循环线程会100%占用一个CPU。</p>
<p>如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。</p>
<p>要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p>
<p>试试用Python写个死循环：</p>
<pre><code>import threading, multiprocessing

def loop():
    x = 0
    while True:
        x = x ^ 1

for i in range(multiprocessing.cpu_count()):
    t = threading.Thread(target=loop)
    t.start()
</code></pre>
<p>启动与CPU核心数量相同的N个线程，<strong>在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</strong></p>
<p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p>
<p>因为Python的线程虽然是真正的线程，但<strong>解释器执行代码时，有一个GIL锁</strong>：Global Interpreter Lock，<strong>任何Python线程执行前，必须先获得GIL锁</strong>，然后，<strong>每执行100条字节码，解释器就自动释放GIL锁</strong>，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，<strong>多线程在Python中只能交替执行</strong>，即使100个线程跑在100核CPU上，也只能用到1个核。</p>
<p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p>
<p>所以，在Python中，<strong>可以使用多线程，但不要指望能有效利用多核</strong>。如果一定要通过多线程利用多核，那<strong>只能通过C扩展来实现，不过这样就失去了Python简单易用的特点</strong>。</p>
<p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但<strong>可以通过多进程实现多核任务</strong>。多个Python进程有各自独立的GIL锁，互不影响。</p>
<h3 id="小结-3">小结</h3>
<p>多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。</p>
<p>Python解释器由于设计时有<strong>GIL全局锁，导致了多线程无法利用多核（但是注意可以多进程利用多核）</strong>。多线程的并发在Python中就是一个美丽的梦。</p>
<h2 id="threadlocal">ThreadLocal</h2>
<p>在多线程环境下，每个线程都有自己的数据。<strong>一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程</strong>，而全局变量的修改必须加锁。</p>
<p>但是<strong>局部变量也有问题，就是在函数调用的时候，传递起来很麻烦</strong>：</p>
<pre><code>def process_student(name):
    std = Student(name)
    # std是局部变量，但是每个函数都要用它，因此必须传进去：
    do_task_1(std)
    do_task_2(std)

def do_task_1(std):
    do_subtask_1(std)
    do_subtask_2(std)

def do_task_2(std):
    do_subtask_2(std)
    do_subtask_2(std)
</code></pre>
<p>每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的Student对象，不能共享。</p>
<p>如果用一个全局dict存放所有的Student对象，然后以thread自身作为key获得线程对应的Student对象如何？</p>
<pre><code>global_dict = {}

def std_thread(name):
    std = Student(name)
    # 把std放到全局变量global_dict中：
    global_dict[threading.current_thread()] = std
    do_task_1()
    do_task_2()

def do_task_1():
    # 不传入std，而是根据当前线程查找：
    std = global_dict[threading.current_thread()]
    ...

def do_task_2():
    # 任何函数都可以查找出当前线程的std变量：
    std = global_dict[threading.current_thread()]
    ...
</code></pre>
<p>这种方式理论上是可行的，它最大的优点是消除了std对象在每层函数中的传递问题，但是，每个函数获取std的代码有点丑。</p>
<p>有没有更简单的方式？</p>
<p>ThreadLocal应运而生，不用查找dict，ThreadLocal帮你自动做这件事：</p>
<pre><code>
import threading
    
# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()

</code></pre>
<p>执行结果：</p>
<pre><code>
Hello, Alice (in Thread-A)
Hello, Bob (in Thread-B)

</code></pre>
<p>全局变量local_school就是一个<strong>ThreadLocal对象</strong>，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school<strong>看成全局变量</strong>，但<strong>每个属性如local_school.student都是线程的局部变量</strong>，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。</p>
<p>可以理解为全局变量local_school是<strong>一个dict</strong>，不但可以用local_school.student，还<strong>可以绑定其他变量</strong>，如local_school.teacher等等。</p>
<p>ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。</p>
<h3 id="小结-4">小结</h3>
<p>一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。<strong>ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题</strong>。</p>
<h2 id="进程-vs-线程">进程 vs. 线程</h2>
<p>我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。</p>
<p>首先，要实现多任务，通常我们会设计<strong>Master-Worker模式</strong>，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。</p>
<p>如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。</p>
<p>如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。</p>
<p>多进程模式最大的优点就是<strong>稳定性高</strong>，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。</p>
<p>多进程模式的缺点是<strong>创建进程的代价大</strong>，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。</p>
<p>多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是<strong>任何一个线程挂掉都可能直接造成整个进程崩溃</strong>，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。</p>
<p>在Windows下，<strong>多线程的效率比多进程要高</strong>，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。</p>
<h3 id="线程切换">线程切换</h3>
<p>无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？</p>
<p>我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。</p>
<p>如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。</p>
<p>假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。</p>
<p>但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。</p>
<p>所以，<strong>多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。</strong></p>
<h3 id="计算密集型-vs-io密集型">计算密集型 vs. IO密集型</h3>
<p>是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。</p>
<p>计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，<strong>计算密集型任务同时进行的数量应当等于CPU的核心数</strong>。</p>
<p>计算密集型任务由于<strong>主要消耗CPU资源</strong>，因此，代码运行效率至关重要。Python这样的<strong>脚本语言运行效率很低，完全不适合计算密集型任务</strong>。对于计算密集型任务，<strong>最好用C语言编写</strong>。</p>
<p>第二种任务的类型是IO密集型，<strong>涉及到网络、磁盘IO的任务都是IO密集型任务</strong>，这类任务的特点是CPU消耗很少，任务的大部分时间都在<strong>等待IO操作完成</strong>（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任<strong>务越多，CPU效率越高</strong>，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。</p>
<p>IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。<strong>对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。</strong></p>
<h3 id="异步io">异步IO</h3>
<p>考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，<strong>因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行</strong>。</p>
<p>现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就<strong>可以用单进程单线程模型来执行多任务</strong>，这种全新的模型称为<strong>事件驱动模型</strong>，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。</p>
<p>对应到Python语言，<strong>单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。</strong></p>
<h2 id="分布式进程">分布式进程</h2>
<p>在Thread和Process中，应当<strong>优选Process</strong>，因为<strong>Process更稳定</strong>，而且，<strong>Process可以分布到多台机器上</strong>，而<strong>Thread最多只能分布到同一台机器的多个CPU上</strong>。</p>
<p>Python的multiprocessing模块不但支持多进程，其中<strong>managers子模块</strong>还支持把多进程<strong>分布到多台机器上</strong>。一个服务进程可以作为调度者，将任务分布到其他多个进程中，<strong>依靠网络通信</strong>。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。</p>
<p>举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？</p>
<p>原有的Queue可以继续使用，但是，<strong>通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了</strong>。</p>
<p>我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务：</p>
<pre><code>
# task_master.py

import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)

# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()

# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()

# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
		
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)
    print('Result: %s' % r)
		
# 关闭:
manager.shutdown()
print('master exit.')

</code></pre>
<p>请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过<code>manager.get_task_queue()</code>获得的Queue接口添加。</p>
<p>然后，在另一台机器上启动任务进程（本机上启动也可以）：</p>
<pre><code>
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')

</code></pre>
<p>任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。</p>
<p>现在，可以试试分布式进程的工作效果了。先启动task_master.py服务进程：</p>
<pre><code>$ python3 task_master.py 
Put task 3411...
Put task 1605...
Put task 1398...
Put task 4729...
Put task 5300...
Put task 7471...
Put task 68...
Put task 4219...
Put task 339...
Put task 7866...
Try get results...
</code></pre>
<p>task_master.py进程发送完任务后，开始等待result队列的结果。现在启动task_worker.py进程：</p>
<pre><code>$ python3 task_worker.py
Connect to server 127.0.0.1...
run task 3411 * 3411...
run task 1605 * 1605...
run task 1398 * 1398...
run task 4729 * 4729...
run task 5300 * 5300...
run task 7471 * 7471...
run task 68 * 68...
run task 4219 * 4219...
run task 339 * 339...
run task 7866 * 7866...
worker exit.
</code></pre>
<p>task_worker.py进程结束，在task_master.py进程中会继续打印出结果：</p>
<pre><code>Result: 3411 * 3411 = 11634921
Result: 1605 * 1605 = 2576025
Result: 1398 * 1398 = 1954404
Result: 4729 * 4729 = 22363441
Result: 5300 * 5300 = 28090000
Result: 7471 * 7471 = 55815841
Result: 68 * 68 = 4624
Result: 4219 * 4219 = 17799961
Result: 339 * 339 = 114921
Result: 7866 * 7866 = 61873956
</code></pre>
<p>这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n * n的代码换成发送邮件，就实现了邮件队列的异步发送。</p>
<p>Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，<strong>Queue对象存储在task_master.py进程中</strong>：</p>
<pre><code>
                                             │
┌─────────────────────────────────────────┐     ┌──────────────────────────────────────┐
│task_master.py                           │  │  │task_worker.py                        │
│                                         │     │                                      │
│  task = manager.get_task_queue()        │  │  │  task = manager.get_task_queue()     │
│  result = manager.get_result_queue()    │     │  result = manager.get_result_queue() │
│              │                          │  │  │              │                       │
│              │                          │     │              │                       │
│              ▼                          │  │  │              │                       │
│  ┌─────────────────────────────────┐    │     │              │                       │
│  │QueueManager                     │    │  │  │              │                       │
│  │ ┌────────────┐ ┌──────────────┐ │    │     │              │                       │
│  │ │ task_queue │ │ result_queue │ │&lt;───┼──┼──┼──────────────┘                       │
│  │ └────────────┘ └──────────────┘ │    │     │                                      │
│  └─────────────────────────────────┘    │  │  │                                      │
└─────────────────────────────────────────┘     └──────────────────────────────────────┘
                                             │

                                          Network
</code></pre>
<p>而<strong>Queue之所以能通过网络访问，就是通过QueueManager实现的</strong>。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。</p>
<p><strong>authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰</strong>。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。</p>
<h3 id="小结-5">小结</h3>
<p>Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。</p>
<p>注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Python - IO编程]]></title>
        <id>https://lixin-scut.github.io//post/python-io-bian-cheng</id>
        <link href="https://lixin-scut.github.io//post/python-io-bian-cheng">
        </link>
        <updated>2020-05-12T14:17:38.000Z</updated>
        <content type="html"><![CDATA[<p>O在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由CPU这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口。</p>
<p>比如你打开浏览器，浏览器这个程序就需要通过网络IO获取网页。浏览器首先会发送数据给服务器，告诉它我想要的HTML，这个动作是往外发数据，叫Output，随后服务器把网页发过来，这个动作是从外面接收数据，叫Input。</p>
<p>所以，通常，程序完成IO操作会有Input和Output两个数据流。当然也有只用一个的情况，比如，从磁盘读取文件到内存，就只有Input操作，反过来，把数据写到磁盘文件里，就只是一个Output操作。</p>
<p>IO编程中，Stream（流）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。对于浏览网页来说，浏览器和服务器之间至少需要建立两根水管，才可以既能发数据，又能收数据。</p>
<p>由于CPU和内存的速度远远高于外设的速度，所以，在IO编程中，就存在速度严重不匹配的问题。举个例子来说，比如要把100M的数据写入磁盘，CPU输出100M的数据只需要0.01秒，可是磁盘要接收这100M数据可能需要10秒，怎么办呢？有两种办法：</p>
<p>第一种是CPU等着，也就是程序暂停执行后续代码，等100M的数据在10秒后写入磁盘，再接着往下执行，这种模式称为同步IO；</p>
<p>另一种方法是CPU不等待，只是告诉磁盘继续读写，于是，后续代码可以立刻接着执行，这种模式称为异步IO。</p>
<p>同步和异步的区别就在于是否等待IO执行的结果</p>
<p>很明显，使用异步IO来编写程序性能会远远高于同步IO，但是异步IO的缺点是编程模型复杂，而通知你的方法也各不相同。比如回调模式和轮询模式。总之，异步IO的复杂度远远高于同步IO。</p>
<p>操作IO的能力都是由操作系统提供的，每一种编程语言都会把操作系统提供的低级C接口封装起来方便使用，Python也不例外。我们后面会详细讨论Python的IO编程接口。</p>
<p>注意，<strong>本章的IO编程都是同步模式</strong>，异步IO由于复杂度太高，后续涉及到服务器端程序开发时我们再讨论。</p>
<h2 id="文件读写">文件读写</h2>
<p>读写文件是最常见的IO操作。Python内置了读写文件的函数，用法和C是兼容的。</p>
<p>读写文件前，我们先必须了解一下，<strong>在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘</strong>，所以，读写文件就是<strong>请求操作系统打开一个文件对象（通常称为文件描述符）</strong>，然后，<strong>通过操作系统提供的接口</strong>从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。</p>
<h3 id="读文件">读文件</h3>
<p>要以读文件的模式打开一个文件对象，使用Python内置的<strong>open()函数，传入文件名和标示符</strong>：</p>
<pre><code>&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'r')
</code></pre>
<p><strong>标示符'r'表示读</strong>，这样，我们就成功地打开了一个文件。</p>
<p>如果<strong>文件不存在，open()函数就会抛出一个IOError的错误</strong>，并且给出错误码和详细的信息告诉你文件不存在：</p>
<pre><code>&gt;&gt;&gt; f=open('/Users/michael/notfound.txt', 'r')
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
FileNotFoundError: [Errno 2] No such file or directory: '/Users/michael/notfound.txt'
</code></pre>
<p>如果文件打开成功，接下来，<strong>调用read()方法可以一次读取文件的全部内容</strong>，Python把内容读到内存，<strong>用一个str对象表示</strong>：</p>
<pre><code>&gt;&gt;&gt; f.read()
'Hello, world!'
</code></pre>
<p>最后一步是调用close()方法关闭文件。<strong>文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的</strong>：</p>
<pre><code>&gt;&gt;&gt; f.close()
</code></pre>
<p>由于文件读写时都有可能产生IOError，<strong>一旦出错，后面的f.close()就不会调用</strong>。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现：</p>
<pre><code>try:
    f = open('/path/to/file', 'r')
    print(f.read())
finally:
    if f:
        f.close()
</code></pre>
<p>但是每次都这么写实在太繁琐，所以，Python<strong>引入了with语句来自动帮我们调用close()方法</strong>：</p>
<pre><code>with open('/path/to/file', 'r') as f:
    print(f.read())
</code></pre>
<p>这和前面的try ... finally是一样的，但是<strong>代码更简洁，并且不必调用f.close()方法。</strong></p>
<ol>
<li><strong>调用read()会一次性读取文件的全部内容</strong>，如果文件有10G，内存就爆了，</li>
<li>所以，要保险起见，<strong>可以反复调用read(size)方法</strong>，每次<strong>最多读取</strong>size个字节的内容。</li>
<li>另外，<strong>调用readline()可以每次读取一行内容</strong></li>
<li>调用<strong>readlines()一次读取所有内容并按行返回list</strong>。<br>
因此，要根据需要决定怎么调用。</li>
</ol>
<p>如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便：</p>
<pre><code>for line in f.readlines():
    print(line.strip()) # 把末尾的'\n'删掉
</code></pre>
<h3 id="file-like-object">file-like Object</h3>
<p>像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。<strong>file-like Object不要求从特定类继承，只要写个read()方法就行</strong>。</p>
<p><strong>StringIO就是在内存中创建的file-like Object，常用作临时缓冲</strong>。</p>
<h3 id="二进制文件">二进制文件</h3>
<p>前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要<strong>读取二进制文件</strong>，比如图片、视频等等，<strong>用'rb'模式</strong>打开文件即可：</p>
<pre><code>&gt;&gt;&gt; f = open('/Users/michael/test.jpg', 'rb')
&gt;&gt;&gt; f.read()
b'\xff\xd8\xff\xe1\x00\x18Exif\x00\x00...' # 十六进制表示的字节
字符编码
</code></pre>
<p>要读取<strong>非UTF-8编码</strong>的文本文件，需要<strong>给open()函数传入encoding参数</strong>，例如，读取GBK编码的文件：</p>
<pre><code>&gt;&gt;&gt; f = open('/Users/michael/gbk.txt', 'r', encoding='gbk')
&gt;&gt;&gt; f.read()
'测试'
</code></pre>
<p>遇到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能<strong>夹杂了一些非法编码的字符</strong>。遇到这种情况，open()函数还接收一个<strong>errors参数</strong>，表示如果<strong>遇到编码错误后如何处理</strong>。最简单的方式是直接忽略：</p>
<pre><code>&gt;&gt;&gt; f = open('/Users/michael/gbk.txt', 'r', encoding='gbk', errors='ignore')

</code></pre>
<h3 id="写文件">写文件</h3>
<p>写文件和读文件是一样的，唯一区别是<strong>调用open()函数时，传入标识符'w'或者'wb'表示写文本文件或写二进制文件</strong>：</p>
<pre><code>&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'w')
&gt;&gt;&gt; f.write('Hello, world!')
&gt;&gt;&gt; f.close()
</code></pre>
<p>你可以反复调用write()来写入文件，但是<strong>务必要调用f.close()来关闭文件</strong>。当我们<strong>写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入</strong>。<br>
<strong>只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘</strong>。<strong>忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了</strong>。所以，<strong>还是用with语句来得保险</strong>：</p>
<pre><code>with open('/Users/michael/test.txt', 'w') as f:
    f.write('Hello, world!')
</code></pre>
<p>要<strong>写入特定编码的文本文件</strong>，请给open()函数传入<strong>encoding参数</strong>，将字符串自动转换成指定编码。</p>
<p>细心的童鞋会发现，<strong>以'w'模式写入文件时，如果文件已存在，会直接覆盖</strong>（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以<strong>传入'a'以追加（append）模式写入</strong>。</p>
<pre><code>with open('/Users/michael/test.txt', 'a') as f:
    f.write('Hello, world!')
</code></pre>
<p>所有模式的定义及含义可以参考Python的官方文档。</p>
<h3 id="windows路径问题">windows路径问题</h3>
<p>文件路径不能用反斜杠‘\’。举个例子，如果我传入的文件路径是这样的：</p>
<p><code>sys.path.append('c:\Users\mshacxiang\VScode_project\web_ddt')</code></p>
<p>则会报错<code>SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: tr</code></p>
<p>原因分析：在windows系统当中读取文件路径可以使用,但是在python字符串中\有转义的含义，如\t可代表TAB，\n代表换行，所以我们需要采取一些方式使得\不被解读为转义字符。目前有3个解决方案</p>
<p>1、在路径前面加r，即保持字符原始值的意思。</p>
<p><code>sys.path.append(r'c:\Users\mshacxiang\VScode_project\web_ddt')</code></p>
<p>2、替换为双反斜杠</p>
<p><code>sys.path.append('c:\\Users\\mshacxiang\\VScode_project\\web_ddt')</code></p>
<p>3、替换为正斜杠</p>
<p><code>sys.path.append('c:/Users/mshacxiang/VScode_project/web_ddt')</code></p>
<h2 id="stringio和bytesio">StringIO和BytesIO</h2>
<h3 id="stringio">StringIO</h3>
<p>很多时候，数据读写不一定是文件，也可以在内存中读写。</p>
<p><strong>StringIO顾名思义就是在内存中读写str。类似于C++的iostringstream</strong></p>
<p>要把str写入StringIO，我们需要先创建一个StringIO，然后，像文件一样写入即可：</p>
<pre><code>&gt;&gt;&gt; from io import StringIO

&gt;&gt;&gt; f = StringIO()
&gt;&gt;&gt; f.write('hello')
5
&gt;&gt;&gt; f.write(' ')
1
&gt;&gt;&gt; f.write('world!')
6
&gt;&gt;&gt; print(f.getvalue())
hello world!
</code></pre>
<p><strong>getvalue()方法用于获得写入后的str</strong>。</p>
<p>要读取StringIO，可以用一个str初始化StringIO，然后，像读文件一样读取：</p>
<pre><code>&gt;&gt;&gt; from io import StringIO

&gt;&gt;&gt; f = StringIO('Hello!\nHi!\nGoodbye!')
&gt;&gt;&gt; while True:
...     s = f.readline()
...     if s == '':
...         break
...     print(s.strip())
...
Hello!
Hi!
Goodbye!

</code></pre>
<h3 id="bytesio">BytesIO</h3>
<p>StringIO操作的只能是str，如果要操作<strong>二进制数据，就需要使用BytesIO</strong>。</p>
<p>BytesIO实现了在内存中读写bytes，我们创建一个BytesIO，然后写入一些bytes：</p>
<pre><code>&gt;&gt;&gt; from io import BytesIO
&gt;&gt;&gt; f = BytesIO()
&gt;&gt;&gt; f.write('中文'.encode('utf-8'))
6
&gt;&gt;&gt; print(f.getvalue())
b'\xe4\xb8\xad\xe6\x96\x87'
</code></pre>
<p>请注意，<strong>写入的不是str，而是经过UTF-8编码的bytes</strong>。</p>
<p>和StringIO类似，可以用一个bytes初始化BytesIO，然后，像读文件一样读取：</p>
<pre><code>&gt;&gt;&gt; from io import BytesIO
&gt;&gt;&gt; f = BytesIO(b'\xe4\xb8\xad\xe6\x96\x87')
&gt;&gt;&gt; f.read()
b'\xe4\xb8\xad\xe6\x96\x87'
</code></pre>
<h3 id="小结">小结</h3>
<p>StringIO和BytesIO是在内存中操作str和bytes的方法，使得和读写文件具有一致的接口。</p>
<h2 id="操作文件和目录">操作文件和目录</h2>
<p>如果我们要操作文件、目录，可以在命令行下面输入操作系统提供的各种命令来完成。比如dir、cp等命令。</p>
<p>如果要在Python程序中执行这些目录和文件的操作怎么办？其实<strong>操作系统提供的命令只是简单地调用了操作系统提供的接口函数，Python内置的os模块也可以直接调用操作系统提供的接口函数。</strong></p>
<p>打开Python交互式命令行，我们来看看如何使用os模块的基本功能：</p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; os.name # 操作系统类型
'posix'
</code></pre>
<p>如果是posix，说明系统是Linux、Unix或Mac OS X，如果是nt，就是Windows系统。</p>
<p>要获取详细的系统信息，可以调用uname()函数：</p>
<pre><code>&gt;&gt;&gt; os.uname()
posix.uname_result(sysname='Darwin', nodename='MichaelMacPro.local', release='14.3.0', version='Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64', machine='x86_64')
</code></pre>
<p>注意不要漏掉括号，否则就是返回函数类型<br>
注意uname()函数在Windows上不提供，也就是说，os模块的某些函数是跟操作系统相关的。</p>
<h3 id="环境变量">环境变量</h3>
<p>在操作系统中定义的<strong>环境变量，全部保存在os.environ这个变量中</strong>，可以直接查看：</p>
<pre><code>&gt;&gt;&gt; os.environ
environ({'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'michael', 'USER': 'michael', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...})
</code></pre>
<p>要获取某个环境变量的值，可以调用os.environ.get('key')：</p>
<pre><code>&gt;&gt;&gt; os.environ.get('PATH')
'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin'
&gt;&gt;&gt; os.environ.get('x', 'default')
'default'
</code></pre>
<h3 id="操作文件和目录-2">操作文件和目录</h3>
<p>操作文件和目录的函数一部分放在<strong>os模块中</strong>，一部分放在<strong>os.path模块</strong>中，这一点要注意一下。</p>
<p>查看、创建和删除目录可以这么调用：</p>
<pre><code># 查看当前目录的绝对路径:
&gt;&gt;&gt; os.path.abspath('.')
'/Users/michael'

# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:
&gt;&gt;&gt; os.path.join('/Users/michael', 'testdir')
'/Users/michael/testdir'

# 然后创建一个目录:
&gt;&gt;&gt; os.mkdir('/Users/michael/testdir')

# 删掉一个目录:
&gt;&gt;&gt; os.rmdir('/Users/michael/testdir')
</code></pre>
<p><strong>把两个路径合成一个时，不要直接拼字符串，而要通过os.path.join()函数</strong>，这样可以<strong>正确处理不同操作系统的路径分隔符</strong>。</p>
<p>在Linux/Unix/Mac下，os.path.join()返回这样的字符串：</p>
<pre><code>part-1/part-2
</code></pre>
<p>而Windows下会返回这样的字符串：</p>
<pre><code>part-1\part-2
</code></pre>
<p>同样的道理，要<strong>拆分路径</strong>时，也不要直接去拆字符串，而要<strong>通过os.path.split()函数</strong>，这样可以把一个路径拆分为两部分，<strong>后一部分总是最后级别的目录或文件名</strong>：</p>
<pre><code>&gt;&gt;&gt; os.path.split('/Users/michael/testdir/file.txt')
('/Users/michael/testdir', 'file.txt')
</code></pre>
<p><strong>os.path.splitext()可以直接让你得到文件扩展名</strong>，很多时候非常方便：</p>
<pre><code>&gt;&gt;&gt; os.path.splitext('/path/to/file.txt')
('/path/to/file', '.txt')
</code></pre>
<p>这些合并、拆分路径的函数<strong>并不要求目录和文件要真实存在</strong>，它们<strong>只对字符串进行操作</strong>。</p>
<p>文件操作使用下面的函数。假定当前目录下有一个test.txt文件：</p>
<pre><code># 对文件重命名:
&gt;&gt;&gt; os.rename('test.txt', 'test.py')
# 删掉文件:
&gt;&gt;&gt; os.remove('test.py')
</code></pre>
<p>但是<strong>复制文件的函数居然在os模块中不存在</strong>！原因是<strong>复制文件并非由操作系统提供的系统调用</strong>。理论上讲，我们通过上一节的读写文件可以完成文件复制，只不过要多写很多代码。</p>
<p>幸运的是<strong>shutil模块提供了copyfile()的函数</strong>，你还可以在shutil模块中找到很多实用函数，它们可以看做是os模块的补充。</p>
<p>最后看看如何利用Python的特性来过滤文件。比如我们要列出当前目录下的所有目录，只需要一行代码：</p>
<pre><code>&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]
['.lein', '.local', '.m2', '.npm', '.ssh', '.Trash', '.vim', 'Applications', 'Desktop', ...]
</code></pre>
<p>要列出所有的.py文件，也只需一行代码：</p>
<pre><code>&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']
['apis.py', 'config.py', 'models.py', 'pymonitor.py', 'test_db.py', 'urls.py', 'wsgiapp.py']
</code></pre>
<h3 id="小结-2">小结</h3>
<p>Python的os模块封装了操作系统的目录和文件操作，要注意这些函数有的在os模块中，有的在os.path模块中。</p>
<h2 id="序列化">序列化</h2>
<p>在程序运行的过程中，所有的变量都是在内存中，比如，定义一个dict：</p>
<pre><code>d = dict(name='Bob', age=20, score=88)
</code></pre>
<p>可以随时修改变量，比如把name改成'Bill'，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的'Bill'存储到磁盘上，下次重新运行程序，变量又被初始化为'Bob'。</p>
<p>我们<strong>把变量从内存中变成可存储或传输的过程称之为序列化</strong>，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。</p>
<p><strong>序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。</strong></p>
<p>反过来，<strong>把变量内容从序列化的对象重新读到内存里称之为反序列化</strong>，即unpickling。</p>
<p>Python提供了pickle模块来实现序列化。</p>
<p>首先，我们尝试把一个对象序列化并写入文件：</p>
<pre><code>&gt;&gt;&gt; import pickle
&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)
&gt;&gt;&gt; pickle.dumps(d)
b'\x80\x03}q\x00(X\x03\x00\x00\x00ageq\x01K\x14X\x05\x00\x00\x00scoreq\x02KXX\x04\x00\x00\x00nameq\x03X\x03\x00\x00\x00Bobq\x04u.'
</code></pre>
<p><strong>pickle.dumps()方法把任意对象序列化成一个bytes</strong>，然后，就可以把这个<strong>bytes写入文件</strong>。或者用另一个方法pickle.dump()直接把对象序列化后<strong>写入一个file-like Object</strong>：</p>
<pre><code>&gt;&gt;&gt; f = open('dump.txt', 'wb')
&gt;&gt;&gt; pickle.dump(d, f)
&gt;&gt;&gt; f.close()
</code></pre>
<p>当我们要把对象从磁盘读到内存时，可以<strong>先把内容读到一个bytes</strong>，然后<strong>用pickle.loads()方法反序列化出对象</strong>，也可以<strong>直接用pickle.load()方法从一个file-like Object中直接反序列化出对象</strong>。</p>
<p>打开另一个Python命令行来反序列化刚才保存的对象：</p>
<pre><code>&gt;&gt;&gt; f = open('dump.txt', 'rb')
&gt;&gt;&gt; d = pickle.load(f)
&gt;&gt;&gt; f.close()
&gt;&gt;&gt; d
{'age': 20, 'score': 88, 'name': 'Bob'}
变量的内容又回来了！
</code></pre>
<p>当然，这个变量和原来的变量是<strong>完全不相干的对象</strong>，它们<strong>只是内容相同而已。</strong></p>
<p>Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它<strong>只能用于Python，并且可能不同版本的Python彼此都不兼容</strong>，因此，<strong>只能用Pickle保存那些不重要的数据</strong>，不能成功地反序列化也没关系。</p>
<h3 id="json">JSON</h3>
<p>如果我们要在不同的编程语言之间传递对象，就<strong>必须把对象序列化为标准格式</strong>，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。</p>
<p>JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">JSON类型</th>
<th style="text-align:center">Python类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">{}</td>
<td style="text-align:center">dict</td>
</tr>
<tr>
<td style="text-align:center">[]</td>
<td style="text-align:center">list</td>
</tr>
<tr>
<td style="text-align:center">&quot;string&quot;</td>
<td style="text-align:center">str</td>
</tr>
<tr>
<td style="text-align:center">1234.56</td>
<td style="text-align:center">int或float</td>
</tr>
<tr>
<td style="text-align:center">true/false</td>
<td style="text-align:center">True/False</td>
</tr>
<tr>
<td style="text-align:center">null</td>
<td style="text-align:center">None</td>
</tr>
</tbody>
</table>
<p>Python<strong>内置的json模块</strong>提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON：</p>
<pre><code>&gt;&gt;&gt; import json
&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)
&gt;&gt;&gt; json.dumps(d)
'{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}'
</code></pre>
<p><strong>dumps()方法返回一个str，内容就是标准的JSON</strong>。类似的，<strong>dump()方法可以直接把JSON写入一个file-like Object</strong>。</p>
<p>要把JSON<strong>反序列化为Python对象，用loads()或者对应的load()方法</strong>，前者把<strong>JSON的字符串</strong>反序列化，后者从<strong>file-like Object中读取字符串</strong>并反序列化：</p>
<pre><code>&gt;&gt;&gt; json_str = '{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}'
&gt;&gt;&gt; json.loads(json_str)
{'age': 20, 'score': 88, 'name': 'Bob'}
</code></pre>
<p>由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。</p>
<h3 id="json进阶">JSON进阶</h3>
<p>Python的dict对象可以直接序列化为JSON的{}，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化：</p>
<pre><code>import json

class Student(object):
    def __init__(self, name, age, score):
        self.name = name
        self.age = age
        self.score = score

s = Student('Bob', 20, 88)
print(json.dumps(s))
</code></pre>
<p>运行代码，毫不留情地得到一个TypeError：</p>
<pre><code>Traceback (most recent call last):
  ...
TypeError: &lt;__main__.Student object at 0x10603cc50&gt; is not JSON serializable
</code></pre>
<p>错误的原因是<strong>Student对象不是一个可序列化为JSON的对象</strong>。</p>
<p>仔细看看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数：</p>
<p>https://docs.python.org/3/library/json.html#json.dumps</p>
<p>这些<strong>可选参数就是让我们来定制JSON序列化</strong>。前面的代码之所以无法把Student类实例序列化为JSON，是因为<strong>默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象</strong>。</p>
<p><strong>可选参数default就是把任意一个对象变成一个可序列为JSON的对象</strong>，我们只需要<strong>为Student专门写一个转换函数，再把函数传进去即可</strong>：</p>
<pre><code>def student2dict(std):
    return {
        'name': std.name,
        'age': std.age,
        'score': std.score
    }
</code></pre>
<p>这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON：</p>
<pre><code>&gt;&gt;&gt; print(json.dumps(s, default=student2dict))
{&quot;age&quot;: 20, &quot;name&quot;: &quot;Bob&quot;, &quot;score&quot;: 88}
</code></pre>
<p>不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以把任意class的实例变为dict：</p>
<pre><code>print(json.dumps(s, default=lambda obj: obj.__dict__))
</code></pre>
<p>因为<strong>通常class的实例都有一个dict属性</strong>，它就是一个dict，用来存储实例变量。也有<strong>少数例外，比如定义了slots的class</strong>。</p>
<p>同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例：</p>
<pre><code>def dict2student(d):
    return Student(d['name'], d['age'], d['score'])
</code></pre>
<p>运行结果如下：</p>
<pre><code>&gt;&gt;&gt; json_str = '{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}'
&gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student))
&lt;__main__.Student object at 0x10cd3c190&gt;
</code></pre>
<p>打印出的是反序列化的Student实例对象。</p>
<h3 id="小结-3">小结</h3>
<p>Python语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，就可以使用json模块。</p>
<p>json模块的dumps()和loads()函数是定义得非常好的接口的典范。当我们使用时，只需要传入一个必须的参数。但是，当默认的序列化或反序列机制不满足我们的要求时，我们又可以传入更多的参数来定制序列化或反序列化的规则，既做到了接口简单易用，又做到了充分的扩展性和灵活性。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python - 错误、调试和测试]]></title>
        <id>https://lixin-scut.github.io//post/python-cuo-wu-diao-shi-he-ce-shi</id>
        <link href="https://lixin-scut.github.io//post/python-cuo-wu-diao-shi-he-ce-shi">
        </link>
        <updated>2020-05-12T12:08:36.000Z</updated>
        <content type="html"><![CDATA[<p>在程序运行过程中，总会遇到各种各样的错误。</p>
<p>有的错误是程序编写有问题造成的，比如本来应该输出整数结果输出了字符串，这种错误我们通常称之为bug，bug是必须修复的。</p>
<p>有的错误是用户输入造成的，比如让用户输入email地址，结果得到一个空字符串，这种错误可以通过检查用户输入来做相应的处理。</p>
<p>还有一类错误是完全无法在程序运行过程中预测的，比如写入文件的时候，磁盘满了，写不进去了，或者从网络抓取数据，网络突然断掉了。这类错误也称为异常，在程序中通常是必须处理的，否则，程序会因为各种问题终止并退出。</p>
<p>Python内置了一套异常处理机制，来帮助我们进行错误处理。</p>
<p>此外，我们也需要跟踪程序的执行，查看变量的值是否正确，这个过程称为调试。Python的pdb可以让我们以单步方式执行代码。</p>
<p>最后，编写测试也很重要。有了良好的测试，就可以在程序修改后反复运行，确保程序输出符合我们编写的测试。</p>
<h2 id="错误处理">错误处理</h2>
<p>在程序运行的过程中，如果发生了错误，可以事先约定返回一个错误代码，这样，就可以知道是否有错，以及出错的原因。在操作系统提供的调用中，返回错误码非常常见。比如打开文件的函数open()，成功时返回文件描述符（就是一个整数），出错时返回-1。</p>
<p>用错误码来表示是否出错十分不便，因为函数本身应该返回的正常结果和错误码混在一起，造成调用者必须用大量的代码来判断是否出错：</p>
<pre><code>def foo():
    r = some_function()
    if r==(-1):
        return (-1)
    # do something
    return r

def bar():
    r = foo()
    if r==(-1):
        print('Error')
    else:
        pass
</code></pre>
<p>一旦出错，还要一级一级上报，直到某个函数可以处理该错误（比如，给用户输出一个错误信息）。</p>
<h3 id="tryexceptfinally">try...except...finally...</h3>
<p>所以高级语言通常都内置了一套try...except...finally...的错误处理机制，Python也不例外。</p>
<h3 id="try">try</h3>
<p>让我们用一个例子来看看try的机制：</p>
<pre><code>try:
    print('try...')
    r = 10 / 0
    print('result:', r)
except ZeroDivisionError as e:
    print('except:', e)
finally:
    print('finally...')
print('END')
</code></pre>
<p>当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，<strong>执行完except后，如果有finally语句块，则执行finally语句块</strong>，至此，执行完毕。</p>
<p>上面的代码在计算10 / 0时会产生一个除法运算错误：</p>
<pre><code>try...
except: division by zero
finally...
END
</code></pre>
<p>从输出可以看到，当错误发生时，后续语句print('result:', r)不会被执行，except由于捕获到ZeroDivisionError，因此被执行。最后，finally语句被执行。然后，程序继续按照流程往下走。</p>
<p>如果把除数0改成2，则执行结果如下：</p>
<pre><code>try...
result: 5
finally...
END
</code></pre>
<p>由于没有错误发生，所以except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。</p>
<p>你还可以猜测，错误应该有很多种类，如果发生了不同类型的错误，应该由不同的except语句块处理。没错，可以<strong>有多个except来捕获不同类型的错误</strong>：</p>
<pre><code>try:
    print('try...')
    r = 10 / int('a')
    print('result:', r)
except ValueError as e:
    print('ValueError:', e)
except ZeroDivisionError as e:
    print('ZeroDivisionError:', e)
finally:
    print('finally...')
print('END')
</code></pre>
<p>int()函数可能会抛出ValueError，所以我们用一个except捕获ValueError，用另一个except捕获ZeroDivisionError。</p>
<h3 id="else">else</h3>
<p>此外，如果没有错误发生，可以<strong>在except语句块后面加一个else，当没有错误发生时，会自动执行else语句</strong>：</p>
<pre><code>try:
    print('try...')
    r = 10 / int('2')
    print('result:', r)
except ValueError as e:
    print('ValueError:', e)
except ZeroDivisionError as e:
    print('ZeroDivisionError:', e)
else:
    print('no error!')
finally:
    print('finally...')
print('END')
</code></pre>
<h3 id="错误类">错误类</h3>
<p>Python的<strong>错误其实也是class</strong>，<strong>所有的错误类型都继承自BaseException</strong>，所以在使用except时需要注意的是，它<strong>不但捕获该类型的错误，还把其子类也“一网打尽”</strong>。比如：</p>
<pre><code>try:
    foo()
except ValueError as e:
    print('ValueError')
except UnicodeError as e:
    print('UnicodeError')
</code></pre>
<p>第二个except<strong>永远也捕获不到UnicodeError</strong>，<strong>因为UnicodeError是ValueError的子类，如果有，也被第一个except给捕获了</strong>。</p>
<p><strong>Python所有的错误都是从BaseException类派生</strong>的，常见的错误类型和继承关系看这里：</p>
<p><a href="https://docs.python.org/3/library/exceptions.html#exception-hierarchy">错误类型和继承关系</a></p>
<h3 id="跨越多层调用">跨越多层调用</h3>
<p>使用try...except捕获错误还有一个巨大的好处，就是可以<strong>跨越多层调用</strong>，比如函数main()调用bar()，bar()调用foo()，结果foo()出错了，这时，<strong>只要main()捕获到了，就可以处理</strong>：</p>
<pre><code>def foo(s):
    return 10 / int(s)

def bar(s):
    return foo(s) * 2

def main():
    try:
        bar('0')
    except Exception as e:
        print('Error:', e)
    finally:
        print('finally...')
</code></pre>
<p>也就是说，<strong>不需要在每个可能出错的地方去捕获错误，只要在合适的层次去捕获错误就可以了</strong>。这样一来，就大大减少了写try...except...finally的麻烦。</p>
<h3 id="调用栈">调用栈</h3>
<p><strong>如果错误没有被捕获，它就会一直往上抛，最后被Python解释器捕获，打印一个错误信息，然后程序退出</strong>。</p>
<p>来看看err.py：</p>
<pre><code># err.py:
def foo(s):
    return 10 / int(s)

def bar(s):
    return foo(s) * 2

def main():
    bar('0')

main()
</code></pre>
<p>执行，结果如下：</p>
<pre><code>$ python3 err.py
Traceback (most recent call last):
  File &quot;err.py&quot;, line 11, in &lt;module&gt;
    main()
  File &quot;err.py&quot;, line 9, in main
    bar('0')
  File &quot;err.py&quot;, line 6, in bar
    return foo(s) * 2
  File &quot;err.py&quot;, line 3, in foo
    return 10 / int(s)
ZeroDivisionError: division by zero
</code></pre>
<p>解读错误信息是定位错误的关键。我们从上往下可以看到整个错误的调用函数链：</p>
<p>错误信息第1行：</p>
<pre><code>Traceback (most recent call last):
</code></pre>
<p>告诉我们这是错误的跟踪信息。</p>
<p>第2~3行：</p>
<pre><code>  File &quot;err.py&quot;, line 11, in &lt;module&gt;
    main()
</code></pre>
<p>调用main()出错了，在代码文件err.py的第11行代码，但原因是第9行：</p>
<pre><code>  File &quot;err.py&quot;, line 9, in main
    bar('0')
</code></pre>
<p>调用bar('0')出错了，在代码文件err.py的第9行代码，但原因是第6行：</p>
<pre><code>  File &quot;err.py&quot;, line 6, in bar
    return foo(s) * 2
</code></pre>
<p>原因是return foo(s) * 2这个语句出错了，但这还不是最终原因，继续往下看：</p>
<pre><code>File &quot;err.py&quot;, line 3, in foo
   return 10 / int(s)
</code></pre>
<p>原因是return 10 / int(s)这个语句出错了，这是错误产生的源头，因为下面打印了：</p>
<pre><code>ZeroDivisionError: integer division or modulo by zero
</code></pre>
<p>根据错误类型ZeroDivisionError，我们判断，int(s)本身并没有出错，但是int(s)返回0，在计算10 / 0时出错，至此，找到错误源头。</p>
<p>出错的时候，一定要分析错误的调用栈信息，才能定位错误的位置。</p>
<h3 id="记录错误logging">记录错误logging</h3>
<p>如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以<strong>把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去</strong>。</p>
<p>Python内置的logging模块可以非常容易地记录错误信息：</p>
<pre><code>
# err_logging.py

import logging

def foo(s):
    return 10 / int(s)

def bar(s):
    return foo(s) * 2

def main():
    try:
        bar('0')
    except Exception as e:
        logging.exception(e)

main()
print('END')

</code></pre>
<p>同样是出错，但<strong>程序打印完错误信息后会继续执行，并正常退出</strong>：</p>
<pre><code>$ python3 err_logging.py
ERROR:root:division by zero
Traceback (most recent call last):
  File &quot;err_logging.py&quot;, line 13, in main
    bar('0')
  File &quot;err_logging.py&quot;, line 9, in bar
    return foo(s) * 2
  File &quot;err_logging.py&quot;, line 6, in foo
    return 10 / int(s)
ZeroDivisionError: division by zero
END
</code></pre>
<p><strong>通过配置，logging还可以把错误记录到日志文件里，方便事后排查。</strong></p>
<h3 id="抛出错误">抛出错误</h3>
<p>因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，<strong>错误并不是凭空产生的，而是有意创建并抛出的</strong>。Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。</p>
<p>如果要抛出错误，首先根据需要，<strong>可以定义一个错误的class，选择好继承关系</strong>，然后，用<strong>raise语句抛出一个错误的实例</strong>：</p>
<pre><code>
# err_raise.py
class FooError(ValueError):
    pass

def foo(s):
    n = int(s)
    if n==0:
        raise FooError('invalid value: %s' % s)
    return 10 / n

foo('0')
</code></pre>
<p>执行，可以最后跟踪到我们自己定义的错误：</p>
<pre><code>$ python3 err_raise.py 
Traceback (most recent call last):
  File &quot;err_throw.py&quot;, line 11, in &lt;module&gt;
    foo('0')
  File &quot;err_throw.py&quot;, line 8, in foo
    raise FooError('invalid value: %s' % s)
__main__.FooError: invalid value: 0

</code></pre>
<p><strong>只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。</strong></p>
<p>最后，我们来看另一种错误处理的方式：</p>
<pre><code>
# err_reraise.py

def foo(s):
    n = int(s)
    if n==0:
        raise ValueError('invalid value: %s' % s)
    return 10 / n

def bar():
    try:
        foo('0')
    except ValueError as e:
        print('ValueError!')
        raise

bar()

</code></pre>
<p>在bar()函数中，我们明明已经捕获了错误，但是，<strong>打印一个ValueError!后，又把错误通过raise语句抛出去了</strong></p>
<p>其实这种错误处理方式相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，<strong>由于当前函数不知道应该怎么处理该错误</strong>，所以，<strong>最恰当的方式是继续往上抛，让顶层调用者去处理</strong>。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终会抛给CEO去处理。</p>
<p><strong>raise语句如果不带参数，就会把当前错误原样抛出</strong>。</p>
<p>此外，<strong>在except中raise一个Error，还可以把一种类型的错误转化成另一种类型</strong>：</p>
<pre><code>try:
    10 / 0
except ZeroDivisionError:
    raise ValueError('input error!')
</code></pre>
<p>只要是<strong>合理的转换逻辑</strong>就可以，但是，<strong>决不应该把一个IOError转换成毫不相干的ValueError</strong>。</p>
<h3 id="小结">小结</h3>
<p>Python内置的try...except...finally用来处理错误十分方便。出错时，会分析错误信息并定位错误发生的代码位置才是最关键的。</p>
<p>程序也可以主动抛出错误，让调用者来处理相应的错误。但是，<strong>应该在文档中写清楚可能会抛出哪些错误，以及错误产生的原因</strong>。</p>
<p><strong>个人代码练习</strong><br>
运行下面的代码，根据异常信息进行分析，定位出错误源头，并修复：</p>
<pre><code>
# -*- coding: utf-8 -*-
from functools import reduce

def str2num(s):
    # return int(s)
    try:
        return int(s)
    except ValueError:
        return float(s)

def calc(exp):
    ss = exp.split('+')
    ns = map(str2num, ss)
    return reduce(lambda acc, x: acc + x, ns)

def main():
    r = calc('100 + 200 + 345')
    print('100 + 200 + 345 =', r)
    r = calc('99 + 88 + 7.6')
    print('99 + 88 + 7.6 =', r)

main()

</code></pre>
<h2 id="调试">调试</h2>
<p>程序能一次写完并正常运行的概率很小，总会有各种各样的bug需要修正。有的bug很简单，看看错误信息就知道，有的bug很复杂，我们需要知道出错时，哪些变量的值是正确的，哪些变量的值是错误的，因此，需要<strong>一整套调试程序的手段来修复bug</strong>。</p>
<h3 id="打印变量">打印变量</h3>
<p>第一种方法简单直接粗暴有效，就是用print()把可能有问题的变量打印出来看看：</p>
<pre><code>def foo(s):
    n = int(s)
    print('&gt;&gt;&gt; n = %d' % n)
    return 10 / n

def main():
    foo('0')

main()
</code></pre>
<p>执行后在输出中查找打印的变量值：</p>
<pre><code>$ python err.py
&gt;&gt;&gt; n = 0
Traceback (most recent call last):
  ...
ZeroDivisionError: integer division or modulo by zero
</code></pre>
<p><strong>用print()最大的坏处是将来还得删掉它</strong>，想想程序里到处都是print()，运行结果也会包含很多垃圾信息。所以，我们又有第二种方法。</p>
<h3 id="断言">断言</h3>
<p><strong>凡是用print()来辅助查看的地方，都可以用断言（assert）来替代</strong>：</p>
<pre><code>def foo(s):
    n = int(s)
    assert n != 0, 'n is zero!'
    return 10 / n

def main():
    foo('0')
</code></pre>
<p><strong>assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错</strong>。</p>
<p><strong>如果断言失败，assert语句本身就会抛出AssertionError</strong>：</p>
<pre><code>$ python err.py
Traceback (most recent call last):
  ...
AssertionError: n is zero!
</code></pre>
<p>程序中如果到处充斥着assert，和print()相比也好不到哪去。<br>
不过，启<strong>动Python解释器时可以用-O参数来关闭assert</strong>：</p>
<pre><code>$ python -O err.py
Traceback (most recent call last):
  ...
ZeroDivisionError: division by zero
</code></pre>
<p><strong>注意：断言的开关“-O”是英文大写字母O，不是数字0。</strong><br>
<strong>关闭后，你可以把所有的assert语句当成pass来看</strong>。</p>
<h3 id="logging">logging</h3>
<p>把print()替换为logging是第3种方式，和assert比，<strong>logging不会抛出错误，而且可以输出到文件</strong>：</p>
<pre><code>import logging

s = '0'
n = int(s)
logging.info('n = %d' % n)
print(10 / n)
</code></pre>
<p>logging.info()就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何输出信息。</p>
<p>然后再在import logging之后添加一行配置再试试：</p>
<pre><code>import logging
logging.basicConfig(level=logging.INFO)
</code></pre>
<p>可以看到输出了：</p>
<pre><code>$ python err.py
INFO:root:n = 0
Traceback (most recent call last):
  File &quot;err.py&quot;, line 8, in &lt;module&gt;
    print(10 / n)
ZeroDivisionError: division by zero
</code></pre>
<p><strong>信息级别</strong><br>
这就是logging的好处，它<strong>允许你指定记录信息的级别，有debug，info，warning，error等几个级别</strong>，<strong>当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了</strong>。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。</p>
<p>logging的另一个好处是通过简单的配置，<strong>一条语句可以同时输出到不同的地方</strong>，比如console和文件。</p>
<h3 id="pdb">pdb</h3>
<p>第4种方式是启动Python的<strong>调试器pdb</strong>，<strong>让程序以单步方式运行，可以随时查看运行状态</strong>。</p>
<p>准备好程序：</p>
<pre><code># err.py
s = '0'
n = int(s)
print(10 / n)
</code></pre>
<p>然后启动：</p>
<pre><code>$ python -m pdb err.py
&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(2)&lt;module&gt;()
-&gt; s = '0'
</code></pre>
<p>以<strong>参数-m pdb</strong>启动后，pdb定位到下一步要执行的代码-&gt; s = '0'。</p>
<p><strong>输入命令l来查看代码</strong>：</p>
<pre><code>(Pdb) l
  1     # err.py
  2  -&gt; s = '0'
  3     n = int(s)
  4     print(10 / n)
</code></pre>
<p><strong>输入命令n可以单步执行代码</strong>：</p>
<pre><code>(Pdb) n
&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(3)&lt;module&gt;()
-&gt; n = int(s)
(Pdb) n
&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(4)&lt;module&gt;()
-&gt; print(10 / n)
</code></pre>
<p>任何时候都可以<strong>输入命令p 变量名来查看变量</strong>：</p>
<pre><code>(Pdb) p s
'0'
(Pdb) p n
0
</code></pre>
<p><strong>输入命令q结束调试</strong>，退出程序：</p>
<pre><code>(Pdb) q
</code></pre>
<p>这种通过pdb在命令行调试的方法理论上是万能的，但实在是太麻烦了，如果有一千行代码，要运行到第999行得敲许多重复命令命令啊。还好，我们还有另一种调试方法。</p>
<h3 id="断点-pdbset_trace">断点 pdb.set_trace()</h3>
<p>这个方法也是用pdb，但是不需要单步执行，我们<strong>只需要import pdb</strong>，然后，<strong>在可能出错的地方放一个pdb.set_trace()</strong>，就可以设置一个断点：</p>
<pre><code># err.py
import pdb

s = '0'
n = int(s)
pdb.set_trace() # 运行到这里会自动暂停
print(10 / n)
</code></pre>
<p>运行代码，<strong>程序会自动在pdb.set_trace()暂停并进入pdb调试环境</strong>，可以用命令p查看变量，或者<strong>用命令c继续运行</strong>：</p>
<pre><code>$ python err.py 
&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(7)&lt;module&gt;()
-&gt; print(10 / n)
(Pdb) p n
0
(Pdb) c
Traceback (most recent call last):
  File &quot;err.py&quot;, line 7, in &lt;module&gt;
    print(10 / n)
ZeroDivisionError: division by zero
</code></pre>
<p>这个方式比直接启动pdb单步调试效率要高很多，但也高不到哪去。</p>
<h3 id="ide">IDE</h3>
<p>如果要比较快捷地设置断点、单步执行，就需要一个支持调试功能的IDE。目前比较好的Python IDE有：</p>
<p><a href="https://code.visualstudio.com/">Visual Studio Code</a> 注意需要安装Python插件。</p>
<p><a href="http://www.jetbrains.com/pycharm/">PyCharm</a></p>
<p>另外，Eclipse加上pydev插件也可以调试Python程序。</p>
<p>虽然用IDE调试起来比较方便，但是最后你会发现，logging才是终极武器。</p>
<h2 id="单元测试">单元测试</h2>
<p>如果你听说过“测试驱动开发”（TDD：Test-Driven Development），单元测试就不陌生。</p>
<p><strong>单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作</strong>。</p>
<p>比如对函数abs()，我们可以编写出以下几个测试用例：</p>
<p>输入正数，比如1、1.2、0.99，期待返回值与输入相同；</p>
<p>输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反；</p>
<p>输入0，期待返回0；</p>
<p>输入非数值类型，比如None、[]、{}，期待抛出TypeError。</p>
<p>把上面的测试用例放到一个测试模块里，就是一个完整的单元测试。</p>
<p>如果单元测试通过，说明我们测试的这个函数能够正常工作。如果单元测试不通过，要么函数有bug，要么测试条件输入不正确，总之，需要修复使单元测试能够通过。</p>
<p>单元测试通过后有什么意义呢？如果我们对abs()函数代码做了修改，只需要再跑一遍单元测试，如果通过，说明我们的修改不会对abs()函数原有的行为造成影响，如果测试不通过，说明我们的修改与原有行为不一致，要么修改代码，要么修改测试。</p>
<p>这种以测试为驱动的开发模式最大的好处就是<strong>确保一个程序模块的行为符合我们设计的测试用例</strong>。在将来修改的时候，可以极大程度地保证该模块行为仍然是正确的。</p>
<p>编写一个Dict类，这个类的行为和dict一致，但是可以通过属性来访问，用起来就像下面这样：</p>
<pre><code>&gt;&gt;&gt; d = Dict(a=1, b=2)
&gt;&gt;&gt; d['a']
1
&gt;&gt;&gt; d.a
1
</code></pre>
<p>mydict.py代码如下：</p>
<pre><code>class Dict(dict):

    def __init__(self, **kw):
        super().__init__(**kw)

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(r&quot;'Dict' object has no attribute '%s'&quot; % key)

    def __setattr__(self, key, value):
        self[key] = value
</code></pre>
<h3 id="unittest模块">unittest模块</h3>
<p>为了编写单元测试，我们需要引入Python自带的unittest模块，编写mydict_test.py如下：</p>
<pre><code>import unittest

from mydict import Dict

class TestDict(unittest.TestCase):

    def test_init(self):
        d = Dict(a=1, b='test')
        self.assertEqual(d.a, 1)
        self.assertEqual(d.b, 'test')
        self.assertTrue(isinstance(d, dict))

    def test_key(self):
        d = Dict()
        d['key'] = 'value'
        self.assertEqual(d.key, 'value')

    def test_attr(self):
        d = Dict()
        d.key = 'value'
        self.assertTrue('key' in d)
        self.assertEqual(d['key'], 'value')

    def test_keyerror(self):
        d = Dict()
        with self.assertRaises(KeyError):
            value = d['empty']

    def test_attrerror(self):
        d = Dict()
        with self.assertRaises(AttributeError):
            value = d.empty
</code></pre>
<p>编写单元测试时，我们<strong>需要编写一个测试类，从unittest.TestCase继承</strong>。</p>
<p><strong>以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。</strong></p>
<p>对每一类测试都需要编写一个test_xxx()方法。由于<strong>unittest.TestCase提供了很多内置的条件判断</strong>，我们只需要调用这些方法就可以断言输出是否是我们所期望的。<strong>最常用的断言就是assertEqual()</strong>：</p>
<pre><code>self.assertEqual(abs(-1), 1) # 断言函数返回的结果与1相等
</code></pre>
<p>另一种重要的断言就是assertRaises，期待抛出指定类型的Error，比如通过d['empty']访问不存在的key时，断言会抛出KeyError：</p>
<pre><code>with self.assertRaises(KeyError):
    value = d['empty']
</code></pre>
<p>而通过d.empty访问不存在的key时，我们期待抛出AttributeError：</p>
<pre><code>with self.assertRaises(AttributeError):
    value = d.empty
</code></pre>
<h3 id="运行单元测试">运行单元测试</h3>
<p>一旦编写好单元测试，我们就可以运行单元测试。最简单的运行方式是在<code>mydict_test.py</code>的最后加上两行代码：</p>
<pre><code>if __name__ == '__main__':
    unittest.main()
</code></pre>
<p>这样就可以把<code>mydict_test.py</code>当做正常的python脚本运行：</p>
<pre><code>$ python mydict_test.py
</code></pre>
<p>另一种方法是在命令行通过参数<code>-m unittest</code>直接运行单元测试：</p>
<pre><code>$ python -m unittest mydict_test
.....
----------------------------------------------------------------------
Ran 5 tests in 0.000s

OK
</code></pre>
<p>这是推荐的做法，因为这样<strong>可以一次批量运行很多单元测试</strong>，并且，<strong>有很多工具可以自动来运行这些单元测试</strong>。</p>
<h3 id="setup与teardown">setUp与tearDown</h3>
<p>可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别<strong>在每调用一个测试方法的前后</strong>分别被执行。</p>
<p>设想你的测试需要启动一个数据库，这时，<strong>就可以在setUp()方法中连接数据库，在tearDown()方法中关闭数据库</strong>，这样，不必在每个测试方法中重复相同的代码：</p>
<pre><code>class TestDict(unittest.TestCase):

    def setUp(self):
        print('setUp...')

    def tearDown(self):
        print('tearDown...')
</code></pre>
<p>再次运行测试看看每个测试方法调用前后是否会打印出setUp...和tearDown...。</p>
<p><strong>个人练习代码</strong><br>
对Student类编写单元测试，结果发现测试不通过，请修改Student类，让测试通过：</p>
<pre><code># -*- coding: utf-8 -*-
import unittest

class Student(object):
    def __init__(self, name, score):
        self.name = name
        self.score = score
    def get_grade(self):
        if self.score &lt; 0 or self.score &gt; 100:
            raise ValueError
        if self.score &gt;= 80:
            return 'A'
        if self.score &gt;= 60:
            return 'B'
        return 'C'
				
		class TestStudent(unittest.TestCase):

    def test_80_to_100(self):
        s1 = Student('Bart', 80)
        s2 = Student('Lisa', 100)
        self.assertEqual(s1.get_grade(), 'A')
        self.assertEqual(s2.get_grade(), 'A')

    def test_60_to_80(self):
        s1 = Student('Bart', 60)
        s2 = Student('Lisa', 79)
        self.assertEqual(s1.get_grade(), 'B')
        self.assertEqual(s2.get_grade(), 'B')

    def test_0_to_60(self):
        s1 = Student('Bart', 0)
        s2 = Student('Lisa', 59)
        self.assertEqual(s1.get_grade(), 'C')
        self.assertEqual(s2.get_grade(), 'C')

    def test_invalid(self):
        s1 = Student('Bart', -1)
        s2 = Student('Lisa', 101)
        with self.assertRaises(ValueError):
            s1.get_grade()
        with self.assertRaises(ValueError):
            s2.get_grade()

if __name__ == '__main__':
    unittest.main()
</code></pre>
<h2 id="文档测试">文档测试</h2>
<p>如果你经常阅读Python的官方文档，可以看到很多文档都有示例代码。比如re模块就带了很多示例代码：</p>
<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; m = re.search('(?&lt;=abc)def', 'abcdef')
&gt;&gt;&gt; m.group(0)
'def'
</code></pre>
<p>可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。</p>
<p>这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以粘贴出来直接运行，那么，可不可以自动执行写在注释中的这些代码呢？</p>
<p>答案是肯定的。</p>
<p>当我们编写注释时，如果写上这样的注释：</p>
<pre><code>def abs(n):
    '''
    Function to get absolute value of number.
    
    Example:
    
    &gt;&gt;&gt; abs(1)
    1
    &gt;&gt;&gt; abs(-1)
    1
    &gt;&gt;&gt; abs(0)
    0
    '''
    return n if n &gt;= 0 else (-n)
</code></pre>
<p>无疑更明确地告诉函数的调用者该函数的期望输入和输出。</p>
<p><strong>Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试</strong>。</p>
<p>doctest严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用...表示中间一大段烦人的输出。</p>
<p>让我们用doctest来测试上次编写的Dict类：</p>
<pre><code># mydict2.py
class Dict(dict):
    # 注意在class内部
    '''
    Simple dict but also support access as x.y style.

    &gt;&gt;&gt; d1 = Dict()
    &gt;&gt;&gt; d1['x'] = 100
    &gt;&gt;&gt; d1.x
    100
    &gt;&gt;&gt; d1.y = 200
    &gt;&gt;&gt; d1['y']
    200
    &gt;&gt;&gt; d2 = Dict(a=1, b=2, c='3')
    &gt;&gt;&gt; d2.c
    '3'
    &gt;&gt;&gt; d2['empty']
    Traceback (most recent call last):
        ...
    KeyError: 'empty'
    &gt;&gt;&gt; d2.empty
    Traceback (most recent call last):
        ...
    AttributeError: 'Dict' object has no attribute 'empty'
    '''
    def __init__(self, **kw):
        super(Dict, self).__init__(**kw)

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(r&quot;'Dict' object has no attribute '%s'&quot; % key)

    def __setattr__(self, key, value):
        self[key] = value

if __name__=='__main__':
    import doctest
    doctest.testmod()
</code></pre>
<p>运行<code>python mydict2.py</code>：</p>
<pre><code>$ python mydict2.py
</code></pre>
<p>什么输出也没有。<strong>这说明我们编写的doctest运行都是正确的。如果程序有问题，比如把getattr()方法注释掉，再运行就会报错</strong>：</p>
<pre><code>$ python mydict2.py
**********************************************************************
File &quot;/Users/michael/Github/learn-python3/samples/debug/mydict2.py&quot;, line 10, in __main__.Dict
Failed example:
    d1.x
Exception raised:
    Traceback (most recent call last):
      ...
    AttributeError: 'Dict' object has no attribute 'x'
**********************************************************************
File &quot;/Users/michael/Github/learn-python3/samples/debug/mydict2.py&quot;, line 16, in __main__.Dict
Failed example:
    d2.c
Exception raised:
    Traceback (most recent call last):
      ...
    AttributeError: 'Dict' object has no attribute 'c'
**********************************************************************
1 items had failures:
   2 of   9 in __main__.Dict
***Test Failed*** 2 failures.
</code></pre>
<p><strong>注意到最后3行代码。当模块正常导入时，doctest不会被执行。只有在命令行直接运行时，才执行doctest。所以，不必担心doctest会在非测试环境下执行。</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Git - 自定义Git]]></title>
        <id>https://lixin-scut.github.io//post/git-zi-ding-yi-git</id>
        <link href="https://lixin-scut.github.io//post/git-zi-ding-yi-git">
        </link>
        <updated>2020-05-12T07:39:26.000Z</updated>
        <content type="html"><![CDATA[<p>在安装Git一节中，我们已经配置了user.name和user.email，实际上，Git还有很多可配置项。</p>
<p>比如，让Git显示颜色，会让命令输出看起来更醒目：</p>
<pre><code>$ git config --global color.ui true
</code></pre>
<p>这样，Git会适当地显示不同的颜色，比如git status命令中会根据不同的状态给文件名标上不同颜色。</p>
<h2 id="忽略特殊文件">忽略特殊文件</h2>
<p>有些时候，必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件、文件夹自动生成的隐藏文件等等，每次git status都会显示Untracked files 等</p>
<h3 id="gitignore文件">.gitignore文件</h3>
<p>在Git工作区的根目录下创建一个<strong>特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件</strong>。</p>
<p>不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：<a href="https://github.com/github/gitignore">配置文件</a></p>
<p>忽略文件的原则是：</p>
<ol>
<li>忽略<strong>操作系统自动生成的文件</strong>，比如缩略图等；</li>
<li>忽略<strong>编译生成的中间文件、可执行文件</strong>等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；</li>
<li>忽略你自己的<strong>带有敏感信息的配置文件</strong>，比如存放口令的配置文件。<br>
举个例子：</li>
</ol>
<p>假设你在Windows下进行Python开发，Windows会自动在有图片的目录下生成隐藏的缩略图文件，如果有自定义目录，目录下就会有Desktop.ini文件，因此你需要忽略Windows自动生成的垃圾文件：</p>
<pre><code># Windows:
Thumbs.db
ehthumbs.db
Desktop.ini
</code></pre>
<p>然后，继续忽略Python编译产生的.pyc、.pyo、dist等文件或目录：</p>
<pre><code># Python:
*.py[cod]
*.so
*.egg
*.egg-info
dist
build
</code></pre>
<p>加上你自己定义的文件，最终得到一个完整的.gitignore文件，内容如下：</p>
<pre><code># Windows:
Thumbs.db
ehthumbs.db
Desktop.ini

# Python:
*.py[cod]
*.so
*.egg
*.egg-info
dist
build

# My configurations:
db.ini
deploy_key_rsa
</code></pre>
<p>最后一步就是把.gitignore也提交到Git，就完成了！当然<strong>检验.gitignore的标准是git status命令是不是说working directory clean</strong>。</p>
<p>使用Windows的注意：如果你在资源管理器里新建一个.gitignore文件，它会提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为.gitignore了。</p>
<h3 id="强制添加git-add-f">强制添加git add -f</h3>
<p>有些时候，你想添加一个文件到Git，但发现添加不了，原因是这个<strong>文件被.gitignore忽略了</strong>：</p>
<pre><code>$ git add App.class
The following paths are ignored by one of your .gitignore files:
App.class
Use -f if you really want to add them.
</code></pre>
<p>如果你确实想添加该文件，<strong>可以用-f强制添加到Git</strong>：</p>
<pre><code>$ git add -f App.class
</code></pre>
<h3 id="git-check-ignore命令检查">git check-ignore命令检查</h3>
<p>或者你发现，可能是.gitignore写得有问题，需要找出来到底哪个规则写错了，可以用<strong>git check-ignore命令检查</strong>：</p>
<pre><code>$ git check-ignore -v App.class
.gitignore:3:*.class	App.class
</code></pre>
<p><strong>Git会告诉我们，.gitignore的第3行规则忽略了该文件，于是我们就可以知道应该修订哪个规则</strong>。</p>
<h3 id="小结">小结</h3>
<p>忽略某些文件时，需要编写.gitignore；</p>
<p>.gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理</p>
<h3 id="配置别名">配置别名</h3>
<p>有没有经常敲错命令？比如git status？status这个单词真心不好记。</p>
<p>如果敲git st就表示git status那就简单多了，当然这种偷懒的办法我们是极力赞成的。</p>
<p>我们只需要敲一行命令，告诉Git，以后st就表示status：</p>
<pre><code>$ git config --global alias.st status
</code></pre>
<p>好了，现在敲git st看看效果。</p>
<p>当然还有别的命令可以简写，很多人都用co表示checkout，ci表示commit，br表示branch：</p>
<pre><code>$ git config --global alias.co checkout
$ git config --global alias.ci commit
$ git config --global alias.br branch
</code></pre>
<p>以后提交就可以简写成：</p>
<pre><code>$ git ci -m &quot;bala bala bala...&quot;
</code></pre>
<p><strong>--global参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用。</strong></p>
<p>在撤销修改一节中，我们知道，命令<code>git reset HEAD file</code>可以把暂存区的修改撤销掉（unstage），重新放回工作区。既然是一个unstage操作，就可以配置一个unstage别名：</p>
<pre><code>$ git config --global alias.unstage 'reset HEAD'
</code></pre>
<p>当你敲入命令：</p>
<pre><code>$ git unstage test.py
</code></pre>
<p>实际上Git执行的是：</p>
<pre><code>$ git reset HEAD test.py
</code></pre>
<p>配置一个git last，让其显示最后一次提交信息：</p>
<pre><code>$ git config --global alias.last 'log -1'
</code></pre>
<p>这样，用git last就能显示最近一次的提交：</p>
<pre><code>$ git last
commit adca45d317e6d8a4b23f9811c3d7b7f0f180bfe2
Merge: bd6ae48 291bea8
Author: Michael Liao &lt;askxuefeng@gmail.com&gt;
Date:   Thu Aug 22 22:49:22 2013 +0800

    merge &amp; fix hello.py
</code></pre>
<p>加入颜色和参数等</p>
<pre><code>git config --global alias.lg &quot;log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit&quot;
</code></pre>
<h3 id="配置文件">配置文件</h3>
<p>配置Git的时候，<strong>加上--global是针对当前用户起作用</strong>的，如果<strong>不加，那只针对当前的仓库起作用</strong>。</p>
<p><strong>每个仓库的Git配置文件都放在<code>.git/config</code>文件中：</strong></p>
<pre><code>$ cat .git/config 
[core]
    repositoryformatversion = 0
    filemode = true
    bare = false
    logallrefupdates = true
    ignorecase = true
    precomposeunicode = true
[remote &quot;origin&quot;]
    url = git@github.com:michaelliao/learngit.git
    fetch = +refs/heads/*:refs/remotes/origin/*
[branch &quot;master&quot;]
    remote = origin
    merge = refs/heads/master
[alias]
    last = log -1
</code></pre>
<p>别名就在<code>[alias]</code>后面，要删除别名，直接把对应的行删掉即可。</p>
<p>而当前用户的Git配置文件放在<strong>用户主目录下的一个隐藏文件<code>.gitconfig</code>中</strong>：</p>
<pre><code>$ cat .gitconfig
[alias]
    co = checkout
    ci = commit
    br = branch
    st = status
[user]
    name = Your Name
    email = your@email.com
</code></pre>
<p>配置别名也可以直接修改这个文件，如果改错了，可以删掉文件重新通过命令配置。</p>
<h2 id="搭建git服务器">搭建Git服务器</h2>
<p>在远程仓库一节中，我们讲了远程仓库实际上和本地仓库没啥不同，纯粹为了7x24小时开机并交换大家的修改。</p>
<p>GitHub就是一个免费托管开源代码的远程仓库。但是对于某些视源代码如生命的商业公司来说，既不想公开源代码，又舍不得给GitHub交保护费，那就只能自己搭建一台Git服务器作为私有仓库使用。</p>
<p>搭建Git服务器需要准备一台运行Linux的机器，强烈推荐用Ubuntu或Debian，这样，通过几条简单的apt命令就可以完成安装。</p>
<p>假设你已经有sudo权限的用户账号，下面，正式开始安装。</p>
<p>第一步，安装git：</p>
<pre><code>$ sudo apt-get install git
</code></pre>
<p>第二步，创建一个git用户，用来运行git服务：</p>
<pre><code>$ sudo adduser git
</code></pre>
<p>第三步，创建证书登录：</p>
<p>收集所有需要登录的用户的公钥，就是他们自己的<code>id_rsa.pub</code>文件，把所有公钥导入到<code>/home/git/.ssh/authorized_keys</code>文件里，一行一个。</p>
<p>第四步，初始化Git仓库：</p>
<p>先选定一个目录作为Git仓库，假定是<code>/srv/sample.git</code>，在<code>/srv</code>目录下输入命令：</p>
<pre><code>$ sudo git init --bare sample.git
</code></pre>
<p>Git就会创建一个裸仓库，裸仓库没有工作区，因为服务器上的Git仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的Git仓库通常都以.git结尾。然后，把owner改为git：</p>
<pre><code>$ sudo chown -R git:git sample.git
</code></pre>
<p>第五步，禁用shell登录：</p>
<p>出于安全考虑，第二步创建的git用户不允许登录shell，这可以通过编辑/etc/passwd文件完成。找到类似下面的一行：</p>
<pre><code>git:x:1001:1001:,,,:/home/git:/bin/bash
</code></pre>
<p>改为：</p>
<pre><code>git:x:1001:1001:,,,:/home/git:/usr/bin/git-shell
</code></pre>
<p>这样，git用户可以正常通过ssh使用git，但无法登录shell，因为我们为git用户指定的git-shell每次一登录就自动退出。</p>
<p>第六步，克隆远程仓库：</p>
<p>现在，可以通过git clone命令克隆远程仓库了，在各自的电脑上运行：</p>
<pre><code>$ git clone git@server:/srv/sample.git
Cloning into 'sample'...
warning: You appear to have cloned an empty repository.
</code></pre>
<p>剩下的推送就简单了。</p>
<h2 id="管理公钥">管理公钥</h2>
<p>如果团队很小，把每个人的公钥收集起来放到服务器的<code>/home/git/.ssh/authorized_keys</code>文件里就是可行的。</p>
<h3 id="管理权限">管理权限</h3>
<p>版本控制系统里可以设置一套完善的权限控制，每个人是否有读写权限会精确到每个分支甚至每个目录下。因为Git是为Linux源代码托管而开发的，所以Git也继承了开源社区的精神，不支持权限控制。不过，因为Git支持钩子（hook），所以，可以在服务器端编写一系列脚本来控制提交等操作，达到权限控制的目的。Gitolite就是这个工具。</p>
<h3 id="小结-2">小结</h3>
<p>搭建Git服务器非常简单，通常10分钟即可完成；</p>
<p>要方便管理公钥，用Gitosis；</p>
<p>要像SVN那样控制权限，用Gitolite。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Git - 远程代码仓库、代码托管网站]]></title>
        <id>https://lixin-scut.github.io//post/git-yuan-cheng-dai-ma-cang-ku-dai-ma-tuo-guan-wang-zhan</id>
        <link href="https://lixin-scut.github.io//post/git-yuan-cheng-dai-ma-cang-ku-dai-ma-tuo-guan-wang-zhan">
        </link>
        <updated>2020-05-12T02:56:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="github">GitHub</h2>
<p>我们一直用GitHub作为免费的远程仓库，如果是个人的开源项目，放到GitHub上是完全没有问题的。其实GitHub还是一个开源协作社区，通过GitHub，既可以让别人参与你的开源项目，也可以参与别人的开源项目。</p>
<p>在GitHub出现以前，开源项目开源容易，但让广大人民群众参与进来比较困难，因为要参与，就要提交代码，而给每个想提交代码的群众都开一个账号那是不现实的，因此，群众也仅限于报个bug，即使能改掉bug，也只能把diff文件用邮件发过去，很不方便。</p>
<p>但是在GitHub上，<strong>利用Git极其强大的克隆和分支功能</strong>，广大人民群众真正可以第一次自由参与各种开源项目了。</p>
<p>如何参与一个开源项目呢？比如人气极高的bootstrap项目，这是一个非常强大的CSS框架，你可以访问它的<a href="https://github.com/twbs/bootstrap">项目主页</a>，点“Fork”就在自己的账号下克隆了一个bootstrap仓库，然后，从自己的账号下clone：</p>
<pre><code>git clone git@github.com:michaelliao/bootstrap.git
</code></pre>
<p>一定要从自己的账号下clone仓库，这样你才能推送修改。如果从bootstrap的作者的仓库地址git@github.com:twbs/bootstrap.git克隆，因为没有权限，你将不能推送修改。</p>
<p>Bootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样：</p>
<pre><code>
┌─ GitHub ────────────────────────────────────┐
│                                             │
│ ┌─────────────────┐     ┌─────────────────┐ │
│ │ twbs/bootstrap  │────&gt;│  my/bootstrap   │ │
│ └─────────────────┘     └─────────────────┘ │
│                                  ▲          │
└──────────────────────────────────┼──────────┘
                                   ▼
                          ┌─────────────────┐
                          │ local/bootstrap │
                          └─────────────────┘
</code></pre>
<p>如果你想修复bootstrap的一个bug，或者新增一个功能，立刻就可以开始干活，干完后，往自己的仓库推送。</p>
<p><strong>如果你希望bootstrap的官方库能接受你的修改，你就可以在GitHub上发起一个pull request</strong>。当然，对方是否接受你的pull request就不一定了。</p>
<h3 id="小结">小结</h3>
<p>在GitHub上，可以任意Fork开源仓库；</p>
<p>自己拥有Fork后的仓库的读写权限；</p>
<p>可以推送pull request给官方仓库来贡献代码。</p>
<h2 id="gitee">Gitee</h2>
<p>使用GitHub时，国内的用户经常遇到的问题是访问速度太慢，有时候还会出现无法连接的情况。</p>
<p>如果我们希望体验Git飞一般的速度，可以使用国内的Git托管服务——Gitee（gitee.com）。</p>
<p>和GitHub相比，Gitee也提供免费的Git仓库。此外，还集成了代码质量检测、项目演示等功能。对于团队协作开发，Gitee还提供了项目管理、代码托管、文档管理的服务，5人以下小团队免费。</p>
<p>Gitee的免费版本也提供私有库功能，只是有5人的成员上限。<br>
使用Gitee和使用GitHub类似，我们在Gitee上注册账号并登录后，需要先上传自己的SSH公钥。选择右上角用户头像 -&gt; 菜单“修改资料”，然后选择“SSH公钥”，填写一个便于识别的标题，然后把用户主目录下的.ssh/id_rsa.pub文件的内容粘贴进去：</p>
<pre><code>gitee-add-ssh-key
</code></pre>
<p>点击“确定”即可完成并看到刚才添加的Key：</p>
<pre><code>gitee-key
</code></pre>
<p>如果我们已经有了一个本地的git仓库（例如，一个名为learngit的本地库），如何把它关联到Gitee的远程库上呢？</p>
<p>首先，我们在Gitee上创建一个新的项目，选择右上角用户头像 -&gt; 菜单“控制面板”，然后点击“创建项目”：</p>
<pre><code>gitee-new-repo
</code></pre>
<p>项目名称最好与本地库保持一致：</p>
<p>然后，我们在本地库上使用命令git remote add把它和Gitee的远程库关联：</p>
<pre><code>git remote add origin git@gitee.com:&lt;user-name&gt;/learngit.git
</code></pre>
<p>之后，就可以正常地用git push和git pull推送了！</p>
<p>如果在使用命令git remote add时报错：</p>
<pre><code>git remote add origin git@gitee.com:&lt;user-name&gt;/learngit.git
fatal: remote origin already exists.
</code></pre>
<p>这说明本地库已经关联了一个名叫origin的远程库，此时，可以先用git remote -v查看远程库信息：</p>
<pre><code>git remote -v
origin	git@github.com:&lt;user-name&gt;/learngit.git (fetch)
origin	git@github.com:&lt;user-name&gt;/learngit.git (push)
</code></pre>
<p>可以看到，本地库已经关联了origin的远程库，并且，该远程库指向GitHub。</p>
<p>我们可以删除已有的GitHub远程库：</p>
<pre><code>git remote rm origin
</code></pre>
<p>再关联Gitee的远程库（注意路径中需要填写正确的用户名）：</p>
<pre><code>git remote add origin git@gitee.com:&lt;user-name&gt;/learngit.git
</code></pre>
<p>此时，我们再查看远程库信息：</p>
<pre><code>git remote -v
origin	git@gitee.com:&lt;user-name&gt;/learngit.git (fetch)
origin	git@gitee.com:&lt;user-name&gt;/learngit.git (push)
</code></pre>
<p>现在可以看到，origin已经被关联到Gitee的远程库了。通过git push命令就可以把本地库推送到Gitee上。</p>
<p>有的小伙伴又要问了，一个本地库能不能既关联GitHub，又关联Gitee呢？</p>
<p>答案是肯定的，因为git本身是分布式版本控制系统，可以同步到另外一个远程库，当然也可以同步到另外两个远程库。</p>
<p>使用多个远程库时，我们要注意，git给远程库起的默认名称是origin，如果有多个远程库，我们需要用不同的名称来标识不同的远程库。</p>
<p>仍然以learngit本地库为例，我们先删除已关联的名为origin的远程库：</p>
<pre><code>git remote rm origin
</code></pre>
<p>然后，先关联GitHub的远程库：</p>
<pre><code>git remote add github git@github.com:&lt;user-name&gt;/learngit.git
</code></pre>
<p>注意，远程库的名称叫github，不叫origin了。</p>
<p>接着，再关联Gitee的远程库：</p>
<pre><code>git remote add gitee git@gitee.com:&lt;user-name&gt;/learngit.git
</code></pre>
<p>同样注意，远程库的名称叫gitee，不叫origin。</p>
<p>现在，我们用git remote -v查看远程库信息，可以看到两个远程库：</p>
<pre><code>git remote -v
gitee	git@gitee.com:&lt;user-name&gt;/learngit.git (fetch)
gitee	git@gitee.com:&lt;user-name&gt;/learngit.git (push)
github	git@github.com:&lt;user-name&gt;/learngit.git (fetch)
github	git@github.com:&lt;user-name&gt;/learngit.git (push)
</code></pre>
<p>如果要推送到GitHub，使用命令：</p>
<pre><code>git push github master
</code></pre>
<p>如果要推送到Gitee，使用命令：</p>
<pre><code>git push gitee master
</code></pre>
<p>这样一来，我们的本地库就可以同时与多个远程库互相同步：</p>
<pre><code>
┌─────────┐ ┌─────────┐
│ GitHub  │ │  Gitee  │
└─────────┘ └─────────┘
     ▲           ▲
     └─────┬─────┘
           │
    ┌─────────────┐
    │ Local Repo  │
    └─────────────┘
</code></pre>
<p>Gitee也同样提供了Pull request功能，可以让其他小伙伴参与到开源项目中来。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Git - 标签管理]]></title>
        <id>https://lixin-scut.github.io//post/git-biao-qian-guan-li</id>
        <link href="https://lixin-scut.github.io//post/git-biao-qian-guan-li">
        </link>
        <updated>2020-05-12T02:28:53.000Z</updated>
        <content type="html"><![CDATA[<p>发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，<strong>标签也是版本库的一个快照</strong>。</p>
<p>Git的标签虽然是版本库的快照，但<strong>其实它就是指向某个commit的指针</strong>（跟分支很像对不对？但是<strong>分支可以移动，标签不能移动</strong>），所以，创建和删除标签都是瞬间完成的。</p>
<p>Git有commit，为什么还要引入tag？</p>
<p>Git有commit，为什么还要引入tag？</p>
<p>“请把上周一的那个版本打包发布，commit号是6a5819e...”</p>
<p>“一串乱七八糟的数字不好找！”</p>
<p>如果换一个办法：</p>
<p>“请把上周一的那个版本打包发布，版本号是v1.2”</p>
<p>“好的，按照tag v1.2查找commit就行！”</p>
<p>所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。</p>
<h3 id="创建标签">创建标签</h3>
<p>在Git中打标签非常简单，首先，切换到需要打标签的分支上：</p>
<pre><code>$ git branch
* dev
  master
$ git checkout master
Switched to branch 'master'
</code></pre>
<p>然后，敲命令<code>git tag &lt;name&gt;</code>就可以打一个新标签：</p>
<pre><code>$ git tag v1.0
</code></pre>
<p>可以用命令git tag查看所有标签：</p>
<pre><code>$ git tag
v1.0
</code></pre>
<p>默认标签是打在最新提交的commit上的。</p>
<p>有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？</p>
<p>方法是找到历史提交的commit id，然后打上就可以了：</p>
<pre><code>$ git log --pretty=oneline --abbrev-commit
12a631b (HEAD -&gt; master, tag: v1.0, origin/master) merged bug fix 101
4c805e2 fix bug 101
e1e9c68 merge with no-ff
f52c633 add merge
cf810e4 conflict fixed
5dc6824 &amp; simple
14096d0 AND simple
b17d20e branch test
d46f35e remove test.txt
b84166e add test.txt
519219b git tracks changes
e43a48b understand how stage works
1094adb append GPL
e475afc add distributed
eaadf4e wrote a readme file
</code></pre>
<p>比方说要对add merge这次提交打标签，它对应的commit id是f52c633，敲入命令：</p>
<pre><code>$ git tag v0.9 f52c633
</code></pre>
<p>再用命令git tag查看标签：</p>
<pre><code>$ git tag
v0.9
v1.0
</code></pre>
<p>注意，<strong>标签不是按时间顺序列出，而是按字母排序的</strong>。</p>
<p>可以用<code>git show &lt;tagname&gt;</code>查看标签信息：</p>
<pre><code>$ git show v0.9
commit f52c63349bc3c1593499807e5c8e972b82c8f286 (tag: v0.9)
Author: Michael Liao &lt;askxuefeng@gmail.com&gt;
Date:   Fri May 18 21:56:54 2018 +0800

    add merge

diff --git a/readme.txt b/readme.txt
...
</code></pre>
<p>可以看到，v0.9确实打在add merge这次提交上。</p>
<p>还可以创建<strong>带有说明</strong>的标签，<strong>用-a指定标签名，-m指定说明文字</strong>：</p>
<pre><code>$ git tag -a v0.1 -m &quot;version 0.1 released&quot; 1094adb
用命令git show &lt;tagname&gt;可以看到说明文字：

$ git show v0.1
tag v0.1
Tagger: Michael Liao &lt;askxuefeng@gmail.com&gt;
Date:   Fri May 18 22:48:43 2018 +0800

version 0.1 released

commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (tag: v0.1)
Author: Michael Liao &lt;askxuefeng@gmail.com&gt;
Date:   Fri May 18 21:06:15 2018 +0800

    append GPL

diff --git a/readme.txt b/readme.txt
...
</code></pre>
<p>注意：<strong>标签总是和某个commit挂钩</strong>。如果这个commit既出现在master分支，又出现在dev分支，那么在这<strong>两个分支上都可以看到这个标签</strong>。</p>
<h3 id="操作标签">操作标签</h3>
<p>如果标签打错了，也可以删除：</p>
<pre><code>$ git tag -d v0.1
Deleted tag 'v0.1' (was f15b0dd)
</code></pre>
<p>因为<strong>创建的标签都只存储在本地，不会自动推送到远程</strong>。所以，打错的标签可以在本地安全删除。</p>
<p>如果要推送某个标签到远程，使用命令<code>git push origin &lt;tagname&gt;</code>：</p>
<pre><code>$ git push origin v1.0
Total 0 (delta 0), reused 0 (delta 0)
To github.com:michaelliao/learngit.git
 * [new tag]         v1.0 -&gt; v1.0
</code></pre>
<p>或者，一次性推送全部尚未推送到远程的本地标签：</p>
<pre><code>$ git push origin --tags
Total 0 (delta 0), reused 0 (delta 0)
To github.com:michaelliao/learngit.git
 * [new tag]         v0.9 -&gt; v0.9
</code></pre>
<p>如果标签已经推送到远程，要删除远程标签就麻烦一点，<strong>先从本地删除</strong>：</p>
<pre><code>$ git tag -d v0.9
Deleted tag 'v0.9' (was f52c633)
</code></pre>
<p><strong>然后，从远程删除。删除命令也是push</strong>，但是<strong>格式如下</strong>：</p>
<pre><code>git push origin :refs/tags/&lt;tagname&gt;
</code></pre>
<pre><code>$ git push origin :refs/tags/v0.9
To github.com:michaelliao/learngit.git
 - [deleted]         v0.9
</code></pre>
<p>要看看是否真的从远程库删除了标签，可以登陆GitHub查看。</p>
<h3 id="小结">小结</h3>
<p>命令<code>git push origin &lt;tagname&gt;</code>可以推送一个本地标签；</p>
<p>命令<code>git push origin --tags</code>可以推送全部未推送过的本地标签；</p>
<p>命令<code>git tag -d &lt;tagname&gt;</code>可以删除一个本地标签；</p>
<p>命令<code>git push origin :refs/tags/&lt;tagname&gt;</code>可以删除一个远程标签。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Git - 分支管理]]></title>
        <id>https://lixin-scut.github.io//post/git-fen-zhi-guan-li</id>
        <link href="https://lixin-scut.github.io//post/git-fen-zhi-guan-li">
        </link>
        <updated>2020-05-11T13:27:55.000Z</updated>
        <content type="html"><![CDATA[<p>分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了50%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。</p>
<p>现在有了分支，就不用怕了。你<strong>创建了一个属于你自己的分支</strong>，别人看不到，还<strong>继续在原来的分支上正常工作</strong>，而你在自己的分支上干活，想提交就提交，直到<strong>开发完毕后，再一次性合并到原来的分支上</strong>，这样，既安全，又不影响别人工作。</p>
<p>其他版本控制系统如SVN等都有分支管理，但是用过之后你会发现，这些版本控制系统创建和切换分支慢，结果分支功能成了摆设，大家都不去用。</p>
<p>但Git的分支是与众不同的，无论创建、切换和删除分支，Git在1秒钟之内就能完成！无论你的版本库是1个文件还是1万个文件。</p>
<h2 id="创建与合并分支">创建与合并分支</h2>
<h3 id="分支概念">分支概念</h3>
<p>在版本回退里，每次提交Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫<strong>主分支，即master分支</strong>。<strong>HEAD严格来说不是指向提交，而是指向master</strong>，master才是指向提交的，所以，HEAD指向的就是当前分支。</p>
<p>一开始的时候，<strong>master分支是一条线</strong>，Git<strong>用master指向最新的提交</strong>，<strong>再用HEAD指向master</strong>，就能确定当前分支，以及当前分支的提交点：<br>
<img src="https://lixin-scut.github.io//post-images/1589204783529.png" alt=""></p>
<p><strong>每次提交，master分支都会向前移动一步</strong>，这样，随着你不断提交，master分支的线也越来越长。</p>
<p>当我们<strong>创建新的分支，例如dev时</strong>，Git<strong>新建了一个指针叫dev</strong>，指向master相同的提交，<strong>再把HEAD指向dev</strong>，就表示当前分支在dev上：<br>
<img src="https://lixin-scut.github.io//post-images/1589204824262.png" alt=""></p>
<p>你看，Git创建一个分支很快，因为<strong>除了增加一个dev指针，改改HEAD的指向，工作区的文件都没有任何变化</strong>！</p>
<p>不过，<strong>从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变</strong>：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589204958791.png" alt=""></p>
<p>假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并：<br>
<img src="https://lixin-scut.github.io//post-images/1589204995130.png" alt=""></p>
<p><strong>所以Git合并分支也很快！就改改指针，工作区内容也不变！</strong></p>
<p>合并完分支后，甚至可以删除dev分支。<strong>删除dev分支就是把dev指针给删掉</strong>，删掉后，我们就剩下了一条master分支：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589205086707.png" alt=""></p>
<h3 id="创建分支">创建分支</h3>
<p>首先，我们创建dev分支，然后切换到dev分支：</p>
<pre><code>$ git checkout -b dev

Switched to a new branch 'dev'
</code></pre>
<p><strong>git checkout命令加上-b参数表示创建并切换</strong>，相当于以下两条命令：</p>
<pre><code>$ git branch dev
$ git checkout dev
Switched to branch 'dev'
</code></pre>
<p>然后，用git branch命令查看当前分支：</p>
<pre><code>$ git branch
* dev
  master
</code></pre>
<p>git branch命令会列出所有分支，当前分支前面会标一个*号。</p>
<p>然后，我们就可以在dev分支上正常提交，比如对readme.txt做个修改，加上一行：</p>
<pre><code>Creating a new branch is quick.
</code></pre>
<p>然后提交：</p>
<pre><code>$ git add readme.txt 
$ git commit -m &quot;branch test&quot;
[dev b17d20e] branch test
 1 file changed, 1 insertion(+)
</code></pre>
<p>现在，dev分支的工作完成，我们就可以切换回master分支：</p>
<pre><code>$ git checkout master
Switched to branch 'master'
</code></pre>
<p><strong>切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了</strong>！因为那个提交是在dev分支上，而<strong>master分支此刻的提交点并没有变</strong>：<br>
<img src="https://lixin-scut.github.io//post-images/1589205993805.png" alt=""></p>
<h3 id="合并分支">合并分支</h3>
<p>现在，我们把dev分支的工作成果合并到master分支上：</p>
<pre><code>$ git merge dev
Updating d46f35e..b17d20e
Fast-forward
 readme.txt | 1 +
 1 file changed, 1 insertion(+)
</code></pre>
<p><strong>git merge命令用于合并指定分支到当前分支</strong>。合并后，再查看readme.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。</p>
<p>注意到上面的<strong>Fast-forward信息</strong>，Git告诉我们，这次<strong>合并是“快进模式”</strong>，也就是<strong>直接把master指向dev的当前提交，所以合并速度非常快</strong>。</p>
<p>当然，<strong>也不是每次合并都能Fast-forward</strong>，我们后面会讲其他方式的合并。</p>
<h3 id="删除分支">删除分支</h3>
<p>合并完成后，就可以放心地<strong>删除dev分支</strong>了：</p>
<pre><code>$ git branch -d dev
Deleted branch dev (was b17d20e).
</code></pre>
<p>删除后，查看branch，就只剩下master分支了：</p>
<pre><code>$ git branch
* master
</code></pre>
<p>因为<strong>创建、合并和删除分支非常快</strong>，所以Git<strong>鼓励你使用分支完成某个任务，合并后再删掉分支</strong>，这和直接在master分支上工作效果是一样的，但<strong>过程更安全</strong>。</p>
<h3 id="switch">switch</h3>
<p>我们注意到切换分支使用<code>git checkout &lt;branch&gt;</code>，而前面讲过的撤销修改则是<code>git checkout -- &lt;file&gt;</code>，同一个命令，有两种作用，确实有点令人迷惑。</p>
<p>实际上，<strong>切换分支这个动作，用switch更科学</strong>。因此，最新版本的Git提供了新的git switch命令来切换分支：</p>
<p>创建并切换到新的dev分支，可以使用：</p>
<pre><code>$ git switch -c dev
</code></pre>
<p>直接切换到已有的master分支，可以使用：</p>
<pre><code>$ git switch master
</code></pre>
<p>使用新的git switch命令，比git checkout要更容易理解。</p>
<h3 id="小结">小结</h3>
<p>Git鼓励大量使用分支：</p>
<p>查看分支：<code>git branch</code></p>
<p>创建分支：<code>git branch &lt;name&gt;</code></p>
<p>切换分支：<code>git checkout &lt;name&gt;</code>或者<code>git switch &lt;name&gt;</code></p>
<p>创建+切换分支：<code>git checkout -b &lt;name&gt;</code>或者<code>git switch -c &lt;name&gt;</code></p>
<p>合并某分支到当前分支：<code>git merge &lt;name&gt;</code></p>
<p>删除分支：<code>git branch -d &lt;name&gt;</code></p>
<h2 id="解决冲突">解决冲突</h2>
<p>准备新的feature1分支，继续我们的新分支开发：</p>
<pre><code>$ git switch -c feature1
Switched to a new branch 'feature1'
</code></pre>
<p>修改readme.txt最后一行，改为：</p>
<pre><code>Creating a new branch is quick AND simple.
</code></pre>
<p>在feature1分支上提交：</p>
<pre><code>$ git add readme.txt

$ git commit -m &quot;AND simple&quot;
[feature1 14096d0] AND simple
 1 file changed, 1 insertion(+), 1 deletion(-)
</code></pre>
<p>切换到master分支：</p>
<pre><code>$ git switch master
Switched to branch 'master'
Your branch is ahead of 'origin/master' by 1 commit.
  (use &quot;git push&quot; to publish your local commits)
</code></pre>
<p>Git还会自动提示我们当前master分支比远程的master分支要超前1个提交。</p>
<p>在master分支上把readme.txt文件的最后一行改为：</p>
<pre><code>Creating a new branch is quick &amp; simple.
</code></pre>
<p>提交：</p>
<pre><code>$ git add readme.txt 
$ git commit -m &quot;&amp; simple&quot;
[master 5dc6824] &amp; simple
 1 file changed, 1 insertion(+), 1 deletion(-)
</code></pre>
<p>现在，<strong>master分支和feature1分支各自都分别有新的提交</strong>，变成了这样：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589242371935.png" alt=""></p>
<p>这种情况下，<strong>Git无法执行“快速合并”，只能试图把各自的修改合并起来</strong>，但这种合并就可能会有冲突，我们试试看：</p>
<pre><code>$ git merge feature1
Auto-merging readme.txt
CONFLICT (content): Merge conflict in readme.txt
Automatic merge failed; fix conflicts and then commit the result.
</code></pre>
<p>Git告诉我们，readme.txt文件存在冲突，<strong>必须手动解决冲突后再提交</strong>。</p>
<p><strong>git status</strong>也可以告诉我们冲突的文件：</p>
<pre><code>$ git status
On branch master
Your branch is ahead of 'origin/master' by 2 commits.
  (use &quot;git push&quot; to publish your local commits)

You have unmerged paths.
  (fix conflicts and run &quot;git commit&quot;)
  (use &quot;git merge --abort&quot; to abort the merge)

Unmerged paths:
  (use &quot;git add &lt;file&gt;...&quot; to mark resolution)

	both modified:   readme.txt

no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
</code></pre>
<p><strong>git diff</strong>也可以显示不同</p>
<p>vim直接查看readme.txt的内容：</p>
<pre><code>Git is a distributed version control system.
Git is free software distributed under the GPL.
Git has a mutable index called stage.
Git tracks changes of files.
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
Creating a new branch is quick &amp; simple.
=======
Creating a new branch is quick AND simple.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1
</code></pre>
<p>Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改如下后保存：</p>
<pre><code>Creating a new branch is quick and simple.
</code></pre>
<p>再提交：</p>
<pre><code>$ git add readme.txt 
$ git commit -m &quot;conflict fixed&quot;
[master cf810e4] conflict fixed
</code></pre>
<p>现在，master分支和feature1分支变成了下图所示：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589242393606.png" alt=""></p>
<pre><code>用带参数的git log也可以看到分支的合并情况：
</code></pre>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
*   cf810e4 (HEAD -&gt; master) conflict fixed
|\  
| * 14096d0 (feature1) AND simple
* | 5dc6824 &amp; simple
|/  
* b17d20e branch test
* d46f35e (origin/master) remove test.txt
* b84166e add test.txt
* 519219b git tracks changes
* e43a48b understand how stage works
* 1094adb append GPL
* e475afc add distributed
* eaadf4e wrote a readme file
</code></pre>
<p>最后，删除feature1分支：</p>
<p>$ git branch -d feature1<br>
Deleted branch feature1 (was 14096d0).</p>
<h3 id="小结-2">小结</h3>
<p>当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。</p>
<p>解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。</p>
<p>用<strong>git log --graph</strong>命令可以看到分支合并图。</p>
<h2 id="分支管理策略">分支管理策略</h2>
<p>通常，合并分支时，如果可能，Git会用<strong>Fast forward模式</strong>，但这种模式下，<strong>删除分支后，会丢掉分支信息</strong>。</p>
<p>如果要<strong>强制禁用Fast forward模式</strong>，Git就会<strong>在merge时生成一个新的commit</strong>，这样，<strong>从分支历史上就可以看出分支信息</strong>。</p>
<p>下面实战一下禁用Fast forward模式 --no-ff方式的git merge：</p>
<p>首先，仍然创建并切换dev分支：</p>
<pre><code>$ git switch -c dev
Switched to a new branch 'dev'
</code></pre>
<p>修改readme.txt文件，并提交一个新的commit：</p>
<pre><code>$ git add readme.txt 
$ git commit -m &quot;add merge&quot;
[dev f52c633] add merge
 1 file changed, 1 insertion(+)
</code></pre>
<p>现在，我们切换回master：</p>
<pre><code>$ git switch master
Switched to branch 'master'
</code></pre>
<p>准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward：</p>
<pre><code>$ git merge --no-ff -m &quot;merge with no-ff&quot; dev
Merge made by the 'recursive' strategy.
 readme.txt | 1 +
 1 file changed, 1 insertion(+)
</code></pre>
<p>因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。</p>
<p>合并后，我们用git log看看分支历史：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
*   e1e9c68 (HEAD -&gt; master) merge with no-ff
|\  
| * f52c633 (dev) add merge
|/  
*   cf810e4 conflict fixed
...
</code></pre>
<p>可以看到，不使用Fast forward模式，merge后就像这样：</p>
<p><img src="https://lixin-scut.github.io//post-images/1589243365392.png" alt=""></p>
<h3 id="分支策略">分支策略</h3>
<p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p>
<p>首先，<strong>master分支应该是非常稳定</strong>的，也就是仅用来发布新版本，<strong>平时不能在上面干活</strong>；</p>
<p>那在哪干活呢？<strong>干活都在dev分支上</strong>，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，<strong>再把dev分支合并到master上</strong>，在master分支发布1.0版本；</p>
<p>你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，<strong>时不时地往dev分支上合并就可以了</strong>。</p>
<p>所以，团队合作的分支看起来就像这样：<br>
<img src="https://lixin-scut.github.io//post-images/1589243389105.png" alt=""></p>
<h3 id="小结-3">小结</h3>
<p>Git分支十分强大，在团队开发中应该充分应用。</p>
<p>合并分支时，<strong>加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并</strong>。</p>
<h2 id="bug分支">Bug分支</h2>
<p>软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，<strong>每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除</strong>。</p>
<p>当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是当前正在dev上进行的工作还没有提交：</p>
<pre><code>$ git status
On branch dev
Changes to be committed:
  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)

	new file:   hello.py

Changes not staged for commit:
  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)

	modified:   readme.txt
</code></pre>
<p>并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？</p>
<h3 id="stash暂存工作现场">stash暂存工作现场</h3>
<p>幸好，Git还提供了一个<strong>stash功能</strong>，可以把当前<strong>工作现场</strong>“储藏”起来，等以后<strong>恢复现场后继续工作</strong>：</p>
<pre><code>$ git stash
Saved working directory and index state WIP on dev: f52c633 add merge
</code></pre>
<p>现在，<strong>用git status查看工作区，就是干净的</strong>（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。</p>
<p>首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支：</p>
<pre><code>$ git checkout master
Switched to branch 'master'
Your branch is ahead of 'origin/master' by 6 commits.
  (use &quot;git push&quot; to publish your local commits)

$ git checkout -b issue-101
Switched to a new branch 'issue-101'
</code></pre>
<p>现在修复bug，需要把“Git is free software ...”改为“Git is a free software ...”，然后提交：</p>
<pre><code>$ git add readme.txt 
$ git commit -m &quot;fix bug 101&quot;
[issue-101 4c805e2] fix bug 101
 1 file changed, 1 insertion(+), 1 deletion(-)
</code></pre>
<p>修复完成后，切换到master分支，并完成合并，最后删除issue-101分支：</p>
<pre><code>$ git switch master
Switched to branch 'master'
Your branch is ahead of 'origin/master' by 6 commits.
  (use &quot;git push&quot; to publish your local commits)

$ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101
Merge made by the 'recursive' strategy.
 readme.txt | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
</code></pre>
<p>太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到dev分支干活了！</p>
<pre><code>$ git switch dev
Switched to branch 'dev'

$ git status
On branch dev
nothing to commit, working tree clean
</code></pre>
<h3 id="恢复现场">恢复现场</h3>
<p>工作区是干净的，刚才的工作现场存到哪去了？用<strong>git stash list命令</strong>看看：</p>
<pre><code>$ git stash list
stash@{0}: WIP on dev: f52c633 add merge
</code></pre>
<p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法：</p>
<p>一是用<strong>git stash apply恢复</strong>，但是恢复后，<strong>stash内容并不删除</strong>，你需要用<strong>git stash drop来删除</strong>；</p>
<p>另一种方式是用<strong>git stash pop，恢复的同时把stash内容也删了</strong>：</p>
<pre><code>$ git stash pop
On branch dev
Changes to be committed:
  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)

	new file:   hello.py

Changes not staged for commit:
  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)

	modified:   readme.txt

Dropped refs/stash@{0} (5d677e2ee266f39ea296182fb2354265b91b3b2a)
</code></pre>
<p>再用git stash list查看，就看不到任何stash内容了：</p>
<pre><code>$ git stash list
</code></pre>
<p>你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令：</p>
<pre><code>$ git stash apply stash@{0}
</code></pre>
<p>注意使用的是stash的序号</p>
<h3 id="dev分支修改bug">dev分支修改bug</h3>
<p>在master分支上修复了bug后，我们要想一想，dev分支是早期从master分支分出来的，所以，这个<strong>bug其实在当前dev分支上也存在</strong>。</p>
<p><strong>同样的bug，要在dev上修复，我们只需要把4c805e2 fix bug 101这个提交所做的修改“复制”到dev分支</strong>。<br>
注意：我们只想复制4c805e2 fix bug 101这个提交所做的修改，<strong>并不是把整个master分支merge过来</strong>。</p>
<p>为了方便操作，Git专门提供了一个<strong>cherry-pick命令</strong>，让我们能复制一个特定的提交到当前分支：</p>
<pre><code>$ git branch
* dev
  master
$ git cherry-pick 4c805e2
[master 1d4b803] fix bug 101
 1 file changed, 1 insertion(+), 1 deletion(-)
</code></pre>
<p>Git自动给dev分支做了一次提交，注意这次提交的commit是1d4b803，它并不同于master的4c805e2，因为<strong>这两个commit只是改动相同，但确实是两个不同的commit</strong>。</p>
<p><strong>用git cherry-pick，我们就不需要在dev分支上手动再把修bug的过程重复一遍。</strong></p>
<p>既然可以在master分支上修复bug后，在dev分支上可以“重放”这个修复过程，那么<strong>直接在dev分支上修复bug，然后在master分支上“重放”行不行</strong>？当然可以，不过你<strong>仍然需要git stash命令保存现场，才能从dev分支切换到master分支</strong>。</p>
<h3 id="小结-4">小结</h3>
<p>修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；</p>
<p>当手头工作没有完成时，先<strong>把工作现场git stash一下</strong>，然后去修复bug，修复后，<strong>再git stash pop，回到工作现场</strong>；</p>
<p>在master分支上修复的bug，想要合并到当前dev分支，可以用<code>git cherry-pick &lt;commit&gt;</code>命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</p>
<h2 id="强制删除分支">强制删除分支</h2>
<p>软件开发中，添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，<strong>每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。</strong></p>
<p>使用新的分支进行开发：</p>
<pre><code>$ git switch -c feature-vulcan
Switched to a new branch 'feature-vulcan'
</code></pre>
<p>开发完毕后提交：</p>
<pre><code>$ git add vulcan.py

$ git status
On branch feature-vulcan
Changes to be committed:
  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)

	new file:   vulcan.py

$ git commit -m &quot;add feature vulcan&quot;
[feature-vulcan 287773e] add feature vulcan
 1 file changed, 2 insertions(+)
 create mode 100644 vulcan.py
</code></pre>
<p>切回dev，准备合并：</p>
<pre><code>$ git switch dev
</code></pre>
<p>一切顺利的话，feature分支和bug分支是类似的，合并，然后删除。</p>
<p>假设在合并分支前我们希望取消这个功能，就需要删除分支，先尝试-d删除分支：</p>
<pre><code>$ git branch -d feature-vulcan
error: The branch 'feature-vulcan' is not fully merged.
If you are sure you want to delete it, run 'git branch -D feature-vulcan'.
</code></pre>
<p>但此时会销毁失败。Git友情提醒，<strong>feature-vulcan分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用大写的-D参数</strong>。</p>
<p>现在我们强行删除：</p>
<pre><code>$ git branch -D feature-vulcan
Deleted branch feature-vulcan (was 287773e).
</code></pre>
<p>终于删除成功</p>
<h3 id="小结-5">小结</h3>
<p>开发一个新feature，最好新建一个分支；</p>
<p>如果要丢弃一个没有被合并过的分支，可以通过<code>git branch -D &lt;name&gt;</code>强行删除。</p>
<h2 id="多人协作">多人协作</h2>
<p>当你从远程仓库克隆时，实际上Git<strong>自动把本地的master分支和远程的master分支对应起来了</strong>，并且，<strong>远程仓库的默认名称是origin</strong>。</p>
<p>要查看远程库的信息，用git remote：</p>
<pre><code>$ git remote
origin
</code></pre>
<p>或者，用git remote -v显示更详细的信息：</p>
<pre><code>$ git remote -v
origin  git@github.com:user/learngit.git (fetch)
origin  git@github.com:user/learngit.git (push)
</code></pre>
<p>上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。</p>
<h3 id="推送分支">推送分支</h3>
<p>推送分支，就是<strong>把该分支上的所有本地提交推送到远程库</strong>。推送时，<strong>要指定本地分支</strong>，这样，Git就会把该分支推送到远程库对应的远程分支上：</p>
<pre><code>$ git push origin master
</code></pre>
<p>如果要推送其他分支，比如dev，就改成：</p>
<pre><code>$ git push origin dev
</code></pre>
<p>但是，<strong>并不是一定要把本地分支往远程推送</strong>，那么，哪些分支需要推送，哪些不需要呢？</p>
<ol>
<li>
<p><strong>master分支</strong>是主分支，因此要<strong>时刻与远程同步</strong>；</p>
</li>
<li>
<p><strong>dev分支</strong>是开发分支，团队所有成员都需要在上面工作，所以<strong>也需要与远程同步</strong>；</p>
</li>
<li>
<p><strong>bug分支</strong>只用于在本地修复bug，就<strong>没必要推到远程了</strong>，除非老板要看看你每周到底修复了几个bug；</p>
</li>
<li>
<p><strong>feature分支</strong>是新功能分支，是否推到远程，<strong>取决于你是否和同事合作在上面开发</strong>。</p>
</li>
</ol>
<p>总之，就是在Git中，分支完全可以在本地保存，是否推送，视个人输球而定！</p>
<h3 id="抓取分支">抓取分支</h3>
<p>多人协作时，大家都会往master和dev分支上推送各自的修改。</p>
<p>现在，模拟一个你的同事，可以在另一台电脑（注意要把SSH Key添加到GitHub）或者同一台电脑的另一个目录下克隆：</p>
<pre><code>$ git clone git@github.com:michaelliao/learngit.git
Cloning into 'learngit'...
remote: Counting objects: 40, done.
remote: Compressing objects: 100% (21/21), done.
remote: Total 40 (delta 14), reused 40 (delta 14), pack-reused 0
Receiving objects: 100% (40/40), done.
Resolving deltas: 100% (14/14), done.
</code></pre>
<p>当你的小伙伴从远程库clone时，默认情况下，你的小伙伴<strong>只能看到本地的master分支</strong>。</p>
<p>可以用git branch命令看看：</p>
<pre><code>$ git branch
* master
</code></pre>
<p>现在，你的小伙伴<strong>要在dev分支上开发，就必须创建远程origin的dev分支到本地</strong>，于是他用这个命令创建本地dev分支：</p>
<pre><code>$ git checkout -b dev origin/dev
</code></pre>
<p>现在，他就可以在dev上继续修改，然后，时不时地把dev分支push到远程：</p>
<pre><code>$ git add env.txt

$ git commit -m &quot;add env&quot;
[dev 7a5e5dd] add env
 1 file changed, 1 insertion(+)
 create mode 100644 env.txt

$ git push origin dev
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 308 bytes | 308.00 KiB/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To github.com:michaelliao/learngit.git
   f52c633..7a5e5dd  dev -&gt; dev
</code></pre>
<p>你的小伙伴已经向origin/dev分支推送了他的提交，而碰巧你也对同样的文件作了修改，并试图推送：</p>
<pre><code>$ cat env.txt
env

$ git add env.txt

$ git commit -m &quot;add new env&quot;
[dev 7bd91f1] add new env
 1 file changed, 1 insertion(+)
 create mode 100644 env.txt

$ git push origin dev
To github.com:michaelliao/learngit.git
 ! [rejected]        dev -&gt; dev (non-fast-forward)
error: failed to push some refs to 'git@github.com:michaelliao/learngit.git'
hint: Updates were rejected because the tip of your current branch is behind
hint: its remote counterpart. Integrate the remote changes (e.g.
hint: 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
</code></pre>
<p><strong>推送失败</strong>，因为你的小伙伴的<strong>最新提交和你试图推送的提交有冲突</strong>，解决办法也很简单，Git已经提示我们，<strong>先用git pull把最新的提交从origin/dev抓下来，然后，在本地合并，解决冲突，再推送</strong>：</p>
<pre><code>$ git pull
There is no tracking information for the current branch.
Please specify which branch you want to merge with.
See git-pull(1) for details.

    git pull &lt;remote&gt; &lt;branch&gt;

If you wish to set tracking information for this branch you can do so with:

    git branch --set-upstream-to=origin/&lt;branch&gt; dev
</code></pre>
<p>git pull也失败了，原因是<strong>没有指定本地dev分支与远程origin/dev分支的链接</strong>，根据提示，设置dev和origin/dev的链接：</p>
<pre><code>$ git branch --set-upstream-to=origin/dev dev
Branch 'dev' set up to track remote branch 'dev' from 'origin'.
</code></pre>
<p>再pull：</p>
<pre><code>$ git pull
Auto-merging env.txt
CONFLICT (add/add): Merge conflict in env.txt
Automatic merge failed; fix conflicts and then commit the result.
</code></pre>
<p>这回git pull成功，但是<strong>合并有冲突，需要手动解决</strong>，解决的方法和分支管理中的解决冲突完全一样。解决后，提交，再push：</p>
<pre><code>$ git commit -m &quot;fix env conflict&quot;
[dev 57c53ab] fix env conflict

$ git push origin dev
Counting objects: 6, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (4/4), done.
Writing objects: 100% (6/6), 621 bytes | 621.00 KiB/s, done.
Total 6 (delta 0), reused 0 (delta 0)
To github.com:michaelliao/learngit.git
   7a5e5dd..57c53ab  dev -&gt; dev
</code></pre>
<h3 id="多人协作的工作模式">多人协作的工作模式</h3>
<p>多人协作的工作模式通常是这样：</p>
<ol>
<li>
<p>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</p>
</li>
<li>
<p>如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并；</p>
</li>
<li>
<p>如果合并有冲突，则解决冲突，并在本地提交；</p>
</li>
<li>
<p>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</p>
</li>
<li>
<p>如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。</p>
</li>
</ol>
<h3 id="小结-6">小结</h3>
<p>查看远程库信息，使用<code>git remote -v</code>；</p>
<p>本地新建的分支如果不推送到远程，对其他人就是不可见的；</p>
<p>从本地推送分支，使用<code>git push origin branch-name</code>，如果推送失败，先用git pull抓取远程的新提交；</p>
<p>在本地创建和远程分支对应的分支，使用<code>git checkout -b branch-name origin/branch-name</code>，本地和远程分支的名称最好一致；</p>
<p>建立本地分支和远程分支的关联，使用<code>git branch --set-upstream branch-name origin/branch-name</code>；</p>
<p>从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。</p>
<h2 id="rebase">Rebase</h2>
<p>在上一节中，多人在同一个分支上协作时，很容易出现冲突。即使没有冲突，<strong>后push的童鞋不得不先pull，在本地合并，然后才能push成功。</strong></p>
<p>每次合并再push后，分支变成了这样：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
* d1be385 (HEAD -&gt; master, origin/master) init hello
*   e5e69f1 Merge branch 'dev'
|\  
| *   57c53ab (origin/dev, dev) fix env conflict
| |\  
| | * 7a5e5dd add env
| * | 7bd91f1 add new env
| |/  
* |   12a631b merged bug fix 101
|\ \  
| * | 4c805e2 fix bug 101
|/ /  
* |   e1e9c68 merge with no-ff
|\ \  
| |/  
| * f52c633 add merge
|/  
*   cf810e4 conflict fixed
</code></pre>
<p>总之看上去很乱，Git的提交历史能否是一条干净的直线？其实是可以做到的</p>
<p>Git有一种称为rebase的操作</p>
<p>在和远程分支同步后，我们对hello.py这个文件做了两次提交。用git log命令看看：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
* 582d922 (HEAD -&gt; master) add author
* 8875536 add comment
* d1be385 (origin/master) init hello
*   e5e69f1 Merge branch 'dev'
|\  
| *   57c53ab (origin/dev, dev) fix env conflict
| |\  
| | * 7a5e5dd add env
| * | 7bd91f1 add new env
...
</code></pre>
<p>注意到Git用(HEAD -&gt; master)和(origin/master)标识出当前分支的HEAD和远程origin的位置分别是582d922 add author和d1be385 init hello，本地分支比远程分支快两个提交。</p>
<p>现在我们尝试推送本地分支：</p>
<pre><code>$ git push origin master
To github.com:michaelliao/learngit.git
 ! [rejected]        master -&gt; master (fetch first)
error: failed to push some refs to 'git@github.com:michaelliao/learngit.git'
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
</code></pre>
<p>很不幸，失败了，这<strong>说明有人先于我们推送了远程分支</strong>。按照经验，先pull一下：</p>
<pre><code>$ git pull
remote: Counting objects: 3, done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0
Unpacking objects: 100% (3/3), done.
From github.com:michaelliao/learngit
   d1be385..f005ed4  master     -&gt; origin/master
 * [new tag]         v1.0       -&gt; v1.0
Auto-merging hello.py
Merge made by the 'recursive' strategy.
 hello.py | 1 +
 1 file changed, 1 insertion(+)
</code></pre>
<p>再用git status看看状态：</p>
<pre><code>$ git status
On branch master
Your branch is ahead of 'origin/master' by 3 commits.
  (use &quot;git push&quot; to publish your local commits)

nothing to commit, working tree clean
</code></pre>
<p>加上刚才合并的提交，现在我们本地分支比远程分支超前3个提交。</p>
<p>用git log看看：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
*   e0ea545 (HEAD -&gt; master) Merge branch 'master' of github.com:michaelliao/learngit
|\  
| * f005ed4 (origin/master) set exit=1
* | 582d922 add author
* | 8875536 add comment
|/  
* d1be385 init hello
...
</code></pre>
<p>现在事情有点不对头，提交历史分叉了。如果现在把本地分支push到远程，容易出现分叉<br>
这个时候，rebase就派上了用场。我们输入命令git rebase试试：</p>
<pre><code>$ git rebase
First, rewinding head to replay your work on top of it...
Applying: add comment
Using index info to reconstruct a base tree...
M	hello.py
Falling back to patching base and 3-way merge...
Auto-merging hello.py
Applying: add author
Using index info to reconstruct a base tree...
M	hello.py
Falling back to patching base and 3-way merge...
Auto-merging hello.py
</code></pre>
<p>再用git log看看：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
* 7e61ed4 (HEAD -&gt; master) add author
* 3611cfe add comment
* f005ed4 (origin/master) set exit=1
* d1be385 init hello
...
</code></pre>
<p>原本分叉的提交现在变成一条直线了，其实原理非常简单。我们注意观察，发现<strong>Git把我们本地的提交“挪动”了位置</strong>，放到了f005ed4 (origin/master) set exit=1之后，这样，整个提交历史就成了一条直线。</p>
<p><strong>rebase操作前后，最终的提交内容是一致的</strong>，但是，我们本地的commit修改内容已经变化了，它们的<strong>修改不再基于</strong>d1be385 init hello，<strong>而是基于</strong>f005ed4 (origin/master) set exit=1，但<strong>最后的提交7e61ed4内容是一致的</strong>。</p>
<p>这就是rebase操作的特点：把分叉的提交历史“整理”成一条直线，看上去更直观。<strong>缺点是本地的分叉提交已经被修改过了</strong>。</p>
<p>最后，通过push操作把本地分支推送到远程：</p>
<pre><code>Mac:~/learngit michael$ git push origin master
Counting objects: 6, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (5/5), done.
Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done.
Total 6 (delta 2), reused 0 (delta 0)
remote: Resolving deltas: 100% (2/2), completed with 1 local object.
To github.com:michaelliao/learngit.git
   f005ed4..7e61ed4  master -&gt; master
</code></pre>
<p>再用git log看看效果：</p>
<pre><code>$ git log --graph --pretty=oneline --abbrev-commit
* 7e61ed4 (HEAD -&gt; master, origin/master) add author
* 3611cfe add comment
* f005ed4 set exit=1
* d1be385 init hello
...
</code></pre>
<p><strong>远程分支的提交历史也是一条直线</strong>。</p>
<h3 id="小结-7">小结</h3>
<p>rebase操作可以把本地未push的分叉提交历史整理成直线；</p>
<p>rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。</p>
<p>网友评论：<br>
作者这个教程中,省略了一堆操作,前面至少要有一个人先提交同一个文件,造成此时要操作的本地git库与远程不符,再在本地git进行提交操作.<br>
并且,重要的是作者并没有写出git rebase处理过程,使用git rebase之后,只是返回冲突出现的提交处的commit,之后要在这个commit中进行解决冲突;再使用git add操作添加好要解决冲突后的文件,之后还要再执行一次git rebase --continu,到此git rebase衍合过程才真正结束;<br>
推荐阅读：<a href="https://www.cnblogs.com/pinefantasy/articles/6287147.html">【Git系列】git rebase详解</a><br>
就使用过程来说： ===只对尚未推送或分享给别人的本地修改执行变基操作清理历史； ===从不对已推送至别处的提交执行变基操作</p>
]]></content>
    </entry>
</feed>